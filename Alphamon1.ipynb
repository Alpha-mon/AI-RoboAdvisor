{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPy9XlIh8DhY3ChUJbQrH4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alpha-mon/AI-RoboAdvisor/blob/main/Alphamon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 야후 파이낸스 주식 정보 가져오기\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'NVDA'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LYWreJgOIF-",
        "outputId": "aa782cc1-5d08-479d-ab85-28ceeb2df46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 알고리즘 전체 코드\n",
        "\n",
        "# Bollinger\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)"
      ],
      "metadata": {
        "id": "6xjMYRWXODZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O-IMmO7OZj3",
        "outputId": "72795fee-74b7-43a4-a6c8-5dbcba9624bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2014-02-13    43.8505 55.0000 40.3087 56.0362 50.1006 27.0795\n",
            "2014-02-14    28.7469 55.0000 40.3868 57.5436 51.5429 25.5513\n",
            "2014-02-18    42.4711 55.0000 40.3676 57.5078 53.3726 28.8534\n",
            "2014-02-19    51.9447 55.0000 40.4145 58.1338 54.9226 29.2618\n",
            "2014-02-20    47.6999 55.0000 40.4602 59.7165 56.5726 28.8429\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.6999 80.0000 32.6874 59.5827 25.8739 59.6536\n",
            "2023-10-05    47.6999 75.0000 43.8813 59.7339 26.7274 63.8320\n",
            "2023-10-06    47.6999 70.0000 48.7282 60.0288 28.9526 55.8796\n",
            "2023-10-09    47.6999 65.0000 48.5214 60.2380 31.8860 55.9784\n",
            "2023-10-10    47.6999 60.0000 57.2054 60.5174 35.5890 51.8735\n",
            "\n",
            "[2431 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "\n",
        "# 첫 번째 LSTM 레이어 (시퀀스 출력을 반환하여 다음 레이어로 전달)\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(T, 6)))\n",
        "\n",
        "# 두 번째 LSTM 레이어\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "\n",
        "# 세 번째 LSTM 레이어 (시퀀스 출력을 반환하지 않음)\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "\n",
        "# 출력 레이어\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx-EF5efOouf",
        "outputId": "9bc0159a-9b9e-4767-9b74-e0ed58bb5ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 6s 31ms/step - loss: 504.8314 - val_loss: 8547.4258\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 403.3673 - val_loss: 9506.1572\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 380.6676 - val_loss: 9572.8760\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 349.2497 - val_loss: 9695.2969\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 313.1483 - val_loss: 10939.1572\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 317.0918 - val_loss: 9302.9932\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 25ms/step - loss: 291.0522 - val_loss: 9176.7188\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 311.4862 - val_loss: 8884.0010\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 263.8724 - val_loss: 9107.6924\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 21ms/step - loss: 270.0659 - val_loss: 9011.0215\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33092.5000\n",
            "Test Loss: 33092.5\n",
            "16/16 [==============================] - 1s 5ms/step\n",
            "[[ 45.181683 ]\n",
            " [ 47.567657 ]\n",
            " [ 47.595608 ]\n",
            " [ 49.325146 ]\n",
            " [ 51.853962 ]\n",
            " [ 52.998516 ]\n",
            " [ 61.934624 ]\n",
            " [ 63.608006 ]\n",
            " [ 59.74411  ]\n",
            " [ 40.13178  ]\n",
            " [ 47.317116 ]\n",
            " [ 45.021194 ]\n",
            " [ 46.727894 ]\n",
            " [ 50.101524 ]\n",
            " [ 45.749355 ]\n",
            " [ 46.98221  ]\n",
            " [ 55.58029  ]\n",
            " [ 58.631836 ]\n",
            " [ 62.85607  ]\n",
            " [ 68.70336  ]\n",
            " [ 70.48724  ]\n",
            " [ 64.88407  ]\n",
            " [ 53.76958  ]\n",
            " [ 43.96835  ]\n",
            " [ 35.421135 ]\n",
            " [ 34.029957 ]\n",
            " [ 31.0104   ]\n",
            " [ 31.72558  ]\n",
            " [ 33.666737 ]\n",
            " [ 31.050613 ]\n",
            " [ 32.331192 ]\n",
            " [ 30.042349 ]\n",
            " [ 29.459967 ]\n",
            " [ 32.486557 ]\n",
            " [ 36.919132 ]\n",
            " [ 39.372887 ]\n",
            " [ 33.493324 ]\n",
            " [ 24.94697  ]\n",
            " [ 25.795185 ]\n",
            " [ 25.947315 ]\n",
            " [ 23.899532 ]\n",
            " [ 19.778893 ]\n",
            " [ 19.305433 ]\n",
            " [ 17.58831  ]\n",
            " [ 17.751314 ]\n",
            " [ 18.9362   ]\n",
            " [ 17.771362 ]\n",
            " [ 15.275535 ]\n",
            " [ 14.505725 ]\n",
            " [ 16.068674 ]\n",
            " [ 18.985811 ]\n",
            " [ 22.262909 ]\n",
            " [ 23.504284 ]\n",
            " [ 27.006893 ]\n",
            " [ 32.094166 ]\n",
            " [ 36.221638 ]\n",
            " [ 40.80374  ]\n",
            " [ 49.03012  ]\n",
            " [ 58.655582 ]\n",
            " [ 61.938145 ]\n",
            " [ 64.6886   ]\n",
            " [ 67.214134 ]\n",
            " [ 52.141327 ]\n",
            " [ 53.019154 ]\n",
            " [ 49.246834 ]\n",
            " [ 46.806374 ]\n",
            " [ 38.66574  ]\n",
            " [ 26.15819  ]\n",
            " [ 20.727337 ]\n",
            " [ 16.864933 ]\n",
            " [ 14.596902 ]\n",
            " [ 14.209734 ]\n",
            " [ 17.175198 ]\n",
            " [ 24.684322 ]\n",
            " [ 24.6876   ]\n",
            " [ 27.13896  ]\n",
            " [ 30.00653  ]\n",
            " [ 31.642767 ]\n",
            " [ 38.89885  ]\n",
            " [ 50.854774 ]\n",
            " [ 12.643271 ]\n",
            " [ 46.3725   ]\n",
            " [ 57.149246 ]\n",
            " [ 62.48067  ]\n",
            " [ 78.74038  ]\n",
            " [ 68.66954  ]\n",
            " [ 61.292057 ]\n",
            " [ 73.018135 ]\n",
            " [ 92.64101  ]\n",
            " [113.00557  ]\n",
            " [111.65414  ]\n",
            " [ 58.33426  ]\n",
            " [ 20.67829  ]\n",
            " [ 19.109594 ]\n",
            " [ 23.777706 ]\n",
            " [ 29.91622  ]\n",
            " [ 32.240536 ]\n",
            " [ 35.593967 ]\n",
            " [ 41.653244 ]\n",
            " [ 43.254433 ]\n",
            " [ 44.944286 ]\n",
            " [ 49.58491  ]\n",
            " [ 52.53972  ]\n",
            " [ 56.82826  ]\n",
            " [ 72.37923  ]\n",
            " [ 78.20161  ]\n",
            " [ 72.958664 ]\n",
            " [ 69.88776  ]\n",
            " [ 59.77974  ]\n",
            " [ 56.490868 ]\n",
            " [ 52.395775 ]\n",
            " [ 46.870815 ]\n",
            " [ 31.316221 ]\n",
            " [ 38.00892  ]\n",
            " [ 68.80069  ]\n",
            " [ 95.1869   ]\n",
            " [117.49518  ]\n",
            " [127.22284  ]\n",
            " [134.47128  ]\n",
            " [127.645836 ]\n",
            " [116.92051  ]\n",
            " [115.381546 ]\n",
            " [124.92702  ]\n",
            " [109.28432  ]\n",
            " [ 96.77487  ]\n",
            " [ 81.312256 ]\n",
            " [ 77.15604  ]\n",
            " [ 55.362904 ]\n",
            " [ 51.95678  ]\n",
            " [ 45.547935 ]\n",
            " [ 41.72754  ]\n",
            " [ 34.181114 ]\n",
            " [ 34.527927 ]\n",
            " [ 36.753563 ]\n",
            " [ 41.18671  ]\n",
            " [ 43.011093 ]\n",
            " [ 42.037727 ]\n",
            " [ 39.454235 ]\n",
            " [ 37.695637 ]\n",
            " [ 33.551838 ]\n",
            " [ 24.79924  ]\n",
            " [ 19.12099  ]\n",
            " [ 21.022558 ]\n",
            " [ 22.581097 ]\n",
            " [ 27.210497 ]\n",
            " [ 34.552216 ]\n",
            " [ 45.639454 ]\n",
            " [ 55.748764 ]\n",
            " [ 58.680767 ]\n",
            " [ 63.440887 ]\n",
            " [ 66.93311  ]\n",
            " [ 68.36675  ]\n",
            " [ 71.28902  ]\n",
            " [ 73.16562  ]\n",
            " [ 72.85726  ]\n",
            " [ 72.40437  ]\n",
            " [ 68.75284  ]\n",
            " [ 65.85519  ]\n",
            " [ 62.099144 ]\n",
            " [ 49.927578 ]\n",
            " [ 35.859245 ]\n",
            " [ 34.225613 ]\n",
            " [ 44.123528 ]\n",
            " [ 30.594704 ]\n",
            " [ 30.644623 ]\n",
            " [ 31.671581 ]\n",
            " [ 35.470848 ]\n",
            " [ 37.646484 ]\n",
            " [ 39.076347 ]\n",
            " [ 38.775524 ]\n",
            " [ 43.720615 ]\n",
            " [ 59.21666  ]\n",
            " [ 53.267967 ]\n",
            " [ 30.73166  ]\n",
            " [ 15.506009 ]\n",
            " [ 11.899968 ]\n",
            " [ 10.676633 ]\n",
            " [ 10.923721 ]\n",
            " [ 18.273226 ]\n",
            " [ 23.769373 ]\n",
            " [ 26.536156 ]\n",
            " [ 30.43552  ]\n",
            " [ 33.364956 ]\n",
            " [ 40.757282 ]\n",
            " [ 56.86659  ]\n",
            " [ 64.39149  ]\n",
            " [ 66.237915 ]\n",
            " [ 60.94793  ]\n",
            " [ 58.524952 ]\n",
            " [ 66.57226  ]\n",
            " [ 73.340614 ]\n",
            " [ 72.741684 ]\n",
            " [ 71.12136  ]\n",
            " [ 63.439484 ]\n",
            " [ 59.385452 ]\n",
            " [ 49.656437 ]\n",
            " [ 44.134777 ]\n",
            " [ 43.09138  ]\n",
            " [ 40.907604 ]\n",
            " [ 40.90958  ]\n",
            " [ 43.595848 ]\n",
            " [ 46.660835 ]\n",
            " [ 48.62151  ]\n",
            " [ 47.11444  ]\n",
            " [ 47.302105 ]\n",
            " [ 46.147366 ]\n",
            " [ 39.069027 ]\n",
            " [ 35.527252 ]\n",
            " [ 36.17267  ]\n",
            " [ 37.79433  ]\n",
            " [ 39.729237 ]\n",
            " [ 42.769665 ]\n",
            " [ 48.18514  ]\n",
            " [ 55.562183 ]\n",
            " [ 58.813156 ]\n",
            " [ 55.026905 ]\n",
            " [ 68.917496 ]\n",
            " [ 61.15638  ]\n",
            " [ 51.765    ]\n",
            " [ 39.513557 ]\n",
            " [ 41.352398 ]\n",
            " [ 38.67898  ]\n",
            " [ 42.087048 ]\n",
            " [ 48.40051  ]\n",
            " [ 45.277363 ]\n",
            " [ 37.887737 ]\n",
            " [ 33.357216 ]\n",
            " [ 32.208477 ]\n",
            " [ 27.509554 ]\n",
            " [ 14.202583 ]\n",
            " [  7.9030857]\n",
            " [  9.030206 ]\n",
            " [ 12.845479 ]\n",
            " [ 12.329003 ]\n",
            " [ 11.25293  ]\n",
            " [ 12.300485 ]\n",
            " [ 13.837258 ]\n",
            " [ 14.102399 ]\n",
            " [ 16.305447 ]\n",
            " [ 24.422915 ]\n",
            " [ 39.003273 ]\n",
            " [ 53.16875  ]\n",
            " [ 64.49245  ]\n",
            " [ 67.57743  ]\n",
            " [ 66.84808  ]\n",
            " [ 64.89812  ]\n",
            " [ 64.77447  ]\n",
            " [ 59.438618 ]\n",
            " [ 59.65898  ]\n",
            " [ 60.892525 ]\n",
            " [ 61.09887  ]\n",
            " [ 62.132824 ]\n",
            " [ 62.340626 ]\n",
            " [ 63.854855 ]\n",
            " [ 63.579117 ]\n",
            " [ 67.732765 ]\n",
            " [ 69.328896 ]\n",
            " [ 72.48831  ]\n",
            " [ 69.73516  ]\n",
            " [ 55.154423 ]\n",
            " [ 55.70466  ]\n",
            " [ 54.442287 ]\n",
            " [ 59.993866 ]\n",
            " [ 66.815346 ]\n",
            " [ 66.088356 ]\n",
            " [ 63.598633 ]\n",
            " [ 57.94842  ]\n",
            " [ 55.346096 ]\n",
            " [ 50.667095 ]\n",
            " [ 45.796917 ]\n",
            " [ 44.349506 ]\n",
            " [ 45.5774   ]\n",
            " [ 46.877453 ]\n",
            " [ 47.674335 ]\n",
            " [ 47.182835 ]\n",
            " [ 46.799065 ]\n",
            " [ 46.626926 ]\n",
            " [ 46.10158  ]\n",
            " [ 45.326443 ]\n",
            " [ 42.91648  ]\n",
            " [ 43.059643 ]\n",
            " [ 43.78016  ]\n",
            " [ 44.968826 ]\n",
            " [ 46.59526  ]\n",
            " [ 47.632896 ]\n",
            " [ 47.26653  ]\n",
            " [ 46.166916 ]\n",
            " [ 41.41289  ]\n",
            " [ 35.07562  ]\n",
            " [ 28.670755 ]\n",
            " [ 20.899456 ]\n",
            " [ 16.509089 ]\n",
            " [ 18.283663 ]\n",
            " [ 22.440834 ]\n",
            " [ 23.550121 ]\n",
            " [ 21.984772 ]\n",
            " [ 21.647522 ]\n",
            " [ 21.50835  ]\n",
            " [ 20.5112   ]\n",
            " [ 20.569212 ]\n",
            " [ 22.027473 ]\n",
            " [ 19.49178  ]\n",
            " [ 18.579964 ]\n",
            " [ 17.58942  ]\n",
            " [ 16.752638 ]\n",
            " [ 17.361984 ]\n",
            " [ 23.63301  ]\n",
            " [ 29.928318 ]\n",
            " [ 35.249805 ]\n",
            " [ 41.796467 ]\n",
            " [ 47.971344 ]\n",
            " [ 54.07544  ]\n",
            " [ 54.72725  ]\n",
            " [ 53.47294  ]\n",
            " [ 56.626106 ]\n",
            " [ 60.773724 ]\n",
            " [ 64.94615  ]\n",
            " [ 64.357216 ]\n",
            " [ 59.44758  ]\n",
            " [ 55.06491  ]\n",
            " [ 56.24576  ]\n",
            " [ 55.79408  ]\n",
            " [ 55.5976   ]\n",
            " [ 56.71355  ]\n",
            " [ 56.271824 ]\n",
            " [ 51.650154 ]\n",
            " [ 46.002346 ]\n",
            " [ 43.0221   ]\n",
            " [ 43.897438 ]\n",
            " [ 44.809788 ]\n",
            " [ 45.43656  ]\n",
            " [ 46.899155 ]\n",
            " [ 48.189724 ]\n",
            " [ 44.558495 ]\n",
            " [ 44.304134 ]\n",
            " [ 46.369987 ]\n",
            " [ 45.409786 ]\n",
            " [ 44.876915 ]\n",
            " [ 46.32681  ]\n",
            " [ 45.536884 ]\n",
            " [ 44.65778  ]\n",
            " [ 42.39239  ]\n",
            " [ 35.94578  ]\n",
            " [ 35.95471  ]\n",
            " [ 33.057304 ]\n",
            " [ 30.34459  ]\n",
            " [ 19.16824  ]\n",
            " [ 23.064957 ]\n",
            " [ 24.378353 ]\n",
            " [ 34.41404  ]\n",
            " [ 40.197792 ]\n",
            " [ 44.830303 ]\n",
            " [ 53.647694 ]\n",
            " [ 58.5088   ]\n",
            " [ 60.660084 ]\n",
            " [ 59.25504  ]\n",
            " [ 55.226307 ]\n",
            " [ 53.73534  ]\n",
            " [ 53.505215 ]\n",
            " [ 53.001884 ]\n",
            " [ 54.02965  ]\n",
            " [ 54.813503 ]\n",
            " [ 53.59906  ]\n",
            " [ 47.677666 ]\n",
            " [ 43.003025 ]\n",
            " [ 39.962975 ]\n",
            " [ 35.462162 ]\n",
            " [ 33.31992  ]\n",
            " [ 35.78193  ]\n",
            " [ 37.887657 ]\n",
            " [ 37.304047 ]\n",
            " [ 27.895107 ]\n",
            " [ 16.881012 ]\n",
            " [ 15.529186 ]\n",
            " [ 14.739023 ]\n",
            " [ 14.370227 ]\n",
            " [ 14.203014 ]\n",
            " [ 14.094489 ]\n",
            " [ 13.862482 ]\n",
            " [ 12.996808 ]\n",
            " [ 12.561824 ]\n",
            " [ 12.500961 ]\n",
            " [ 14.409971 ]\n",
            " [ 16.26583  ]\n",
            " [ 17.811346 ]\n",
            " [ 20.816303 ]\n",
            " [ 32.733097 ]\n",
            " [ 41.329937 ]\n",
            " [ 47.77917  ]\n",
            " [ 52.13168  ]\n",
            " [ 53.0167   ]\n",
            " [ 49.13821  ]\n",
            " [ 47.523018 ]\n",
            " [ 49.322773 ]\n",
            " [ 47.80348  ]\n",
            " [ 49.154823 ]\n",
            " [ 49.890205 ]\n",
            " [ 50.571117 ]\n",
            " [ 63.700237 ]\n",
            " [ 52.525867 ]\n",
            " [ 55.719322 ]\n",
            " [ 59.155838 ]\n",
            " [ 65.043465 ]\n",
            " [ 73.11643  ]\n",
            " [ 38.54199  ]\n",
            " [ 27.696001 ]\n",
            " [ 25.402414 ]\n",
            " [ 40.213764 ]\n",
            " [ 54.33208  ]\n",
            " [ 60.60332  ]\n",
            " [ 61.493393 ]\n",
            " [ 61.06663  ]\n",
            " [ 78.98534  ]\n",
            " [ 60.53443  ]\n",
            " [ 49.277084 ]\n",
            " [ 43.6256   ]\n",
            " [ 44.878933 ]\n",
            " [ 46.560856 ]\n",
            " [ 49.955868 ]\n",
            " [ 52.85985  ]\n",
            " [ 55.4594   ]\n",
            " [ 53.484676 ]\n",
            " [ 48.264984 ]\n",
            " [ 37.139618 ]\n",
            " [ 21.581673 ]\n",
            " [ 20.585794 ]\n",
            " [ 29.772995 ]\n",
            " [ 35.05259  ]\n",
            " [ 38.457233 ]\n",
            " [ 42.13844  ]\n",
            " [ 44.059696 ]\n",
            " [ 41.126305 ]\n",
            " [ 41.82082  ]\n",
            " [ 38.890537 ]\n",
            " [ 36.308533 ]\n",
            " [ 34.756428 ]\n",
            " [ 34.653515 ]\n",
            " [ 35.612568 ]\n",
            " [ 36.358204 ]\n",
            " [ 34.927834 ]\n",
            " [ 30.33867  ]\n",
            " [ 17.242422 ]\n",
            " [ 15.732726 ]\n",
            " [ 17.797306 ]\n",
            " [ 19.126852 ]\n",
            " [ 21.750975 ]\n",
            " [ 22.714733 ]\n",
            " [ 24.20082  ]\n",
            " [ 25.279694 ]\n",
            " [ 25.910248 ]\n",
            " [ 26.41224  ]\n",
            " [ 26.282143 ]\n",
            " [ 22.788286 ]\n",
            " [ 18.475769 ]\n",
            " [ 17.29392  ]\n",
            " [ 16.496233 ]\n",
            " [ 16.374624 ]\n",
            " [ 16.85974  ]\n",
            " [ 16.790781 ]\n",
            " [ 20.213291 ]\n",
            " [ 34.232677 ]\n",
            " [ 45.95084  ]\n",
            " [ 54.846344 ]\n",
            " [ 59.13849  ]\n",
            " [ 61.33663  ]\n",
            " [ 62.204002 ]\n",
            " [ 59.024254 ]\n",
            " [ 53.26522  ]\n",
            " [ 50.371082 ]\n",
            " [ 44.478985 ]\n",
            " [ 31.819931 ]\n",
            " [ 21.168402 ]\n",
            " [ 23.900047 ]\n",
            " [ 29.896374 ]\n",
            " [ 37.45901  ]\n",
            " [ 51.44046  ]\n",
            " [105.67581  ]\n",
            " [141.1581   ]\n",
            " [129.21735  ]\n",
            " [ 71.46738  ]\n",
            " [ 43.815266 ]\n",
            " [ 43.477856 ]\n",
            " [ 39.750095 ]\n",
            " [ 27.003536 ]\n",
            " [ 22.152164 ]]\n",
            "     Predictions\n",
            "0        45.1817\n",
            "1        47.5677\n",
            "2        47.5956\n",
            "3        49.3251\n",
            "4        51.8540\n",
            "..           ...\n",
            "480      43.8153\n",
            "481      43.4779\n",
            "482      39.7501\n",
            "483      27.0035\n",
            "484      22.1522\n",
            "\n",
            "[485 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLWQhGpOO3Cu",
        "outputId": "bdf04155-ea5b-45ab-fc50-9d1833b4baaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7b743ab9e2f2>:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)"
      ],
      "metadata": {
        "id": "aRP4uYp6O4JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")"
      ],
      "metadata": {
        "id": "RaqE9b67O8Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "#학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "#학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juv4MC4gPDVw",
        "outputId": "485b886a-06dd-4a30-c51a-34e9858220d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 1s 848ms/step\n",
            "0/10 [D loss: 0.6527732610702515 | D accuracy: 51.5625] [G loss: 0.6945138573646545]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.6584954559803009 | D accuracy: 78.125] [G loss: 0.6950043439865112]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "2/10 [D loss: 0.6436459124088287 | D accuracy: 98.4375] [G loss: 0.6960358619689941]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "3/10 [D loss: 0.6232345700263977 | D accuracy: 100.0] [G loss: 0.6969425678253174]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "4/10 [D loss: 0.618502289056778 | D accuracy: 100.0] [G loss: 0.6974769830703735]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "5/10 [D loss: 0.6075031459331512 | D accuracy: 100.0] [G loss: 0.6976855993270874]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "6/10 [D loss: 0.5987242460250854 | D accuracy: 100.0] [G loss: 0.6983477473258972]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "7/10 [D loss: 0.5874119997024536 | D accuracy: 100.0] [G loss: 0.6980558633804321]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.5759556293487549 | D accuracy: 100.0] [G loss: 0.6985049247741699]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "9/10 [D loss: 0.5653042495250702 | D accuracy: 100.0] [G loss: 0.6986566781997681]\n",
            "16/16 [==============================] - 0s 6ms/step\n",
            "16/16 [==============================] - 1s 12ms/step\n",
            "[[22.591293 22.576921 22.565102 ... 22.646496 22.666279 22.669437]\n",
            " [23.78428  23.769909 23.75809  ... 23.839483 23.859266 23.862425]\n",
            " [23.798256 23.783884 23.772064 ... 23.853458 23.873241 23.8764  ]\n",
            " ...\n",
            " [19.8755   19.861128 19.849308 ... 19.930702 19.950485 19.953644]\n",
            " [13.50222  13.487847 13.476028 ... 13.557423 13.577206 13.580364]\n",
            " [11.076534 11.062161 11.050343 ... 11.131737 11.15152  11.154678]]\n",
            "22.245869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrG64NUKN5ro",
        "outputId": "d9e56986-7a31-4f39-da24-8b795ae1bc6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2013-12-06    47.4883 60.0000 54.5310 65.5699 70.6979 34.4688\n",
            "2013-12-09    47.4883 55.0000 55.4552 66.6116 72.1410 38.1999\n",
            "2013-12-10    47.4883 50.0000 55.3249 66.3360 73.8539 41.9822\n",
            "2013-12-11    47.4883 45.0000 55.3510 65.0548 75.1676 45.3457\n",
            "2013-12-12    47.4883 40.0000 54.8473 64.8099 76.5296 44.1786\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.4883 80.0000 47.7812 52.8057 13.6627 60.1780\n",
            "2023-10-05    47.4883 80.0000 51.7933 52.8968 15.0374 59.0812\n",
            "2023-10-06    47.4883 80.0000 51.0233 53.2469 17.6592 58.7956\n",
            "2023-10-09    47.4883 75.0000 51.8338 53.9705 20.9058 58.6424\n",
            "2023-10-10    47.4883 70.0000 57.8723 53.8735 24.7798 59.4091\n",
            "\n",
            "[2477 rows x 6 columns]\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 2s 13ms/step - loss: 288.5013 - val_loss: 5907.2471\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 147.4686 - val_loss: 5986.4375\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 142.5626 - val_loss: 5876.0371\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 136.5634 - val_loss: 6070.0464\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 128.3015 - val_loss: 5954.5669\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.2404 - val_loss: 6035.2227\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 127.6632 - val_loss: 5800.6040\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.5344 - val_loss: 6073.8711\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 132.1304 - val_loss: 5958.4849\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 117.2527 - val_loss: 5754.4131\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 12979.8340\n",
            "Test Loss: 12979.833984375\n",
            "16/16 [==============================] - 0s 3ms/step\n",
            "     Predictions\n",
            "0        33.2402\n",
            "1        30.5310\n",
            "2        32.8486\n",
            "3        31.9444\n",
            "4        39.3545\n",
            "..           ...\n",
            "489      69.7483\n",
            "490      71.1996\n",
            "491      64.5924\n",
            "492      51.1719\n",
            "493      33.6937\n",
            "\n",
            "[494 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-74ded9a2326e>:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "0/10 [D loss: 0.749887079000473 | D accuracy: 45.3125] [G loss: 0.6953793168067932]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.7350181043148041 | D accuracy: 50.0] [G loss: 0.696094274520874]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "2/10 [D loss: 0.7098847925662994 | D accuracy: 50.0] [G loss: 0.6962449550628662]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "3/10 [D loss: 0.6899526715278625 | D accuracy: 82.8125] [G loss: 0.6970466375350952]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "4/10 [D loss: 0.674744725227356 | D accuracy: 98.4375] [G loss: 0.6967859268188477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "5/10 [D loss: 0.6626344323158264 | D accuracy: 96.875] [G loss: 0.6973535418510437]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "6/10 [D loss: 0.6468399465084076 | D accuracy: 98.4375] [G loss: 0.6977488994598389]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "7/10 [D loss: 0.6356022953987122 | D accuracy: 98.4375] [G loss: 0.697795033454895]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.6208434998989105 | D accuracy: 96.875] [G loss: 0.6997222900390625]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "9/10 [D loss: 0.6132535338401794 | D accuracy: 96.875] [G loss: 0.6994189023971558]\n",
            "16/16 [==============================] - 0s 5ms/step\n",
            "16/16 [==============================] - 2s 26ms/step\n",
            "[[16.666477 16.668842 16.680546 ... 16.65741  16.657124 16.659262]\n",
            " [15.311872 15.314238 15.325941 ... 15.302805 15.302518 15.304657]\n",
            " [16.47065  16.473015 16.484718 ... 16.461582 16.461296 16.463434]\n",
            " ...\n",
            " [32.34256  32.344925 32.35663  ... 32.333492 32.333206 32.335346]\n",
            " [25.632305 25.63467  25.646374 ... 25.623238 25.622952 25.62509 ]\n",
            " [16.89319  16.895555 16.907259 ... 16.884123 16.883837 16.885975]]\n",
            "23.329542\n"
          ]
        }
      ],
      "source": [
        "# 주가 예측 전체 코드\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'AAPL'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()\n",
        "\n",
        "\n",
        "# Bollinger Bands 계산 함수\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD 계산 함수\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)\n",
        "\n",
        "\n",
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)\n",
        "\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(T, 6)))  # num_features는 지표의 수입니다.\n",
        "model.add(Dense(1))  # 주식 가격 예측을 위한 뉴런 하나를 가진 출력 레이어\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "#print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)\n",
        "\n",
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n",
        "\n",
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)\n",
        "\n",
        "# GAN 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "\n",
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여 GAN 모델 만들\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# GAN 학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# GAN 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "# GAN 학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인기 주식 검색 목록 -> 분마다 새로운 값으로 자동 업데이트\n",
        "# 주식 이름과 순위 변동을 출력\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# 웹 크롤링을 위한 함수 정의\n",
        "def get_naver_stock_list():\n",
        "    URL = \"https://finance.naver.com/sise/lastsearch2.nhn\"  # 네이버 증권 인기검색주식 URL\n",
        "\n",
        "    response = requests.get(URL)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    stock_list = []\n",
        "\n",
        "    for item in soup.select(\"a.tltle\"):\n",
        "        stock_list.append(item.text)\n",
        "\n",
        "    return stock_list\n",
        "\n",
        "prev_stock_list = []  # 이전 주식 리스트 초기화\n",
        "\n",
        "# 주기적인 데이터 수집 및 처리\n",
        "while True:\n",
        "    stocks = get_naver_stock_list()\n",
        "    top_10_stocks = stocks[:10]  # 처음 10개의 주식만 선택\n",
        "\n",
        "    rank_changes = []\n",
        "    for stock in top_10_stocks:\n",
        "        if stock in prev_stock_list:\n",
        "            # 순위의 변화 계산\n",
        "            rank_changes.append(prev_stock_list.index(stock) - top_10_stocks.index(stock))\n",
        "        else:\n",
        "            # 새롭게 진입한 주식은 0으로 처리\n",
        "            rank_changes.append(0)\n",
        "\n",
        "    # 데이터 프레임으로 변환\n",
        "    df = pd.DataFrame({\n",
        "        'Stock Name': top_10_stocks,\n",
        "        'Rank Change': rank_changes\n",
        "    })\n",
        "    print(df)\n",
        "\n",
        "    prev_stock_list = top_10_stocks.copy()  # 현재 주식 리스트를 이전 주식 리스트로 저장\n",
        "\n",
        "    time.sleep(60)  # 매 분마다 데이터 수집"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "h1DU9CsIDOy9",
        "outputId": "fcb53e5b-2245-41f1-c08f-f1a97890ed99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Stock Name  Rank Change\n",
            "0       삼성전자            0\n",
            "1       에코프로            0\n",
            "2     에코프로비엠            0\n",
            "3   LG에너지솔루션            0\n",
            "4   POSCO홀딩스            0\n",
            "5     두산로보틱스            0\n",
            "6      동운아나텍            0\n",
            "7     포스코퓨처엠            0\n",
            "8     신성델타테크            0\n",
            "9       신신제약            0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9768915086bd>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprev_stock_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_10_stocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 현재 주식 리스트를 이전 주식 리스트로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 매 분마다 데이터 수집\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwEYuJCr7ot0",
        "outputId": "dd2e7df0-a450-40bd-a547-5bb470ae38d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 제목, 일자 크롤링 성공\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "\n",
        "# 1. User-Agent를 설정합니다.\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# 시작 날짜와 종료 날짜 설정\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=30)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=5)\n",
        "\n",
        "current_date = start_date\n",
        "\n",
        "news_data = []\n",
        "\n",
        "while current_date <= end_date:\n",
        "    # 2. 요청을 보낼 때 headers를 추가합니다.\n",
        "    response = requests.get(base_url + current_date.strftime('%Y%m%d'), headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    for item in soup.select(\".cluster_text a\"):\n",
        "        title = item.text.strip()\n",
        "        if item.attrs[\"href\"].startswith(\"http\"):\n",
        "            news_url = item.attrs[\"href\"]\n",
        "        else:\n",
        "            news_url = \"https:\" + item.attrs[\"href\"]\n",
        "\n",
        "        detail_response = requests.get(news_url, headers=headers)  # headers 추가\n",
        "        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
        "        date_element = detail_soup.select_one(\"span.media_end_head_info_datestamp_time\")\n",
        "        date = date_element.attrs[\"data-date-time\"].split()[0]\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'date': date\n",
        "        })\n",
        "\n",
        "        # 요청 간에 약간의 지연을 두어 IP 차단을 피합니다.\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    # 다음 날짜로 이동\n",
        "    current_date += datetime.timedelta(days=1)\n",
        "\n",
        "# 뉴스 데이터를 날짜 순으로 정렬\n",
        "news_data_sorted = sorted(news_data, key=lambda x: x['date'])\n",
        "\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 시작 날짜와 종료 날짜를 문자열로 변환\n",
        "start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "# 원하는 날짜 범위만 선택\n",
        "news_df = news_df[(news_df['date'] >= start_date_str) & (news_df['date'] <= end_date_str)]\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt 객체 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 제목에서 명사만 추출하는 함수\n",
        "def extract_nouns(title):\n",
        "    return ', '.join(okt.nouns(title))\n",
        "\n",
        "# 'title' 열의 각 제목에 대하여 명사만 추출\n",
        "news_df['nouns'] = news_df['title'].apply(extract_nouns)\n",
        "\n",
        "print(news_df)\n",
        "\n",
        "# 코스피 종가 가져오기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_kospi_closing_prices():\n",
        "    url = \"https://finance.naver.com/sise/sise_index_day.nhn?code=KOSPI\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    kospi_closings = []\n",
        "\n",
        "    for i in range(1, 7):  # 네이버 금융은 한 페이지에 6일치 데이터를 보여줍니다. 6페이지(36일치)를 가져옵니다.\n",
        "        response = requests.get(url + f\"&page={i}\", headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        dates = soup.select(\".date\")\n",
        "        closings = soup.select(\".number_1\")\n",
        "\n",
        "        for d, c in zip(dates, closings[::4]):  # 종가만 가져오기 위해 slicing 사용\n",
        "            kospi_closings.append([d.text.strip(), float(c.text.replace(',', ''))])\n",
        "\n",
        "    return kospi_closings\n",
        "\n",
        "kospi_data = get_kospi_closing_prices()\n",
        "df = pd.DataFrame(kospi_data, columns=[\"Date\", \"Closing\"])\n",
        "\n",
        "# Shift를 사용해 다음 날짜의 종가를 가져와서 현재 날짜와 비교\n",
        "df[\"Up/Down\"] = (df[\"Closing\"].shift(1) > df[\"Closing\"]).astype(int)\n",
        "\n",
        "# 날짜 기준으로 최근 30일의 데이터를 가져온 후, 정렬\n",
        "df = df.sort_values(by=\"Date\").tail(30).reset_index(drop=True)\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(df)\n",
        "\n",
        "merged_df = pd.merge(news_df, df, left_on='date', right_on='Date', how='inner')\n",
        "merged_df = merged_df[['date', 'Up/Down', 'title', 'nouns']]\n",
        "print(merged_df)\n",
        "\n",
        "# 단어 점수 초기화\n",
        "\n",
        "# 'nouns' 칼럼의 값을 문자열로 변환\n",
        "df['nouns'] = merged_df['nouns'].astype(str)\n",
        "df = df.dropna(subset=['nouns'])\n",
        "\n",
        "# 'filtered_nouns' 컬럼 생성\n",
        "merged_df['filtered_nouns'] = merged_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word_list in merged_df['filtered_nouns'] for word in word_list}\n",
        "\n",
        "# 단어 빈도수 계산\n",
        "from collections import Counter\n",
        "word_counts = Counter(word for word_list in merged_df['filtered_nouns'] for word in word_list)\n",
        "\n",
        "# 결과 출력\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# 전체 단어 개수 출력\n",
        "total_words = sum(word_counts.values())\n",
        "print(f\"\\nTotal number of words: {total_words}\")\n",
        "\n",
        "# Up/Down 값이 1인 데이터에서 포함된 단어의 리스트\n",
        "up = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 1]['filtered_nouns']:\n",
        "    up.extend(nouns)\n",
        "\n",
        "# Up/Down 값이 0인 데이터에서 포함된 단어의 리스트\n",
        "down = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 0]['filtered_nouns']:\n",
        "    down.extend(nouns)\n",
        "\n",
        "print(\"up :\", len(up))\n",
        "print(\"down :\", len(down))\n",
        "\n",
        "# 상승 비율과 하락 비율 계산\n",
        "total_words = len(up) + len(down)\n",
        "up_ratio = len(up) / total_words\n",
        "down_ratio = len(down) / total_words\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word in word_scores.keys()}  # 기존의 word_scores 딕셔너리 사용\n",
        "\n",
        "# Up(1) 데이터의 단어들에 대해서 하락 비율을 더해주기\n",
        "for word in up:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] += down_ratio\n",
        "\n",
        "# Down(0) 데이터의 단어들에 대해서 상승 비율을 차감해주기\n",
        "for word in down:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] -= up_ratio\n",
        "\n",
        "# 결과 확인\n",
        "print(word_scores)\n",
        "\n",
        "total = []\n",
        "for nouns in merged_df['filtered_nouns']:\n",
        "    sent_score = 0\n",
        "    for noun in nouns:\n",
        "        if noun in word_scores:\n",
        "            sent_score += word_scores[noun]\n",
        "\n",
        "    # 해당 뉴스 제목에 포함된 단어의 수로 나누어 평균 점수를 계산\n",
        "    avg_sent_score = sent_score / len(nouns) if nouns else 0  # 단어가 없는 경우 0으로 처리\n",
        "    total.append(avg_sent_score)\n",
        "\n",
        "merged_df['sent_score'] = total\n",
        "\n",
        "# 감성사전의 평균 점수 계산\n",
        "\n",
        "sent_mean = sum(word_scores.values()) / len(word_scores)\n",
        "print('감성 사전 평균 점수 : ',sent_mean)\n",
        "\n",
        "# 감성 점수 계산\n",
        "def calculate_sentiment_score(noun_list):\n",
        "    score = 0\n",
        "    for noun in noun_list:\n",
        "        if noun in word_scores:\n",
        "            score += word_scores[noun]\n",
        "    return score / (len(noun_list) if len(noun_list) != 0 else 1)\n",
        "\n",
        "merged_df['sent_score'] = merged_df['filtered_nouns'].apply(calculate_sentiment_score)\n",
        "\n",
        "# 평균 점수를 기준으로 라벨링\n",
        "merged_df['sent_label'] = merged_df['sent_score'].apply(lambda x: 1 if x > sent_mean else 0)\n",
        "\n",
        "\n",
        "\n",
        "result_df = merged_df[['date', 'Up/Down', 'sent_score', 'sent_label', 'title', 'nouns']]\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDDG0VxG6TjR",
        "outputId": "369904ca-56b5-4dcc-9ffc-ffc8c251480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date                                     title  \\\n",
            "7   2023-09-12                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "8   2023-09-12          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "9   2023-09-13                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "10  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "11  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "12  2023-09-14           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "13  2023-09-19                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "14  2023-09-21          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "15  2023-09-22                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "16  2023-09-26                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "17  2023-09-27          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "18  2023-09-27                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "19  2023-09-27                        이더리움 NFT가 지갑이 된다고?   \n",
            "20  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "21  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "22  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "23  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "24  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "25  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "26  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "27  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "28  2023-10-04  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "29  2023-10-05                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "30  2023-10-05               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "31  2023-10-05                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "32  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "33  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "34  2023-10-05       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "35  2023-10-06            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "36  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "37  2023-10-06                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "38  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "39  2023-10-07     'LG트윈스' 우승하면 준다 했는데…금고 속 '롤렉스·소주' 나오나   \n",
            "\n",
            "                                             nouns  \n",
            "7                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "8                              집값, 전세, 욕망, 거품, 부동산  \n",
            "9                             체인, 데이터, 코인, 투자, 데이터  \n",
            "10                              토스, 보기, 개편, 혜택, 변화  \n",
            "11                              토스, 보기, 개편, 혜택, 변화  \n",
            "12                   누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "13                                삼성, 전자, 주, 다시, 로  \n",
            "14                       개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "15                               슬슴슬금, 비트코인, 이유, 뭘  \n",
            "16                          대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "17               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "18                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "19                                       더, 리움, 지갑  \n",
            "20                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "21                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "22                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "23                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "24                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "25               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "26               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "27                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "28  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "29                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "30                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "31                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "32                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "33                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "34                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "35                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "36                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "37                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "38                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "39                             트윈스, 금고, 속, 롤렉스, 소주  \n",
            "          Date   Closing  Up/Down\n",
            "0   2023-08-25 2519.1400        1\n",
            "1   2023-08-28 2543.4100        1\n",
            "2   2023-08-29 2552.1600        1\n",
            "3   2023-08-30 2561.2200        0\n",
            "4   2023-08-31 2556.2700        1\n",
            "5   2023-09-01 2563.7100        1\n",
            "6   2023-09-04 2584.5500        0\n",
            "7   2023-09-05 2582.1800        0\n",
            "8   2023-09-06 2563.3400        0\n",
            "9   2023-09-07 2548.2600        0\n",
            "10  2023-09-08 2547.6800        1\n",
            "11  2023-09-11 2556.8800        0\n",
            "12  2023-09-12 2536.5800        0\n",
            "13  2023-09-13 2534.7000        1\n",
            "14  2023-09-14 2572.8900        1\n",
            "15  2023-09-15 2601.2800        0\n",
            "16  2023-09-18 2574.7200        0\n",
            "17  2023-09-19 2559.2100        1\n",
            "18  2023-09-20 2559.7400        0\n",
            "19  2023-09-21 2514.9700        0\n",
            "20  2023-09-22 2508.1300        0\n",
            "21  2023-09-25 2495.7600        0\n",
            "22  2023-09-26 2462.9700        1\n",
            "23  2023-09-27 2465.0700        0\n",
            "24  2023-10-04 2405.6900        0\n",
            "25  2023-10-05 2403.6000        1\n",
            "26  2023-10-06 2408.7300        0\n",
            "27  2023-10-10 2402.5800        1\n",
            "28  2023-10-11 2450.0800        1\n",
            "29  2023-10-12 2479.8200        0\n",
            "          date  Up/Down                                     title  \\\n",
            "0   2023-09-12        0                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1   2023-09-12        0          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2   2023-09-13        1                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5   2023-09-14        1           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6   2023-09-19        1                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7   2023-09-21        0          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8   2023-09-22        0                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9   2023-09-26        1                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10  2023-09-27        0          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11  2023-09-27        0                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12  2023-09-27        0                        이더리움 NFT가 지갑이 된다고?   \n",
            "13  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  2023-10-04        0  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18  2023-10-05        1                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19  2023-10-05        1               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20  2023-10-05        1                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23  2023-10-05        1       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24  2023-10-06        0            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26  2023-10-06        0                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "두산: 3\n",
            "로보틱스: 2\n",
            "대감: 1\n",
            "신고: 1\n",
            "경신: 3\n",
            "집값: 1\n",
            "전세: 1\n",
            "욕망: 1\n",
            "거품: 1\n",
            "부동산: 1\n",
            "체인: 1\n",
            "데이터: 2\n",
            "코인: 1\n",
            "투자: 1\n",
            "토스: 2\n",
            "보기: 2\n",
            "개편: 2\n",
            "혜택: 2\n",
            "변화: 2\n",
            "누적: 1\n",
            "사용자: 1\n",
            "육박: 1\n",
            "교과서: 1\n",
            "플레이어: 1\n",
            "부상: 1\n",
            "삼성: 1\n",
            "전자: 1\n",
            "다시: 1\n",
            "서비스: 1\n",
            "이용자: 1\n",
            "화면: 1\n",
            "차지: 1\n",
            "슬슴슬금: 1\n",
            "비트코인: 1\n",
            "이유: 2\n",
            "대륙: 1\n",
            "실수: 1\n",
            "반복: 1\n",
            "실력: 1\n",
            "방심: 1\n",
            "금물: 1\n",
            "부당: 1\n",
            "이득: 1\n",
            "최대: 1\n",
            "벌금: 1\n",
            "질서: 1\n",
            "중인: 1\n",
            "가상: 1\n",
            "자산: 1\n",
            "제네시스: 1\n",
            "럭셔리: 1\n",
            "쿠페: 1\n",
            "출시: 1\n",
            "리움: 1\n",
            "지갑: 1\n",
            "달러: 4\n",
            "환율: 4\n",
            "연고: 2\n",
            "독주: 2\n",
            "인생: 2\n",
            "절반: 2\n",
            "나병환자: 2\n",
            "위해: 2\n",
            "명의: 2\n",
            "청년: 2\n",
            "의사: 2\n",
            "백양: 1\n",
            "라엘: 1\n",
            "대표: 1\n",
            "생리대: 1\n",
            "판매: 1\n",
            "글로벌: 1\n",
            "니스: 1\n",
            "케어: 1\n",
            "기업: 2\n",
            "도약: 1\n",
            "스킨: 1\n",
            "부스터: 1\n",
            "발주: 1\n",
            "자신감: 1\n",
            "테크: 1\n",
            "전문가: 1\n",
            "육성: 1\n",
            "한경: 1\n",
            "부산: 1\n",
            "상장: 1\n",
            "첫날: 1\n",
            "따블: 1\n",
            "기업인: 2\n",
            "출신: 2\n",
            "연세: 2\n",
            "크림: 2\n",
            "외화: 1\n",
            "쇼크: 1\n",
            "초비상: 1\n",
            "농업: 1\n",
            "고용: 1\n",
            "지표: 1\n",
            "대기: 1\n",
            "하락: 1\n",
            "위스키: 2\n",
            "발렌타인: 2\n",
            "상자: 2\n",
            "곰팡이: 2\n",
            "빗썸: 1\n",
            "수수료: 1\n",
            "무료: 1\n",
            "프로젝트: 1\n",
            "덕분: 1\n",
            "\n",
            "Total number of words: 145\n",
            "up : 58\n",
            "down : 87\n",
            "{'두산': -0.20000000000000007, '로보틱스': 0.19999999999999996, '대감': -0.4, '신고': -0.4, '경신': -1.2000000000000002, '집값': -0.4, '전세': -0.4, '욕망': -0.4, '거품': -0.4, '부동산': -0.4, '체인': 0.6, '데이터': 1.2, '코인': 0.6, '투자': 0.6, '토스': 1.2, '보기': 1.2, '개편': 1.2, '혜택': 1.2, '변화': 1.2, '누적': 0.6, '사용자': 0.6, '육박': 0.6, '교과서': 0.6, '플레이어': 0.6, '부상': 0.6, '삼성': 0.6, '전자': 0.6, '다시': 0.6, '서비스': -0.4, '이용자': -0.4, '화면': -0.4, '차지': -0.4, '슬슴슬금': -0.4, '비트코인': -0.4, '이유': 0.19999999999999996, '대륙': 0.6, '실수': 0.6, '반복': 0.6, '실력': 0.6, '방심': 0.6, '금물': 0.6, '부당': -0.4, '이득': -0.4, '최대': -0.4, '벌금': -0.4, '질서': -0.4, '중인': -0.4, '가상': -0.4, '자산': -0.4, '제네시스': -0.4, '럭셔리': -0.4, '쿠페': -0.4, '출시': -0.4, '리움': -0.4, '지갑': -0.4, '달러': -1.6, '환율': -0.6000000000000001, '연고': -0.8, '독주': -0.8, '인생': -0.8, '절반': -0.8, '나병환자': -0.8, '위해': -0.8, '명의': -0.8, '청년': -0.8, '의사': -0.8, '백양': -0.4, '라엘': -0.4, '대표': -0.4, '생리대': -0.4, '판매': -0.4, '글로벌': -0.4, '니스': -0.4, '케어': -0.4, '기업': 0.19999999999999996, '도약': -0.4, '스킨': 0.6, '부스터': 0.6, '발주': 0.6, '자신감': 0.6, '테크': 0.6, '전문가': 0.6, '육성': 0.6, '한경': 0.6, '부산': 0.6, '상장': 0.6, '첫날': 0.6, '따블': 0.6, '기업인': 1.2, '출신': 1.2, '연세': 1.2, '크림': 1.2, '외화': 0.6, '쇼크': 0.6, '초비상': 0.6, '농업': -0.4, '고용': -0.4, '지표': -0.4, '대기': -0.4, '하락': -0.4, '위스키': -0.8, '발렌타인': -0.8, '상자': -0.8, '곰팡이': -0.8, '빗썸': -0.4, '수수료': -0.4, '무료': -0.4, '프로젝트': -0.4, '덕분': -0.4}\n",
            "감성 사전 평균 점수 :  -1.7417260294578143e-16\n",
            "          date  Up/Down  sent_score  sent_label  \\\n",
            "0   2023-09-12        0     -0.3667           0   \n",
            "1   2023-09-12        0     -0.4000           0   \n",
            "2   2023-09-13        1      0.8400           1   \n",
            "3   2023-09-13        1      1.2000           1   \n",
            "4   2023-09-13        1      1.2000           1   \n",
            "5   2023-09-14        1      0.6000           1   \n",
            "6   2023-09-19        1      0.6000           1   \n",
            "7   2023-09-21        0     -0.4000           0   \n",
            "8   2023-09-22        0     -0.2000           0   \n",
            "9   2023-09-26        1      0.6000           1   \n",
            "10  2023-09-27        0     -0.4000           0   \n",
            "11  2023-09-27        0     -0.4000           0   \n",
            "12  2023-09-27        0     -0.4000           0   \n",
            "13  2023-09-27        0     -1.1000           0   \n",
            "14  2023-09-27        0     -0.8000           0   \n",
            "15  2023-09-27        0     -0.8000           0   \n",
            "16  2023-09-27        0     -1.1000           0   \n",
            "17  2023-10-04        0     -0.3400           0   \n",
            "18  2023-10-05        1      0.5200           1   \n",
            "19  2023-10-05        1      0.6000           1   \n",
            "20  2023-10-05        1      0.3600           1   \n",
            "21  2023-10-05        1      1.2000           1   \n",
            "22  2023-10-05        1      1.2000           1   \n",
            "23  2023-10-05        1      0.2800           1   \n",
            "24  2023-10-06        0     -0.4333           0   \n",
            "25  2023-10-06        0     -0.8000           0   \n",
            "26  2023-10-06        0     -0.4000           0   \n",
            "27  2023-10-06        0     -0.8000           0   \n",
            "\n",
            "                                       title  \\\n",
            "0                 두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1           집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2                 온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5            누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6                      삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7           \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8                     슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9                   '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12                        이더리움 NFT가 지갑이 된다고?   \n",
            "13         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델링\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 데이터 준비\n",
        "X_data = merged_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values\n",
        "Y_data = merged_df['sent_label'].values  # 기존의 'merged_df'를 사용\n",
        "\n",
        "# 2. 토큰화 및 패딩\n",
        "vocab_size = 2000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_data)\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=30)\n",
        "\n",
        "# 3. Bi-LSTM 모델 구축 및 훈련\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU0fANWF6cDB",
        "outputId": "089a7e74-6549-417e-b96e-21f24a16f8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5000\n",
            "Epoch 1: val_acc improved from -inf to 0.66667, saving model to best_model.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6938 - acc: 0.5000 - val_loss: 0.6803 - val_acc: 0.6667\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6885 - acc: 0.5455\n",
            "Epoch 2: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6885 - acc: 0.5455 - val_loss: 0.6737 - val_acc: 0.6667\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6857 - acc: 0.5455"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6857 - acc: 0.5455 - val_loss: 0.6700 - val_acc: 0.6667\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.5455\n",
            "Epoch 4: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6833 - acc: 0.5455 - val_loss: 0.6679 - val_acc: 0.6667\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.5455\n",
            "Epoch 5: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6809 - acc: 0.5455 - val_loss: 0.6667 - val_acc: 0.6667\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.5455\n",
            "Epoch 6: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6782 - acc: 0.5455 - val_loss: 0.6658 - val_acc: 0.6667\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6750 - acc: 0.5455\n",
            "Epoch 7: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6750 - acc: 0.5455 - val_loss: 0.6652 - val_acc: 0.6667\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6712 - acc: 0.5455\n",
            "Epoch 8: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6712 - acc: 0.5455 - val_loss: 0.6646 - val_acc: 0.6667\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6668 - acc: 0.5455\n",
            "Epoch 9: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6668 - acc: 0.5455 - val_loss: 0.6640 - val_acc: 0.6667\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6614 - acc: 0.5455\n",
            "Epoch 10: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6614 - acc: 0.5455 - val_loss: 0.6634 - val_acc: 0.6667\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6549 - acc: 0.5455\n",
            "Epoch 11: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6549 - acc: 0.5455 - val_loss: 0.6627 - val_acc: 0.6667\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6472 - acc: 0.5455\n",
            "Epoch 12: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.6472 - acc: 0.5455 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6379 - acc: 0.5455\n",
            "Epoch 13: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6379 - acc: 0.5455 - val_loss: 0.6614 - val_acc: 0.6667\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6267 - acc: 0.6364\n",
            "Epoch 14: val_acc improved from 0.66667 to 0.83333, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6267 - acc: 0.6364 - val_loss: 0.6609 - val_acc: 0.8333\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6133 - acc: 0.8182\n",
            "Epoch 15: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6133 - acc: 0.8182 - val_loss: 0.6606 - val_acc: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 뉴스 수집 후 모델 통과시키기\n",
        "# 결과 확인\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from konlpy.tag import Okt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 4일전 ~ 1일전의 뉴스 제목 크롤링\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=4)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "current_date = start_date\n",
        "news_data = []\n",
        "\n",
        "# 크롤링 코드...\n",
        "\n",
        "# 데이터 정렬 및 저장\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 2. 명사 추출\n",
        "okt = Okt()\n",
        "news_df['nouns'] = news_df['title'].apply(lambda x: ', '.join(okt.nouns(x)))\n",
        "\n",
        "# 3. 감성 사전과 비교하여 감성 점수 계산\n",
        "news_df['filtered_nouns'] = news_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "news_df['sent_score'] = news_df['filtered_nouns'].apply(calculate_sentiment_score)  # 이전에 정의한 함수\n",
        "\n",
        "# 4. 예측을 위한 데이터 전처리\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(news_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values)\n",
        "X_test_padded = pad_sequences(X_test_tokenized, maxlen=30)\n",
        "\n",
        "# 5. 훈련된 Bi-LSTM 모델로 예측\n",
        "predicted = model.predict(X_test_padded)\n",
        "news_df['predicted_label'] = (predicted > 0.5).astype(int)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\n",
        "\n",
        "# 훈련 데이터와 검증 데이터에 대한 정확도 출력\n",
        "train_acc = history.history['acc'][-1]  # 훈련 데이터의 마지막 epoch의 정확도\n",
        "val_acc = history.history['val_acc'][-1]  # 검증 데이터의 마지막 epoch의 정확도\n",
        "\n",
        "print(f\"훈련 데이터 정확도: {train_acc:.4f}\")\n",
        "print(f\"검증 데이터 정확도: {val_acc:.4f}\")\n",
        "\n",
        "# 6. 미래 주식 시장 예측 결과 출력\n",
        "positive_news_ratio = news_df['predicted_label'].sum() / len(news_df)\n",
        "if positive_news_ratio > 0.5:\n",
        "    print(\"미래 주식 시장은 긍정적으로 움직일 것으로 예상됩니다.\")\n",
        "else:\n",
        "    print(\"미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\")\n",
        "\n",
        "pos_news = len(merged_df[merged_df['sent_score'] > sent_mean])\n",
        "neg_news = len(merged_df[merged_df['sent_score'] <= sent_mean])\n",
        "total_news = len(merged_df)\n",
        "\n",
        "print(f\"긍정적 뉴스 수: {pos_news} ({pos_news/total_news*100:.2f}%)\")\n",
        "print(f\"부정적 뉴스 수: {neg_news} ({neg_news/total_news*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n긍정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] > sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)\n",
        "\n",
        "print(\"\\n부정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] <= sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVhdxyFX83-V",
        "outputId": "6ad6212e-a991-44d1-f8eb-b33b89e63718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 19ms/step\n",
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5971 - acc: 0.9091\n",
            "Epoch 1: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5971 - acc: 0.9091 - val_loss: 0.6603 - val_acc: 0.8333\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5778 - acc: 1.0000\n",
            "Epoch 2: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5778 - acc: 1.0000 - val_loss: 0.6599 - val_acc: 0.8333\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5549 - acc: 1.0000\n",
            "Epoch 3: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.5549 - acc: 1.0000 - val_loss: 0.6587 - val_acc: 0.8333\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5282 - acc: 1.0000\n",
            "Epoch 4: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.5282 - acc: 1.0000 - val_loss: 0.6584 - val_acc: 0.8333\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4975 - acc: 1.0000\n",
            "Epoch 5: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4975 - acc: 1.0000 - val_loss: 0.6386 - val_acc: 0.8333\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4652 - acc: 1.0000\n",
            "Epoch 6: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4652 - acc: 1.0000 - val_loss: 1.0218 - val_acc: 0.3333\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6120 - acc: 0.5000\n",
            "Epoch 7: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.6120 - acc: 0.5000 - val_loss: 0.6158 - val_acc: 0.8333\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.8636\n",
            "Epoch 8: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4534 - acc: 0.8636 - val_loss: 0.6321 - val_acc: 0.8333\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4208 - acc: 1.0000\n",
            "Epoch 9: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4208 - acc: 1.0000 - val_loss: 0.6398 - val_acc: 0.8333\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3981 - acc: 1.0000\n",
            "Epoch 10: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3981 - acc: 1.0000 - val_loss: 0.6440 - val_acc: 0.8333\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3763 - acc: 1.0000\n",
            "Epoch 11: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3763 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.8333\n",
            "Epoch 11: early stopping\n",
            "훈련 데이터 정확도: 1.0000\n",
            "검증 데이터 정확도: 0.8333\n",
            "미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\n",
            "긍정적 뉴스 수: 12 (42.86%)\n",
            "부정적 뉴스 수: 16 (57.14%)\n",
            "\n",
            "긍정적 뉴스 예시:\n",
            "- 온체인 데이터를 보면 코인 투자 데이터가 보인다\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 누적 사용자 100만명 육박…AI 교과서 키플레이어 부상\n",
            "- 삼성전자, 3주만에 다시 '6만전자'로\n",
            "\n",
            "부정적 뉴스 예시:\n",
            "- 두산, 두산로보틱스 IPO 기대감에 신고가 경신\n",
            "- 집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다\n",
            "- \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"\n",
            "- 슬슴슬금 오르는 비트코인, 이유가 뭘까?\n",
            "- “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자로부터 보유 주식 입력 받기\n",
        "stock_list = input(\"보유하고 있는 주식의 티커 목록을 쉼표로 구분하여 입력하세요 (예: AAPL, GOOGL, MSFT): \").split(\",\")\n",
        "stock_list = [stock.strip() for stock in stock_list]\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "# 주식 데이터 가져오기\n",
        "def fetch_data(tickers):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.Ticker(ticker)\n",
        "        data[ticker] = stock_data.history(period=\"1y\")  # 최근 1년간의 데이터 가져오기\n",
        "    return data\n",
        "\n",
        "stock_data = fetch_data(stock_list)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 수익률 및 변동성 계산\n",
        "returns = {}\n",
        "for stock, data in stock_data.items():\n",
        "    prices = data['Close'].values.astype(float)\n",
        "    daily_returns = prices[1:] / prices[:-1] - 1\n",
        "    returns[stock] = daily_returns\n",
        "\n",
        "# LSTM 모델 학습을 위한 데이터 준비\n",
        "look_back = 5\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset) - look_back - 1):\n",
        "        a = dataset[i:(i + look_back)]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# LSTM 모델 학습\n",
        "models = {}\n",
        "for stock, daily_returns in returns.items():\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    daily_returns = scaler.fit_transform(daily_returns.reshape(-1, 1))\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_size = int(len(daily_returns) * 0.67)\n",
        "    test_size = len(daily_returns) - train_size\n",
        "    train, test = daily_returns[0:train_size, :], daily_returns[train_size:len(daily_returns), :]\n",
        "\n",
        "    # LSTM에 필요한 데이터 형식으로 변환\n",
        "    X_train, Y_train = create_dataset(train, look_back)\n",
        "    X_test, Y_test = create_dataset(test, look_back)\n",
        "\n",
        "    # LSTM 모델 구성\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "    # 모델 저장\n",
        "    models[stock] = model\n",
        "\n",
        "print(\"모든 주식에 대한 LSTM 모델 학습이 완료되었습니다.\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# 각 주식의 예측된 변동성 계산\n",
        "predicted_volatilities = {}\n",
        "for stock, model in models.items():\n",
        "    last_sequence = returns[stock][-look_back:]\n",
        "    scaled_sequence = scaler.transform(last_sequence.reshape(-1, 1))\n",
        "    predicted_return = model.predict(scaled_sequence.reshape(1, look_back, 1))\n",
        "    predicted_volatility = scaler.inverse_transform(predicted_return)[0][0]\n",
        "    predicted_volatilities[stock] = predicted_volatility\n",
        "\n",
        "# 리스크 기여도를 동일하게 하는 최적의 주식 비중 계산 함수\n",
        "def risk_parity_objective(weights, volatilities):\n",
        "    # 각 주식의 리스크 기여도 계산\n",
        "    risk_contributions = [vol * weight for vol, weight in zip(volatilities, weights)]\n",
        "    total_portfolio_volatility = np.sum(risk_contributions)\n",
        "    risk_contributions = [rc / total_portfolio_volatility for rc in risk_contributions]\n",
        "\n",
        "    # 리스크 기여도 간의 편차를 최소화하는 것이 목표\n",
        "    target_risk_contribution = 1 / len(volatilities)\n",
        "    return sum([(rc - target_risk_contribution)**2 for rc in risk_contributions])\n",
        "\n",
        "# 최적화 시작\n",
        "initial_weights = [1/len(stock_list) for _ in stock_list]\n",
        "bounds = [(0, 1) for _ in stock_list]\n",
        "constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}\n",
        "optimized = minimize(risk_parity_objective, initial_weights, args=(list(predicted_volatilities.values()),),\n",
        "                     method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "# 최적의 주식 비중 출력\n",
        "optimal_weights = optimized.x\n",
        "print(\"최적의 주식 비중:\")\n",
        "for stock, weight in zip(stock_list, optimal_weights):\n",
        "    print(f\"{stock}: {weight:.2f}\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# 실제 변동성 계산\n",
        "actual_volatilities = {stock: data['Close'].pct_change().std() for stock, data in stock_data.items()}\n",
        "\n",
        "# 그래프 그리기\n",
        "stocks = list(stock_data.keys())\n",
        "predicted_vols = [predicted_volatilities[stock] for stock in stocks]\n",
        "actual_vols = [actual_volatilities[stock] for stock in stocks]\n",
        "weights = [weight for weight in optimal_weights]\n",
        "\n",
        "barWidth = 0.25\n",
        "r1 = np.arange(len(stocks))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# 바 차트 생성\n",
        "plt.bar(r1, predicted_vols, width=barWidth, color='blue', edgecolor='grey', label='예측된 변동성')\n",
        "plt.bar(r2, actual_vols, width=barWidth, color='red', edgecolor='grey', label='실제 변동성')\n",
        "plt.bar(r3, weights, width=barWidth, color='green', edgecolor='grey', label='최적의 주식 비중')\n",
        "\n",
        "# 그래프 제목 및 축 이름 설정\n",
        "plt.title('주식별 예측된 변동성, 실제 변동성 및 최적의 주식 비중', fontweight='bold')\n",
        "plt.xlabel('주식', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(stocks))], stocks)\n",
        "plt.ylabel('값', fontweight='bold')\n",
        "\n",
        "# 범례 표시\n",
        "plt.legend()\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()\n",
        "\n",
        "def recommend_weights(optimal_weights, stocks):\n",
        "    print(\"\\n추천 주식 비중:\")\n",
        "    for stock, weight in zip(stocks, optimal_weights):\n",
        "        print(f\"{stock}: {weight:.2%}\")\n",
        "\n",
        "recommend_weights(weights, stocks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXNq6cbi3VYu",
        "outputId": "8f25ca0c-0706-42fe-b216-f580c41389a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보유하고 있는 주식의 티커 목록을 쉼표로 구분하여 입력하세요 (예: AAPL, GOOGL, MSFT): 005930.KS,LPL,NVDA\n",
            "Epoch 1/100\n",
            "158/158 - 1s - loss: 0.0346 - 1s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "158/158 - 0s - loss: 0.0277 - 323ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "158/158 - 0s - loss: 0.0268 - 320ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "158/158 - 0s - loss: 0.0277 - 295ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "158/158 - 0s - loss: 0.0262 - 329ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "158/158 - 0s - loss: 0.0263 - 267ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "158/158 - 0s - loss: 0.0258 - 277ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "158/158 - 0s - loss: 0.0266 - 285ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "158/158 - 0s - loss: 0.0248 - 246ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "158/158 - 0s - loss: 0.0246 - 243ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "158/158 - 0s - loss: 0.0251 - 307ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "158/158 - 0s - loss: 0.0248 - 287ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "158/158 - 0s - loss: 0.0257 - 257ms/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "158/158 - 0s - loss: 0.0253 - 270ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "158/158 - 0s - loss: 0.0241 - 256ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "158/158 - 0s - loss: 0.0253 - 275ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "158/158 - 0s - loss: 0.0257 - 240ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "158/158 - 0s - loss: 0.0247 - 255ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "158/158 - 0s - loss: 0.0247 - 256ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "158/158 - 0s - loss: 0.0246 - 248ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "158/158 - 0s - loss: 0.0243 - 239ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "158/158 - 0s - loss: 0.0248 - 287ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "158/158 - 0s - loss: 0.0246 - 246ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "158/158 - 0s - loss: 0.0243 - 260ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "158/158 - 0s - loss: 0.0246 - 264ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "158/158 - 0s - loss: 0.0249 - 268ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "158/158 - 0s - loss: 0.0244 - 246ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "158/158 - 0s - loss: 0.0247 - 244ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "158/158 - 0s - loss: 0.0241 - 264ms/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "158/158 - 0s - loss: 0.0238 - 242ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "158/158 - 0s - loss: 0.0251 - 245ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "158/158 - 0s - loss: 0.0244 - 280ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "158/158 - 0s - loss: 0.0243 - 269ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "158/158 - 0s - loss: 0.0250 - 263ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "158/158 - 0s - loss: 0.0242 - 260ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "158/158 - 0s - loss: 0.0247 - 245ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "158/158 - 0s - loss: 0.0255 - 262ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "158/158 - 0s - loss: 0.0245 - 243ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "158/158 - 0s - loss: 0.0245 - 252ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "158/158 - 0s - loss: 0.0244 - 249ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "158/158 - 0s - loss: 0.0246 - 262ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "158/158 - 0s - loss: 0.0243 - 284ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "158/158 - 0s - loss: 0.0235 - 271ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "158/158 - 0s - loss: 0.0246 - 349ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "158/158 - 0s - loss: 0.0244 - 306ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "158/158 - 0s - loss: 0.0243 - 339ms/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "158/158 - 0s - loss: 0.0238 - 302ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "158/158 - 0s - loss: 0.0244 - 313ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "158/158 - 0s - loss: 0.0243 - 291ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "158/158 - 0s - loss: 0.0242 - 247ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "158/158 - 0s - loss: 0.0248 - 275ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "158/158 - 0s - loss: 0.0237 - 269ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "158/158 - 0s - loss: 0.0243 - 250ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "158/158 - 0s - loss: 0.0244 - 249ms/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "158/158 - 0s - loss: 0.0246 - 240ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "158/158 - 0s - loss: 0.0241 - 259ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "158/158 - 0s - loss: 0.0236 - 239ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "158/158 - 0s - loss: 0.0238 - 249ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "158/158 - 0s - loss: 0.0247 - 252ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "158/158 - 0s - loss: 0.0242 - 247ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "158/158 - 0s - loss: 0.0241 - 256ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "158/158 - 0s - loss: 0.0247 - 265ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "158/158 - 0s - loss: 0.0239 - 304ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "158/158 - 0s - loss: 0.0237 - 246ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "158/158 - 0s - loss: 0.0242 - 248ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "158/158 - 0s - loss: 0.0239 - 261ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "158/158 - 0s - loss: 0.0235 - 243ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "158/158 - 0s - loss: 0.0239 - 258ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "158/158 - 0s - loss: 0.0241 - 230ms/epoch - 1ms/step\n",
            "Epoch 70/100\n",
            "158/158 - 0s - loss: 0.0240 - 238ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "158/158 - 0s - loss: 0.0240 - 248ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "158/158 - 0s - loss: 0.0235 - 241ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "158/158 - 0s - loss: 0.0237 - 247ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "158/158 - 0s - loss: 0.0237 - 256ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "158/158 - 0s - loss: 0.0236 - 258ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "158/158 - 0s - loss: 0.0238 - 272ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "158/158 - 0s - loss: 0.0232 - 238ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "158/158 - 0s - loss: 0.0241 - 245ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "158/158 - 0s - loss: 0.0236 - 248ms/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "158/158 - 0s - loss: 0.0240 - 239ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "158/158 - 0s - loss: 0.0236 - 230ms/epoch - 1ms/step\n",
            "Epoch 82/100\n",
            "158/158 - 0s - loss: 0.0232 - 251ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "158/158 - 0s - loss: 0.0240 - 282ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "158/158 - 0s - loss: 0.0238 - 275ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "158/158 - 0s - loss: 0.0237 - 244ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "158/158 - 0s - loss: 0.0235 - 260ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "158/158 - 0s - loss: 0.0233 - 246ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "158/158 - 0s - loss: 0.0232 - 258ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "158/158 - 0s - loss: 0.0235 - 315ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "158/158 - 0s - loss: 0.0233 - 322ms/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "158/158 - 0s - loss: 0.0236 - 334ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "158/158 - 0s - loss: 0.0236 - 305ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "158/158 - 0s - loss: 0.0235 - 326ms/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "158/158 - 0s - loss: 0.0234 - 287ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "158/158 - 0s - loss: 0.0230 - 235ms/epoch - 1ms/step\n",
            "Epoch 96/100\n",
            "158/158 - 0s - loss: 0.0236 - 246ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "158/158 - 0s - loss: 0.0235 - 256ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "158/158 - 0s - loss: 0.0234 - 265ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "158/158 - 0s - loss: 0.0234 - 245ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "158/158 - 0s - loss: 0.0232 - 271ms/epoch - 2ms/step\n",
            "Epoch 1/100\n",
            "161/161 - 1s - loss: 0.0362 - 1s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "161/161 - 0s - loss: 0.0294 - 242ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "161/161 - 0s - loss: 0.0289 - 272ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "161/161 - 0s - loss: 0.0277 - 258ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "161/161 - 0s - loss: 0.0284 - 254ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "161/161 - 0s - loss: 0.0282 - 253ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "161/161 - 0s - loss: 0.0280 - 285ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "161/161 - 0s - loss: 0.0276 - 251ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "161/161 - 0s - loss: 0.0271 - 263ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "161/161 - 0s - loss: 0.0276 - 269ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "161/161 - 0s - loss: 0.0275 - 246ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "161/161 - 0s - loss: 0.0274 - 249ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "161/161 - 0s - loss: 0.0264 - 247ms/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "161/161 - 0s - loss: 0.0274 - 260ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "161/161 - 0s - loss: 0.0273 - 274ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "161/161 - 0s - loss: 0.0261 - 254ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "161/161 - 0s - loss: 0.0271 - 325ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "161/161 - 0s - loss: 0.0265 - 307ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "161/161 - 0s - loss: 0.0273 - 284ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "161/161 - 0s - loss: 0.0264 - 258ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "161/161 - 0s - loss: 0.0270 - 243ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "161/161 - 0s - loss: 0.0264 - 249ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "161/161 - 0s - loss: 0.0271 - 270ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "161/161 - 0s - loss: 0.0262 - 239ms/epoch - 1ms/step\n",
            "Epoch 25/100\n",
            "161/161 - 0s - loss: 0.0262 - 250ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "161/161 - 0s - loss: 0.0265 - 277ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "161/161 - 0s - loss: 0.0275 - 337ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "161/161 - 0s - loss: 0.0264 - 328ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "161/161 - 0s - loss: 0.0271 - 320ms/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "161/161 - 0s - loss: 0.0265 - 372ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "161/161 - 0s - loss: 0.0264 - 386ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "161/161 - 0s - loss: 0.0268 - 251ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "161/161 - 0s - loss: 0.0268 - 255ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "161/161 - 0s - loss: 0.0256 - 243ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "161/161 - 0s - loss: 0.0271 - 256ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "161/161 - 0s - loss: 0.0266 - 296ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "161/161 - 0s - loss: 0.0265 - 253ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "161/161 - 0s - loss: 0.0267 - 249ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "161/161 - 0s - loss: 0.0270 - 289ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "161/161 - 0s - loss: 0.0263 - 282ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "161/161 - 0s - loss: 0.0263 - 261ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "161/161 - 0s - loss: 0.0264 - 262ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "161/161 - 0s - loss: 0.0263 - 309ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "161/161 - 0s - loss: 0.0271 - 266ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "161/161 - 0s - loss: 0.0264 - 240ms/epoch - 1ms/step\n",
            "Epoch 46/100\n",
            "161/161 - 0s - loss: 0.0264 - 240ms/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "161/161 - 0s - loss: 0.0264 - 256ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "161/161 - 0s - loss: 0.0261 - 251ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "161/161 - 0s - loss: 0.0258 - 253ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "161/161 - 0s - loss: 0.0263 - 278ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "161/161 - 0s - loss: 0.0267 - 302ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "161/161 - 0s - loss: 0.0263 - 264ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "161/161 - 0s - loss: 0.0260 - 260ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "161/161 - 0s - loss: 0.0260 - 311ms/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "161/161 - 0s - loss: 0.0262 - 278ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "161/161 - 0s - loss: 0.0258 - 247ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "161/161 - 0s - loss: 0.0262 - 259ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "161/161 - 0s - loss: 0.0263 - 299ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "161/161 - 0s - loss: 0.0262 - 246ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "161/161 - 0s - loss: 0.0260 - 236ms/epoch - 1ms/step\n",
            "Epoch 61/100\n",
            "161/161 - 0s - loss: 0.0263 - 260ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "161/161 - 0s - loss: 0.0256 - 247ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "161/161 - 0s - loss: 0.0263 - 246ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "161/161 - 0s - loss: 0.0263 - 244ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "161/161 - 0s - loss: 0.0261 - 267ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "161/161 - 0s - loss: 0.0267 - 250ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "161/161 - 0s - loss: 0.0262 - 261ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "161/161 - 0s - loss: 0.0258 - 251ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "161/161 - 0s - loss: 0.0260 - 314ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "161/161 - 0s - loss: 0.0261 - 331ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "161/161 - 0s - loss: 0.0253 - 335ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "161/161 - 0s - loss: 0.0259 - 313ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "161/161 - 0s - loss: 0.0259 - 347ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "161/161 - 0s - loss: 0.0259 - 325ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "161/161 - 0s - loss: 0.0259 - 282ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "161/161 - 0s - loss: 0.0256 - 272ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "161/161 - 0s - loss: 0.0255 - 255ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "161/161 - 0s - loss: 0.0249 - 260ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "161/161 - 0s - loss: 0.0260 - 250ms/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "161/161 - 0s - loss: 0.0259 - 261ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "161/161 - 0s - loss: 0.0254 - 244ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "161/161 - 0s - loss: 0.0255 - 278ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "161/161 - 0s - loss: 0.0256 - 267ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "161/161 - 0s - loss: 0.0253 - 254ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "161/161 - 0s - loss: 0.0256 - 255ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "161/161 - 0s - loss: 0.0252 - 259ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "161/161 - 0s - loss: 0.0256 - 297ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "161/161 - 0s - loss: 0.0251 - 272ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "161/161 - 0s - loss: 0.0252 - 246ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "161/161 - 0s - loss: 0.0245 - 258ms/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "161/161 - 0s - loss: 0.0241 - 248ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "161/161 - 0s - loss: 0.0247 - 254ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "161/161 - 0s - loss: 0.0239 - 284ms/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "161/161 - 0s - loss: 0.0241 - 250ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "161/161 - 0s - loss: 0.0229 - 268ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "161/161 - 0s - loss: 0.0234 - 275ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "161/161 - 0s - loss: 0.0230 - 259ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "161/161 - 0s - loss: 0.0233 - 251ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "161/161 - 0s - loss: 0.0225 - 267ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "161/161 - 0s - loss: 0.0227 - 246ms/epoch - 2ms/step\n",
            "Epoch 1/100\n",
            "161/161 - 2s - loss: 0.0189 - 2s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "161/161 - 0s - loss: 0.0165 - 304ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "161/161 - 0s - loss: 0.0149 - 320ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "161/161 - 0s - loss: 0.0167 - 253ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "161/161 - 0s - loss: 0.0154 - 246ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "161/161 - 0s - loss: 0.0159 - 241ms/epoch - 1ms/step\n",
            "Epoch 7/100\n",
            "161/161 - 0s - loss: 0.0153 - 287ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "161/161 - 0s - loss: 0.0154 - 265ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "161/161 - 0s - loss: 0.0155 - 262ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "161/161 - 0s - loss: 0.0152 - 283ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "161/161 - 0s - loss: 0.0154 - 271ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "161/161 - 0s - loss: 0.0149 - 267ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "161/161 - 0s - loss: 0.0152 - 274ms/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "161/161 - 0s - loss: 0.0149 - 249ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "161/161 - 0s - loss: 0.0148 - 250ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "161/161 - 0s - loss: 0.0150 - 290ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "161/161 - 0s - loss: 0.0145 - 246ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "161/161 - 0s - loss: 0.0150 - 258ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "161/161 - 0s - loss: 0.0139 - 246ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "161/161 - 0s - loss: 0.0146 - 246ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "161/161 - 0s - loss: 0.0150 - 257ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "161/161 - 0s - loss: 0.0150 - 259ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "161/161 - 0s - loss: 0.0150 - 289ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "161/161 - 0s - loss: 0.0147 - 281ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "161/161 - 0s - loss: 0.0148 - 253ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "161/161 - 0s - loss: 0.0149 - 278ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "161/161 - 0s - loss: 0.0143 - 247ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "161/161 - 0s - loss: 0.0146 - 250ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "161/161 - 0s - loss: 0.0146 - 263ms/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "161/161 - 0s - loss: 0.0144 - 277ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "161/161 - 0s - loss: 0.0146 - 249ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "161/161 - 0s - loss: 0.0142 - 252ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "161/161 - 0s - loss: 0.0144 - 260ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "161/161 - 0s - loss: 0.0144 - 317ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "161/161 - 0s - loss: 0.0145 - 313ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "161/161 - 0s - loss: 0.0145 - 313ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "161/161 - 0s - loss: 0.0144 - 327ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "161/161 - 0s - loss: 0.0144 - 256ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "161/161 - 0s - loss: 0.0148 - 326ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "161/161 - 0s - loss: 0.0144 - 309ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "161/161 - 0s - loss: 0.0144 - 316ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "161/161 - 0s - loss: 0.0141 - 335ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "161/161 - 0s - loss: 0.0144 - 326ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "161/161 - 0s - loss: 0.0146 - 329ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "161/161 - 0s - loss: 0.0140 - 243ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "161/161 - 0s - loss: 0.0142 - 234ms/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "161/161 - 0s - loss: 0.0140 - 251ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "161/161 - 0s - loss: 0.0143 - 253ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "161/161 - 0s - loss: 0.0142 - 270ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "161/161 - 0s - loss: 0.0141 - 243ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "161/161 - 0s - loss: 0.0139 - 255ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "161/161 - 0s - loss: 0.0143 - 255ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "161/161 - 0s - loss: 0.0142 - 280ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "161/161 - 0s - loss: 0.0140 - 307ms/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "161/161 - 0s - loss: 0.0141 - 250ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "161/161 - 0s - loss: 0.0140 - 285ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "161/161 - 0s - loss: 0.0138 - 246ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "161/161 - 0s - loss: 0.0142 - 248ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "161/161 - 0s - loss: 0.0142 - 255ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "161/161 - 0s - loss: 0.0143 - 284ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "161/161 - 0s - loss: 0.0139 - 248ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "161/161 - 0s - loss: 0.0144 - 265ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "161/161 - 0s - loss: 0.0139 - 273ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "161/161 - 0s - loss: 0.0141 - 260ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "161/161 - 0s - loss: 0.0139 - 289ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "161/161 - 0s - loss: 0.0141 - 265ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "161/161 - 0s - loss: 0.0139 - 255ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "161/161 - 0s - loss: 0.0139 - 255ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "161/161 - 0s - loss: 0.0139 - 254ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "161/161 - 0s - loss: 0.0139 - 260ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "161/161 - 0s - loss: 0.0140 - 262ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "161/161 - 0s - loss: 0.0138 - 270ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "161/161 - 0s - loss: 0.0137 - 299ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "161/161 - 0s - loss: 0.0137 - 242ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "161/161 - 0s - loss: 0.0136 - 277ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "161/161 - 0s - loss: 0.0137 - 289ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "161/161 - 0s - loss: 0.0133 - 257ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "161/161 - 0s - loss: 0.0138 - 255ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "161/161 - 0s - loss: 0.0136 - 249ms/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "161/161 - 0s - loss: 0.0140 - 247ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "161/161 - 0s - loss: 0.0136 - 246ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "161/161 - 0s - loss: 0.0136 - 296ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "161/161 - 0s - loss: 0.0137 - 374ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "161/161 - 0s - loss: 0.0135 - 338ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "161/161 - 0s - loss: 0.0138 - 372ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "161/161 - 0s - loss: 0.0136 - 309ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "161/161 - 0s - loss: 0.0138 - 321ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "161/161 - 0s - loss: 0.0136 - 250ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "161/161 - 0s - loss: 0.0134 - 253ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "161/161 - 0s - loss: 0.0136 - 238ms/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "161/161 - 0s - loss: 0.0135 - 251ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "161/161 - 0s - loss: 0.0133 - 250ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "161/161 - 0s - loss: 0.0132 - 232ms/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "161/161 - 0s - loss: 0.0136 - 247ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "161/161 - 0s - loss: 0.0133 - 268ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "161/161 - 0s - loss: 0.0136 - 265ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "161/161 - 0s - loss: 0.0133 - 291ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "161/161 - 0s - loss: 0.0133 - 252ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "161/161 - 0s - loss: 0.0134 - 290ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "161/161 - 0s - loss: 0.0133 - 257ms/epoch - 2ms/step\n",
            "모든 주식에 대한 LSTM 모델 학습이 완료되었습니다.\n",
            "1/1 [==============================] - 0s 272ms/step\n",
            "1/1 [==============================] - 0s 281ms/step\n",
            "1/1 [==============================] - 0s 294ms/step\n",
            "최적의 주식 비중:\n",
            "005930.KS: 0.05\n",
            "LPL: 0.05\n",
            "NVDA: 0.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44050 (\\N{HANGUL SYLLABLE GABS}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50696 (\\N{HANGUL SYLLABLE YE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52769 (\\N{HANGUL SYLLABLE CEUG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51228 (\\N{HANGUL SYLLABLE JE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48143 (\\N{HANGUL SYLLABLE MIC}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52572 (\\N{HANGUL SYLLABLE COE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51201 (\\N{HANGUL SYLLABLE JEOG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "추천 주식 비중:\n",
            "005930.KS: 4.61%\n",
            "LPL: 4.98%\n",
            "NVDA: 90.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_volatility, actual_volatilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZOLr5RcMjRv",
        "outputId": "59a89d56-7bc2-4e0e-d417-8c52e85345c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0036807635 {'005930.KS': 0.01452299748430584, 'LPL': 0.025597889466783548, 'NVDA': 0.03381753063933791}\n"
          ]
        }
      ]
    }
  ]
}