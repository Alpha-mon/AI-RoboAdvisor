{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8yGUXTbtPT03LZZJ5vxZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alpha-mon/AI-RoboAdvisor/blob/main/Alphamon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 야후 파이낸스 주식 정보 가져오기\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'NVDA'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LYWreJgOIF-",
        "outputId": "aa782cc1-5d08-479d-ab85-28ceeb2df46d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 알고리즘 전체 코드\n",
        "\n",
        "# Bollinger\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)"
      ],
      "metadata": {
        "id": "6xjMYRWXODZP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O-IMmO7OZj3",
        "outputId": "72795fee-74b7-43a4-a6c8-5dbcba9624bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2014-02-13    43.8505 55.0000 40.3087 56.0362 50.1006 27.0795\n",
            "2014-02-14    28.7469 55.0000 40.3868 57.5436 51.5429 25.5513\n",
            "2014-02-18    42.4711 55.0000 40.3676 57.5078 53.3726 28.8534\n",
            "2014-02-19    51.9447 55.0000 40.4145 58.1338 54.9226 29.2618\n",
            "2014-02-20    47.6999 55.0000 40.4602 59.7165 56.5726 28.8429\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.6999 80.0000 32.6874 59.5827 25.8739 59.6536\n",
            "2023-10-05    47.6999 75.0000 43.8813 59.7339 26.7274 63.8320\n",
            "2023-10-06    47.6999 70.0000 48.7282 60.0288 28.9526 55.8796\n",
            "2023-10-09    47.6999 65.0000 48.5214 60.2380 31.8860 55.9784\n",
            "2023-10-10    47.6999 60.0000 57.2054 60.5174 35.5890 51.8735\n",
            "\n",
            "[2431 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "\n",
        "# 첫 번째 LSTM 레이어 (시퀀스 출력을 반환하여 다음 레이어로 전달)\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(T, 6)))\n",
        "\n",
        "# 두 번째 LSTM 레이어\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "\n",
        "# 세 번째 LSTM 레이어 (시퀀스 출력을 반환하지 않음)\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "\n",
        "# 출력 레이어\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx-EF5efOouf",
        "outputId": "9bc0159a-9b9e-4767-9b74-e0ed58bb5ebe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 6s 31ms/step - loss: 504.8314 - val_loss: 8547.4258\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 403.3673 - val_loss: 9506.1572\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 380.6676 - val_loss: 9572.8760\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 349.2497 - val_loss: 9695.2969\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 313.1483 - val_loss: 10939.1572\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 317.0918 - val_loss: 9302.9932\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 25ms/step - loss: 291.0522 - val_loss: 9176.7188\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 311.4862 - val_loss: 8884.0010\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 263.8724 - val_loss: 9107.6924\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 21ms/step - loss: 270.0659 - val_loss: 9011.0215\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33092.5000\n",
            "Test Loss: 33092.5\n",
            "16/16 [==============================] - 1s 5ms/step\n",
            "[[ 45.181683 ]\n",
            " [ 47.567657 ]\n",
            " [ 47.595608 ]\n",
            " [ 49.325146 ]\n",
            " [ 51.853962 ]\n",
            " [ 52.998516 ]\n",
            " [ 61.934624 ]\n",
            " [ 63.608006 ]\n",
            " [ 59.74411  ]\n",
            " [ 40.13178  ]\n",
            " [ 47.317116 ]\n",
            " [ 45.021194 ]\n",
            " [ 46.727894 ]\n",
            " [ 50.101524 ]\n",
            " [ 45.749355 ]\n",
            " [ 46.98221  ]\n",
            " [ 55.58029  ]\n",
            " [ 58.631836 ]\n",
            " [ 62.85607  ]\n",
            " [ 68.70336  ]\n",
            " [ 70.48724  ]\n",
            " [ 64.88407  ]\n",
            " [ 53.76958  ]\n",
            " [ 43.96835  ]\n",
            " [ 35.421135 ]\n",
            " [ 34.029957 ]\n",
            " [ 31.0104   ]\n",
            " [ 31.72558  ]\n",
            " [ 33.666737 ]\n",
            " [ 31.050613 ]\n",
            " [ 32.331192 ]\n",
            " [ 30.042349 ]\n",
            " [ 29.459967 ]\n",
            " [ 32.486557 ]\n",
            " [ 36.919132 ]\n",
            " [ 39.372887 ]\n",
            " [ 33.493324 ]\n",
            " [ 24.94697  ]\n",
            " [ 25.795185 ]\n",
            " [ 25.947315 ]\n",
            " [ 23.899532 ]\n",
            " [ 19.778893 ]\n",
            " [ 19.305433 ]\n",
            " [ 17.58831  ]\n",
            " [ 17.751314 ]\n",
            " [ 18.9362   ]\n",
            " [ 17.771362 ]\n",
            " [ 15.275535 ]\n",
            " [ 14.505725 ]\n",
            " [ 16.068674 ]\n",
            " [ 18.985811 ]\n",
            " [ 22.262909 ]\n",
            " [ 23.504284 ]\n",
            " [ 27.006893 ]\n",
            " [ 32.094166 ]\n",
            " [ 36.221638 ]\n",
            " [ 40.80374  ]\n",
            " [ 49.03012  ]\n",
            " [ 58.655582 ]\n",
            " [ 61.938145 ]\n",
            " [ 64.6886   ]\n",
            " [ 67.214134 ]\n",
            " [ 52.141327 ]\n",
            " [ 53.019154 ]\n",
            " [ 49.246834 ]\n",
            " [ 46.806374 ]\n",
            " [ 38.66574  ]\n",
            " [ 26.15819  ]\n",
            " [ 20.727337 ]\n",
            " [ 16.864933 ]\n",
            " [ 14.596902 ]\n",
            " [ 14.209734 ]\n",
            " [ 17.175198 ]\n",
            " [ 24.684322 ]\n",
            " [ 24.6876   ]\n",
            " [ 27.13896  ]\n",
            " [ 30.00653  ]\n",
            " [ 31.642767 ]\n",
            " [ 38.89885  ]\n",
            " [ 50.854774 ]\n",
            " [ 12.643271 ]\n",
            " [ 46.3725   ]\n",
            " [ 57.149246 ]\n",
            " [ 62.48067  ]\n",
            " [ 78.74038  ]\n",
            " [ 68.66954  ]\n",
            " [ 61.292057 ]\n",
            " [ 73.018135 ]\n",
            " [ 92.64101  ]\n",
            " [113.00557  ]\n",
            " [111.65414  ]\n",
            " [ 58.33426  ]\n",
            " [ 20.67829  ]\n",
            " [ 19.109594 ]\n",
            " [ 23.777706 ]\n",
            " [ 29.91622  ]\n",
            " [ 32.240536 ]\n",
            " [ 35.593967 ]\n",
            " [ 41.653244 ]\n",
            " [ 43.254433 ]\n",
            " [ 44.944286 ]\n",
            " [ 49.58491  ]\n",
            " [ 52.53972  ]\n",
            " [ 56.82826  ]\n",
            " [ 72.37923  ]\n",
            " [ 78.20161  ]\n",
            " [ 72.958664 ]\n",
            " [ 69.88776  ]\n",
            " [ 59.77974  ]\n",
            " [ 56.490868 ]\n",
            " [ 52.395775 ]\n",
            " [ 46.870815 ]\n",
            " [ 31.316221 ]\n",
            " [ 38.00892  ]\n",
            " [ 68.80069  ]\n",
            " [ 95.1869   ]\n",
            " [117.49518  ]\n",
            " [127.22284  ]\n",
            " [134.47128  ]\n",
            " [127.645836 ]\n",
            " [116.92051  ]\n",
            " [115.381546 ]\n",
            " [124.92702  ]\n",
            " [109.28432  ]\n",
            " [ 96.77487  ]\n",
            " [ 81.312256 ]\n",
            " [ 77.15604  ]\n",
            " [ 55.362904 ]\n",
            " [ 51.95678  ]\n",
            " [ 45.547935 ]\n",
            " [ 41.72754  ]\n",
            " [ 34.181114 ]\n",
            " [ 34.527927 ]\n",
            " [ 36.753563 ]\n",
            " [ 41.18671  ]\n",
            " [ 43.011093 ]\n",
            " [ 42.037727 ]\n",
            " [ 39.454235 ]\n",
            " [ 37.695637 ]\n",
            " [ 33.551838 ]\n",
            " [ 24.79924  ]\n",
            " [ 19.12099  ]\n",
            " [ 21.022558 ]\n",
            " [ 22.581097 ]\n",
            " [ 27.210497 ]\n",
            " [ 34.552216 ]\n",
            " [ 45.639454 ]\n",
            " [ 55.748764 ]\n",
            " [ 58.680767 ]\n",
            " [ 63.440887 ]\n",
            " [ 66.93311  ]\n",
            " [ 68.36675  ]\n",
            " [ 71.28902  ]\n",
            " [ 73.16562  ]\n",
            " [ 72.85726  ]\n",
            " [ 72.40437  ]\n",
            " [ 68.75284  ]\n",
            " [ 65.85519  ]\n",
            " [ 62.099144 ]\n",
            " [ 49.927578 ]\n",
            " [ 35.859245 ]\n",
            " [ 34.225613 ]\n",
            " [ 44.123528 ]\n",
            " [ 30.594704 ]\n",
            " [ 30.644623 ]\n",
            " [ 31.671581 ]\n",
            " [ 35.470848 ]\n",
            " [ 37.646484 ]\n",
            " [ 39.076347 ]\n",
            " [ 38.775524 ]\n",
            " [ 43.720615 ]\n",
            " [ 59.21666  ]\n",
            " [ 53.267967 ]\n",
            " [ 30.73166  ]\n",
            " [ 15.506009 ]\n",
            " [ 11.899968 ]\n",
            " [ 10.676633 ]\n",
            " [ 10.923721 ]\n",
            " [ 18.273226 ]\n",
            " [ 23.769373 ]\n",
            " [ 26.536156 ]\n",
            " [ 30.43552  ]\n",
            " [ 33.364956 ]\n",
            " [ 40.757282 ]\n",
            " [ 56.86659  ]\n",
            " [ 64.39149  ]\n",
            " [ 66.237915 ]\n",
            " [ 60.94793  ]\n",
            " [ 58.524952 ]\n",
            " [ 66.57226  ]\n",
            " [ 73.340614 ]\n",
            " [ 72.741684 ]\n",
            " [ 71.12136  ]\n",
            " [ 63.439484 ]\n",
            " [ 59.385452 ]\n",
            " [ 49.656437 ]\n",
            " [ 44.134777 ]\n",
            " [ 43.09138  ]\n",
            " [ 40.907604 ]\n",
            " [ 40.90958  ]\n",
            " [ 43.595848 ]\n",
            " [ 46.660835 ]\n",
            " [ 48.62151  ]\n",
            " [ 47.11444  ]\n",
            " [ 47.302105 ]\n",
            " [ 46.147366 ]\n",
            " [ 39.069027 ]\n",
            " [ 35.527252 ]\n",
            " [ 36.17267  ]\n",
            " [ 37.79433  ]\n",
            " [ 39.729237 ]\n",
            " [ 42.769665 ]\n",
            " [ 48.18514  ]\n",
            " [ 55.562183 ]\n",
            " [ 58.813156 ]\n",
            " [ 55.026905 ]\n",
            " [ 68.917496 ]\n",
            " [ 61.15638  ]\n",
            " [ 51.765    ]\n",
            " [ 39.513557 ]\n",
            " [ 41.352398 ]\n",
            " [ 38.67898  ]\n",
            " [ 42.087048 ]\n",
            " [ 48.40051  ]\n",
            " [ 45.277363 ]\n",
            " [ 37.887737 ]\n",
            " [ 33.357216 ]\n",
            " [ 32.208477 ]\n",
            " [ 27.509554 ]\n",
            " [ 14.202583 ]\n",
            " [  7.9030857]\n",
            " [  9.030206 ]\n",
            " [ 12.845479 ]\n",
            " [ 12.329003 ]\n",
            " [ 11.25293  ]\n",
            " [ 12.300485 ]\n",
            " [ 13.837258 ]\n",
            " [ 14.102399 ]\n",
            " [ 16.305447 ]\n",
            " [ 24.422915 ]\n",
            " [ 39.003273 ]\n",
            " [ 53.16875  ]\n",
            " [ 64.49245  ]\n",
            " [ 67.57743  ]\n",
            " [ 66.84808  ]\n",
            " [ 64.89812  ]\n",
            " [ 64.77447  ]\n",
            " [ 59.438618 ]\n",
            " [ 59.65898  ]\n",
            " [ 60.892525 ]\n",
            " [ 61.09887  ]\n",
            " [ 62.132824 ]\n",
            " [ 62.340626 ]\n",
            " [ 63.854855 ]\n",
            " [ 63.579117 ]\n",
            " [ 67.732765 ]\n",
            " [ 69.328896 ]\n",
            " [ 72.48831  ]\n",
            " [ 69.73516  ]\n",
            " [ 55.154423 ]\n",
            " [ 55.70466  ]\n",
            " [ 54.442287 ]\n",
            " [ 59.993866 ]\n",
            " [ 66.815346 ]\n",
            " [ 66.088356 ]\n",
            " [ 63.598633 ]\n",
            " [ 57.94842  ]\n",
            " [ 55.346096 ]\n",
            " [ 50.667095 ]\n",
            " [ 45.796917 ]\n",
            " [ 44.349506 ]\n",
            " [ 45.5774   ]\n",
            " [ 46.877453 ]\n",
            " [ 47.674335 ]\n",
            " [ 47.182835 ]\n",
            " [ 46.799065 ]\n",
            " [ 46.626926 ]\n",
            " [ 46.10158  ]\n",
            " [ 45.326443 ]\n",
            " [ 42.91648  ]\n",
            " [ 43.059643 ]\n",
            " [ 43.78016  ]\n",
            " [ 44.968826 ]\n",
            " [ 46.59526  ]\n",
            " [ 47.632896 ]\n",
            " [ 47.26653  ]\n",
            " [ 46.166916 ]\n",
            " [ 41.41289  ]\n",
            " [ 35.07562  ]\n",
            " [ 28.670755 ]\n",
            " [ 20.899456 ]\n",
            " [ 16.509089 ]\n",
            " [ 18.283663 ]\n",
            " [ 22.440834 ]\n",
            " [ 23.550121 ]\n",
            " [ 21.984772 ]\n",
            " [ 21.647522 ]\n",
            " [ 21.50835  ]\n",
            " [ 20.5112   ]\n",
            " [ 20.569212 ]\n",
            " [ 22.027473 ]\n",
            " [ 19.49178  ]\n",
            " [ 18.579964 ]\n",
            " [ 17.58942  ]\n",
            " [ 16.752638 ]\n",
            " [ 17.361984 ]\n",
            " [ 23.63301  ]\n",
            " [ 29.928318 ]\n",
            " [ 35.249805 ]\n",
            " [ 41.796467 ]\n",
            " [ 47.971344 ]\n",
            " [ 54.07544  ]\n",
            " [ 54.72725  ]\n",
            " [ 53.47294  ]\n",
            " [ 56.626106 ]\n",
            " [ 60.773724 ]\n",
            " [ 64.94615  ]\n",
            " [ 64.357216 ]\n",
            " [ 59.44758  ]\n",
            " [ 55.06491  ]\n",
            " [ 56.24576  ]\n",
            " [ 55.79408  ]\n",
            " [ 55.5976   ]\n",
            " [ 56.71355  ]\n",
            " [ 56.271824 ]\n",
            " [ 51.650154 ]\n",
            " [ 46.002346 ]\n",
            " [ 43.0221   ]\n",
            " [ 43.897438 ]\n",
            " [ 44.809788 ]\n",
            " [ 45.43656  ]\n",
            " [ 46.899155 ]\n",
            " [ 48.189724 ]\n",
            " [ 44.558495 ]\n",
            " [ 44.304134 ]\n",
            " [ 46.369987 ]\n",
            " [ 45.409786 ]\n",
            " [ 44.876915 ]\n",
            " [ 46.32681  ]\n",
            " [ 45.536884 ]\n",
            " [ 44.65778  ]\n",
            " [ 42.39239  ]\n",
            " [ 35.94578  ]\n",
            " [ 35.95471  ]\n",
            " [ 33.057304 ]\n",
            " [ 30.34459  ]\n",
            " [ 19.16824  ]\n",
            " [ 23.064957 ]\n",
            " [ 24.378353 ]\n",
            " [ 34.41404  ]\n",
            " [ 40.197792 ]\n",
            " [ 44.830303 ]\n",
            " [ 53.647694 ]\n",
            " [ 58.5088   ]\n",
            " [ 60.660084 ]\n",
            " [ 59.25504  ]\n",
            " [ 55.226307 ]\n",
            " [ 53.73534  ]\n",
            " [ 53.505215 ]\n",
            " [ 53.001884 ]\n",
            " [ 54.02965  ]\n",
            " [ 54.813503 ]\n",
            " [ 53.59906  ]\n",
            " [ 47.677666 ]\n",
            " [ 43.003025 ]\n",
            " [ 39.962975 ]\n",
            " [ 35.462162 ]\n",
            " [ 33.31992  ]\n",
            " [ 35.78193  ]\n",
            " [ 37.887657 ]\n",
            " [ 37.304047 ]\n",
            " [ 27.895107 ]\n",
            " [ 16.881012 ]\n",
            " [ 15.529186 ]\n",
            " [ 14.739023 ]\n",
            " [ 14.370227 ]\n",
            " [ 14.203014 ]\n",
            " [ 14.094489 ]\n",
            " [ 13.862482 ]\n",
            " [ 12.996808 ]\n",
            " [ 12.561824 ]\n",
            " [ 12.500961 ]\n",
            " [ 14.409971 ]\n",
            " [ 16.26583  ]\n",
            " [ 17.811346 ]\n",
            " [ 20.816303 ]\n",
            " [ 32.733097 ]\n",
            " [ 41.329937 ]\n",
            " [ 47.77917  ]\n",
            " [ 52.13168  ]\n",
            " [ 53.0167   ]\n",
            " [ 49.13821  ]\n",
            " [ 47.523018 ]\n",
            " [ 49.322773 ]\n",
            " [ 47.80348  ]\n",
            " [ 49.154823 ]\n",
            " [ 49.890205 ]\n",
            " [ 50.571117 ]\n",
            " [ 63.700237 ]\n",
            " [ 52.525867 ]\n",
            " [ 55.719322 ]\n",
            " [ 59.155838 ]\n",
            " [ 65.043465 ]\n",
            " [ 73.11643  ]\n",
            " [ 38.54199  ]\n",
            " [ 27.696001 ]\n",
            " [ 25.402414 ]\n",
            " [ 40.213764 ]\n",
            " [ 54.33208  ]\n",
            " [ 60.60332  ]\n",
            " [ 61.493393 ]\n",
            " [ 61.06663  ]\n",
            " [ 78.98534  ]\n",
            " [ 60.53443  ]\n",
            " [ 49.277084 ]\n",
            " [ 43.6256   ]\n",
            " [ 44.878933 ]\n",
            " [ 46.560856 ]\n",
            " [ 49.955868 ]\n",
            " [ 52.85985  ]\n",
            " [ 55.4594   ]\n",
            " [ 53.484676 ]\n",
            " [ 48.264984 ]\n",
            " [ 37.139618 ]\n",
            " [ 21.581673 ]\n",
            " [ 20.585794 ]\n",
            " [ 29.772995 ]\n",
            " [ 35.05259  ]\n",
            " [ 38.457233 ]\n",
            " [ 42.13844  ]\n",
            " [ 44.059696 ]\n",
            " [ 41.126305 ]\n",
            " [ 41.82082  ]\n",
            " [ 38.890537 ]\n",
            " [ 36.308533 ]\n",
            " [ 34.756428 ]\n",
            " [ 34.653515 ]\n",
            " [ 35.612568 ]\n",
            " [ 36.358204 ]\n",
            " [ 34.927834 ]\n",
            " [ 30.33867  ]\n",
            " [ 17.242422 ]\n",
            " [ 15.732726 ]\n",
            " [ 17.797306 ]\n",
            " [ 19.126852 ]\n",
            " [ 21.750975 ]\n",
            " [ 22.714733 ]\n",
            " [ 24.20082  ]\n",
            " [ 25.279694 ]\n",
            " [ 25.910248 ]\n",
            " [ 26.41224  ]\n",
            " [ 26.282143 ]\n",
            " [ 22.788286 ]\n",
            " [ 18.475769 ]\n",
            " [ 17.29392  ]\n",
            " [ 16.496233 ]\n",
            " [ 16.374624 ]\n",
            " [ 16.85974  ]\n",
            " [ 16.790781 ]\n",
            " [ 20.213291 ]\n",
            " [ 34.232677 ]\n",
            " [ 45.95084  ]\n",
            " [ 54.846344 ]\n",
            " [ 59.13849  ]\n",
            " [ 61.33663  ]\n",
            " [ 62.204002 ]\n",
            " [ 59.024254 ]\n",
            " [ 53.26522  ]\n",
            " [ 50.371082 ]\n",
            " [ 44.478985 ]\n",
            " [ 31.819931 ]\n",
            " [ 21.168402 ]\n",
            " [ 23.900047 ]\n",
            " [ 29.896374 ]\n",
            " [ 37.45901  ]\n",
            " [ 51.44046  ]\n",
            " [105.67581  ]\n",
            " [141.1581   ]\n",
            " [129.21735  ]\n",
            " [ 71.46738  ]\n",
            " [ 43.815266 ]\n",
            " [ 43.477856 ]\n",
            " [ 39.750095 ]\n",
            " [ 27.003536 ]\n",
            " [ 22.152164 ]]\n",
            "     Predictions\n",
            "0        45.1817\n",
            "1        47.5677\n",
            "2        47.5956\n",
            "3        49.3251\n",
            "4        51.8540\n",
            "..           ...\n",
            "480      43.8153\n",
            "481      43.4779\n",
            "482      39.7501\n",
            "483      27.0035\n",
            "484      22.1522\n",
            "\n",
            "[485 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLWQhGpOO3Cu",
        "outputId": "bdf04155-ea5b-45ab-fc50-9d1833b4baaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7b743ab9e2f2>:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)"
      ],
      "metadata": {
        "id": "aRP4uYp6O4JD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")"
      ],
      "metadata": {
        "id": "RaqE9b67O8Hc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "#학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "#학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juv4MC4gPDVw",
        "outputId": "485b886a-06dd-4a30-c51a-34e9858220d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 1s 848ms/step\n",
            "0/10 [D loss: 0.6527732610702515 | D accuracy: 51.5625] [G loss: 0.6945138573646545]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.6584954559803009 | D accuracy: 78.125] [G loss: 0.6950043439865112]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "2/10 [D loss: 0.6436459124088287 | D accuracy: 98.4375] [G loss: 0.6960358619689941]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "3/10 [D loss: 0.6232345700263977 | D accuracy: 100.0] [G loss: 0.6969425678253174]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "4/10 [D loss: 0.618502289056778 | D accuracy: 100.0] [G loss: 0.6974769830703735]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "5/10 [D loss: 0.6075031459331512 | D accuracy: 100.0] [G loss: 0.6976855993270874]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "6/10 [D loss: 0.5987242460250854 | D accuracy: 100.0] [G loss: 0.6983477473258972]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "7/10 [D loss: 0.5874119997024536 | D accuracy: 100.0] [G loss: 0.6980558633804321]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.5759556293487549 | D accuracy: 100.0] [G loss: 0.6985049247741699]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "9/10 [D loss: 0.5653042495250702 | D accuracy: 100.0] [G loss: 0.6986566781997681]\n",
            "16/16 [==============================] - 0s 6ms/step\n",
            "16/16 [==============================] - 1s 12ms/step\n",
            "[[22.591293 22.576921 22.565102 ... 22.646496 22.666279 22.669437]\n",
            " [23.78428  23.769909 23.75809  ... 23.839483 23.859266 23.862425]\n",
            " [23.798256 23.783884 23.772064 ... 23.853458 23.873241 23.8764  ]\n",
            " ...\n",
            " [19.8755   19.861128 19.849308 ... 19.930702 19.950485 19.953644]\n",
            " [13.50222  13.487847 13.476028 ... 13.557423 13.577206 13.580364]\n",
            " [11.076534 11.062161 11.050343 ... 11.131737 11.15152  11.154678]]\n",
            "22.245869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrG64NUKN5ro",
        "outputId": "d9e56986-7a31-4f39-da24-8b795ae1bc6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2013-12-06    47.4883 60.0000 54.5310 65.5699 70.6979 34.4688\n",
            "2013-12-09    47.4883 55.0000 55.4552 66.6116 72.1410 38.1999\n",
            "2013-12-10    47.4883 50.0000 55.3249 66.3360 73.8539 41.9822\n",
            "2013-12-11    47.4883 45.0000 55.3510 65.0548 75.1676 45.3457\n",
            "2013-12-12    47.4883 40.0000 54.8473 64.8099 76.5296 44.1786\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.4883 80.0000 47.7812 52.8057 13.6627 60.1780\n",
            "2023-10-05    47.4883 80.0000 51.7933 52.8968 15.0374 59.0812\n",
            "2023-10-06    47.4883 80.0000 51.0233 53.2469 17.6592 58.7956\n",
            "2023-10-09    47.4883 75.0000 51.8338 53.9705 20.9058 58.6424\n",
            "2023-10-10    47.4883 70.0000 57.8723 53.8735 24.7798 59.4091\n",
            "\n",
            "[2477 rows x 6 columns]\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 2s 13ms/step - loss: 288.5013 - val_loss: 5907.2471\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 147.4686 - val_loss: 5986.4375\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 142.5626 - val_loss: 5876.0371\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 136.5634 - val_loss: 6070.0464\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 128.3015 - val_loss: 5954.5669\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.2404 - val_loss: 6035.2227\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 127.6632 - val_loss: 5800.6040\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.5344 - val_loss: 6073.8711\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 132.1304 - val_loss: 5958.4849\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 117.2527 - val_loss: 5754.4131\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 12979.8340\n",
            "Test Loss: 12979.833984375\n",
            "16/16 [==============================] - 0s 3ms/step\n",
            "     Predictions\n",
            "0        33.2402\n",
            "1        30.5310\n",
            "2        32.8486\n",
            "3        31.9444\n",
            "4        39.3545\n",
            "..           ...\n",
            "489      69.7483\n",
            "490      71.1996\n",
            "491      64.5924\n",
            "492      51.1719\n",
            "493      33.6937\n",
            "\n",
            "[494 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-74ded9a2326e>:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "0/10 [D loss: 0.749887079000473 | D accuracy: 45.3125] [G loss: 0.6953793168067932]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.7350181043148041 | D accuracy: 50.0] [G loss: 0.696094274520874]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "2/10 [D loss: 0.7098847925662994 | D accuracy: 50.0] [G loss: 0.6962449550628662]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "3/10 [D loss: 0.6899526715278625 | D accuracy: 82.8125] [G loss: 0.6970466375350952]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "4/10 [D loss: 0.674744725227356 | D accuracy: 98.4375] [G loss: 0.6967859268188477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "5/10 [D loss: 0.6626344323158264 | D accuracy: 96.875] [G loss: 0.6973535418510437]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "6/10 [D loss: 0.6468399465084076 | D accuracy: 98.4375] [G loss: 0.6977488994598389]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "7/10 [D loss: 0.6356022953987122 | D accuracy: 98.4375] [G loss: 0.697795033454895]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.6208434998989105 | D accuracy: 96.875] [G loss: 0.6997222900390625]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "9/10 [D loss: 0.6132535338401794 | D accuracy: 96.875] [G loss: 0.6994189023971558]\n",
            "16/16 [==============================] - 0s 5ms/step\n",
            "16/16 [==============================] - 2s 26ms/step\n",
            "[[16.666477 16.668842 16.680546 ... 16.65741  16.657124 16.659262]\n",
            " [15.311872 15.314238 15.325941 ... 15.302805 15.302518 15.304657]\n",
            " [16.47065  16.473015 16.484718 ... 16.461582 16.461296 16.463434]\n",
            " ...\n",
            " [32.34256  32.344925 32.35663  ... 32.333492 32.333206 32.335346]\n",
            " [25.632305 25.63467  25.646374 ... 25.623238 25.622952 25.62509 ]\n",
            " [16.89319  16.895555 16.907259 ... 16.884123 16.883837 16.885975]]\n",
            "23.329542\n"
          ]
        }
      ],
      "source": [
        "# 주가 예측 전체 코드\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'AAPL'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()\n",
        "\n",
        "\n",
        "# Bollinger Bands 계산 함수\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD 계산 함수\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)\n",
        "\n",
        "\n",
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)\n",
        "\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(T, 6)))  # num_features는 지표의 수입니다.\n",
        "model.add(Dense(1))  # 주식 가격 예측을 위한 뉴런 하나를 가진 출력 레이어\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "#print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)\n",
        "\n",
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n",
        "\n",
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)\n",
        "\n",
        "# GAN 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "\n",
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여 GAN 모델 만들\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# GAN 학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# GAN 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "# GAN 학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인기 주식 검색 목록 -> 분마다 새로운 값으로 자동 업데이트\n",
        "# 주식 이름과 순위 변동을 출력\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# 웹 크롤링을 위한 함수 정의\n",
        "def get_naver_stock_list():\n",
        "    URL = \"https://finance.naver.com/sise/lastsearch2.nhn\"  # 네이버 증권 인기검색주식 URL\n",
        "\n",
        "    response = requests.get(URL)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    stock_list = []\n",
        "\n",
        "    for item in soup.select(\"a.tltle\"):\n",
        "        stock_list.append(item.text)\n",
        "\n",
        "    return stock_list\n",
        "\n",
        "prev_stock_list = []  # 이전 주식 리스트 초기화\n",
        "\n",
        "# 주기적인 데이터 수집 및 처리\n",
        "while True:\n",
        "    stocks = get_naver_stock_list()\n",
        "    top_10_stocks = stocks[:10]  # 처음 10개의 주식만 선택\n",
        "\n",
        "    rank_changes = []\n",
        "    for stock in top_10_stocks:\n",
        "        if stock in prev_stock_list:\n",
        "            # 순위의 변화 계산\n",
        "            rank_changes.append(prev_stock_list.index(stock) - top_10_stocks.index(stock))\n",
        "        else:\n",
        "            # 새롭게 진입한 주식은 0으로 처리\n",
        "            rank_changes.append(0)\n",
        "\n",
        "    # 데이터 프레임으로 변환\n",
        "    df = pd.DataFrame({\n",
        "        'Stock Name': top_10_stocks,\n",
        "        'Rank Change': rank_changes\n",
        "    })\n",
        "    print(df)\n",
        "\n",
        "    prev_stock_list = top_10_stocks.copy()  # 현재 주식 리스트를 이전 주식 리스트로 저장\n",
        "\n",
        "    time.sleep(60)  # 매 분마다 데이터 수집"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "h1DU9CsIDOy9",
        "outputId": "fcb53e5b-2245-41f1-c08f-f1a97890ed99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Stock Name  Rank Change\n",
            "0       삼성전자            0\n",
            "1       에코프로            0\n",
            "2     에코프로비엠            0\n",
            "3   LG에너지솔루션            0\n",
            "4   POSCO홀딩스            0\n",
            "5     두산로보틱스            0\n",
            "6      동운아나텍            0\n",
            "7     포스코퓨처엠            0\n",
            "8     신성델타테크            0\n",
            "9       신신제약            0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9768915086bd>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprev_stock_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_10_stocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 현재 주식 리스트를 이전 주식 리스트로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 매 분마다 데이터 수집\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwEYuJCr7ot0",
        "outputId": "dd2e7df0-a450-40bd-a547-5bb470ae38d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 제목, 일자 크롤링 성공\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "\n",
        "# 1. User-Agent를 설정합니다.\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# 시작 날짜와 종료 날짜 설정\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=30)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=5)\n",
        "\n",
        "current_date = start_date\n",
        "\n",
        "news_data = []\n",
        "\n",
        "while current_date <= end_date:\n",
        "    # 2. 요청을 보낼 때 headers를 추가합니다.\n",
        "    response = requests.get(base_url + current_date.strftime('%Y%m%d'), headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    for item in soup.select(\".cluster_text a\"):\n",
        "        title = item.text.strip()\n",
        "        if item.attrs[\"href\"].startswith(\"http\"):\n",
        "            news_url = item.attrs[\"href\"]\n",
        "        else:\n",
        "            news_url = \"https:\" + item.attrs[\"href\"]\n",
        "\n",
        "        detail_response = requests.get(news_url, headers=headers)  # headers 추가\n",
        "        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
        "        date_element = detail_soup.select_one(\"span.media_end_head_info_datestamp_time\")\n",
        "        date = date_element.attrs[\"data-date-time\"].split()[0]\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'date': date\n",
        "        })\n",
        "\n",
        "        # 요청 간에 약간의 지연을 두어 IP 차단을 피합니다.\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    # 다음 날짜로 이동\n",
        "    current_date += datetime.timedelta(days=1)\n",
        "\n",
        "# 뉴스 데이터를 날짜 순으로 정렬\n",
        "news_data_sorted = sorted(news_data, key=lambda x: x['date'])\n",
        "\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 시작 날짜와 종료 날짜를 문자열로 변환\n",
        "start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "# 원하는 날짜 범위만 선택\n",
        "news_df = news_df[(news_df['date'] >= start_date_str) & (news_df['date'] <= end_date_str)]\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt 객체 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 제목에서 명사만 추출하는 함수\n",
        "def extract_nouns(title):\n",
        "    return ', '.join(okt.nouns(title))\n",
        "\n",
        "# 'title' 열의 각 제목에 대하여 명사만 추출\n",
        "news_df['nouns'] = news_df['title'].apply(extract_nouns)\n",
        "\n",
        "print(news_df)\n",
        "\n",
        "# 코스피 종가 가져오기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_kospi_closing_prices():\n",
        "    url = \"https://finance.naver.com/sise/sise_index_day.nhn?code=KOSPI\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    kospi_closings = []\n",
        "\n",
        "    for i in range(1, 7):  # 네이버 금융은 한 페이지에 6일치 데이터를 보여줍니다. 6페이지(36일치)를 가져옵니다.\n",
        "        response = requests.get(url + f\"&page={i}\", headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        dates = soup.select(\".date\")\n",
        "        closings = soup.select(\".number_1\")\n",
        "\n",
        "        for d, c in zip(dates, closings[::4]):  # 종가만 가져오기 위해 slicing 사용\n",
        "            kospi_closings.append([d.text.strip(), float(c.text.replace(',', ''))])\n",
        "\n",
        "    return kospi_closings\n",
        "\n",
        "kospi_data = get_kospi_closing_prices()\n",
        "df = pd.DataFrame(kospi_data, columns=[\"Date\", \"Closing\"])\n",
        "\n",
        "# Shift를 사용해 다음 날짜의 종가를 가져와서 현재 날짜와 비교\n",
        "df[\"Up/Down\"] = (df[\"Closing\"].shift(1) > df[\"Closing\"]).astype(int)\n",
        "\n",
        "# 날짜 기준으로 최근 30일의 데이터를 가져온 후, 정렬\n",
        "df = df.sort_values(by=\"Date\").tail(30).reset_index(drop=True)\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(df)\n",
        "\n",
        "merged_df = pd.merge(news_df, df, left_on='date', right_on='Date', how='inner')\n",
        "merged_df = merged_df[['date', 'Up/Down', 'title', 'nouns']]\n",
        "print(merged_df)\n",
        "\n",
        "# 단어 점수 초기화\n",
        "\n",
        "# 'nouns' 칼럼의 값을 문자열로 변환\n",
        "df['nouns'] = merged_df['nouns'].astype(str)\n",
        "df = df.dropna(subset=['nouns'])\n",
        "\n",
        "# 'filtered_nouns' 컬럼 생성\n",
        "merged_df['filtered_nouns'] = merged_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word_list in merged_df['filtered_nouns'] for word in word_list}\n",
        "\n",
        "# 단어 빈도수 계산\n",
        "from collections import Counter\n",
        "word_counts = Counter(word for word_list in merged_df['filtered_nouns'] for word in word_list)\n",
        "\n",
        "# 결과 출력\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# 전체 단어 개수 출력\n",
        "total_words = sum(word_counts.values())\n",
        "print(f\"\\nTotal number of words: {total_words}\")\n",
        "\n",
        "# Up/Down 값이 1인 데이터에서 포함된 단어의 리스트\n",
        "up = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 1]['filtered_nouns']:\n",
        "    up.extend(nouns)\n",
        "\n",
        "# Up/Down 값이 0인 데이터에서 포함된 단어의 리스트\n",
        "down = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 0]['filtered_nouns']:\n",
        "    down.extend(nouns)\n",
        "\n",
        "print(\"up :\", len(up))\n",
        "print(\"down :\", len(down))\n",
        "\n",
        "# 상승 비율과 하락 비율 계산\n",
        "total_words = len(up) + len(down)\n",
        "up_ratio = len(up) / total_words\n",
        "down_ratio = len(down) / total_words\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word in word_scores.keys()}  # 기존의 word_scores 딕셔너리 사용\n",
        "\n",
        "# Up(1) 데이터의 단어들에 대해서 하락 비율을 더해주기\n",
        "for word in up:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] += down_ratio\n",
        "\n",
        "# Down(0) 데이터의 단어들에 대해서 상승 비율을 차감해주기\n",
        "for word in down:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] -= up_ratio\n",
        "\n",
        "# 결과 확인\n",
        "print(word_scores)\n",
        "\n",
        "total = []\n",
        "for nouns in merged_df['filtered_nouns']:\n",
        "    sent_score = 0\n",
        "    for noun in nouns:\n",
        "        if noun in word_scores:\n",
        "            sent_score += word_scores[noun]\n",
        "\n",
        "    # 해당 뉴스 제목에 포함된 단어의 수로 나누어 평균 점수를 계산\n",
        "    avg_sent_score = sent_score / len(nouns) if nouns else 0  # 단어가 없는 경우 0으로 처리\n",
        "    total.append(avg_sent_score)\n",
        "\n",
        "merged_df['sent_score'] = total\n",
        "\n",
        "# 감성사전의 평균 점수 계산\n",
        "\n",
        "sent_mean = sum(word_scores.values()) / len(word_scores)\n",
        "print('감성 사전 평균 점수 : ',sent_mean)\n",
        "\n",
        "# 감성 점수 계산\n",
        "def calculate_sentiment_score(noun_list):\n",
        "    score = 0\n",
        "    for noun in noun_list:\n",
        "        if noun in word_scores:\n",
        "            score += word_scores[noun]\n",
        "    return score / (len(noun_list) if len(noun_list) != 0 else 1)\n",
        "\n",
        "merged_df['sent_score'] = merged_df['filtered_nouns'].apply(calculate_sentiment_score)\n",
        "\n",
        "# 평균 점수를 기준으로 라벨링\n",
        "merged_df['sent_label'] = merged_df['sent_score'].apply(lambda x: 1 if x > sent_mean else 0)\n",
        "\n",
        "\n",
        "\n",
        "result_df = merged_df[['date', 'Up/Down', 'sent_score', 'sent_label', 'title', 'nouns']]\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDDG0VxG6TjR",
        "outputId": "369904ca-56b5-4dcc-9ffc-ffc8c251480f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date                                     title  \\\n",
            "7   2023-09-12                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "8   2023-09-12          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "9   2023-09-13                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "10  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "11  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "12  2023-09-14           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "13  2023-09-19                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "14  2023-09-21          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "15  2023-09-22                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "16  2023-09-26                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "17  2023-09-27          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "18  2023-09-27                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "19  2023-09-27                        이더리움 NFT가 지갑이 된다고?   \n",
            "20  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "21  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "22  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "23  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "24  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "25  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "26  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "27  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "28  2023-10-04  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "29  2023-10-05                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "30  2023-10-05               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "31  2023-10-05                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "32  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "33  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "34  2023-10-05       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "35  2023-10-06            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "36  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "37  2023-10-06                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "38  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "39  2023-10-07     'LG트윈스' 우승하면 준다 했는데…금고 속 '롤렉스·소주' 나오나   \n",
            "\n",
            "                                             nouns  \n",
            "7                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "8                              집값, 전세, 욕망, 거품, 부동산  \n",
            "9                             체인, 데이터, 코인, 투자, 데이터  \n",
            "10                              토스, 보기, 개편, 혜택, 변화  \n",
            "11                              토스, 보기, 개편, 혜택, 변화  \n",
            "12                   누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "13                                삼성, 전자, 주, 다시, 로  \n",
            "14                       개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "15                               슬슴슬금, 비트코인, 이유, 뭘  \n",
            "16                          대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "17               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "18                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "19                                       더, 리움, 지갑  \n",
            "20                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "21                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "22                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "23                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "24                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "25               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "26               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "27                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "28  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "29                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "30                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "31                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "32                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "33                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "34                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "35                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "36                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "37                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "38                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "39                             트윈스, 금고, 속, 롤렉스, 소주  \n",
            "          Date   Closing  Up/Down\n",
            "0   2023-08-25 2519.1400        1\n",
            "1   2023-08-28 2543.4100        1\n",
            "2   2023-08-29 2552.1600        1\n",
            "3   2023-08-30 2561.2200        0\n",
            "4   2023-08-31 2556.2700        1\n",
            "5   2023-09-01 2563.7100        1\n",
            "6   2023-09-04 2584.5500        0\n",
            "7   2023-09-05 2582.1800        0\n",
            "8   2023-09-06 2563.3400        0\n",
            "9   2023-09-07 2548.2600        0\n",
            "10  2023-09-08 2547.6800        1\n",
            "11  2023-09-11 2556.8800        0\n",
            "12  2023-09-12 2536.5800        0\n",
            "13  2023-09-13 2534.7000        1\n",
            "14  2023-09-14 2572.8900        1\n",
            "15  2023-09-15 2601.2800        0\n",
            "16  2023-09-18 2574.7200        0\n",
            "17  2023-09-19 2559.2100        1\n",
            "18  2023-09-20 2559.7400        0\n",
            "19  2023-09-21 2514.9700        0\n",
            "20  2023-09-22 2508.1300        0\n",
            "21  2023-09-25 2495.7600        0\n",
            "22  2023-09-26 2462.9700        1\n",
            "23  2023-09-27 2465.0700        0\n",
            "24  2023-10-04 2405.6900        0\n",
            "25  2023-10-05 2403.6000        1\n",
            "26  2023-10-06 2408.7300        0\n",
            "27  2023-10-10 2402.5800        1\n",
            "28  2023-10-11 2450.0800        1\n",
            "29  2023-10-12 2479.8200        0\n",
            "          date  Up/Down                                     title  \\\n",
            "0   2023-09-12        0                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1   2023-09-12        0          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2   2023-09-13        1                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5   2023-09-14        1           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6   2023-09-19        1                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7   2023-09-21        0          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8   2023-09-22        0                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9   2023-09-26        1                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10  2023-09-27        0          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11  2023-09-27        0                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12  2023-09-27        0                        이더리움 NFT가 지갑이 된다고?   \n",
            "13  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  2023-10-04        0  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18  2023-10-05        1                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19  2023-10-05        1               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20  2023-10-05        1                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23  2023-10-05        1       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24  2023-10-06        0            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26  2023-10-06        0                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "두산: 3\n",
            "로보틱스: 2\n",
            "대감: 1\n",
            "신고: 1\n",
            "경신: 3\n",
            "집값: 1\n",
            "전세: 1\n",
            "욕망: 1\n",
            "거품: 1\n",
            "부동산: 1\n",
            "체인: 1\n",
            "데이터: 2\n",
            "코인: 1\n",
            "투자: 1\n",
            "토스: 2\n",
            "보기: 2\n",
            "개편: 2\n",
            "혜택: 2\n",
            "변화: 2\n",
            "누적: 1\n",
            "사용자: 1\n",
            "육박: 1\n",
            "교과서: 1\n",
            "플레이어: 1\n",
            "부상: 1\n",
            "삼성: 1\n",
            "전자: 1\n",
            "다시: 1\n",
            "서비스: 1\n",
            "이용자: 1\n",
            "화면: 1\n",
            "차지: 1\n",
            "슬슴슬금: 1\n",
            "비트코인: 1\n",
            "이유: 2\n",
            "대륙: 1\n",
            "실수: 1\n",
            "반복: 1\n",
            "실력: 1\n",
            "방심: 1\n",
            "금물: 1\n",
            "부당: 1\n",
            "이득: 1\n",
            "최대: 1\n",
            "벌금: 1\n",
            "질서: 1\n",
            "중인: 1\n",
            "가상: 1\n",
            "자산: 1\n",
            "제네시스: 1\n",
            "럭셔리: 1\n",
            "쿠페: 1\n",
            "출시: 1\n",
            "리움: 1\n",
            "지갑: 1\n",
            "달러: 4\n",
            "환율: 4\n",
            "연고: 2\n",
            "독주: 2\n",
            "인생: 2\n",
            "절반: 2\n",
            "나병환자: 2\n",
            "위해: 2\n",
            "명의: 2\n",
            "청년: 2\n",
            "의사: 2\n",
            "백양: 1\n",
            "라엘: 1\n",
            "대표: 1\n",
            "생리대: 1\n",
            "판매: 1\n",
            "글로벌: 1\n",
            "니스: 1\n",
            "케어: 1\n",
            "기업: 2\n",
            "도약: 1\n",
            "스킨: 1\n",
            "부스터: 1\n",
            "발주: 1\n",
            "자신감: 1\n",
            "테크: 1\n",
            "전문가: 1\n",
            "육성: 1\n",
            "한경: 1\n",
            "부산: 1\n",
            "상장: 1\n",
            "첫날: 1\n",
            "따블: 1\n",
            "기업인: 2\n",
            "출신: 2\n",
            "연세: 2\n",
            "크림: 2\n",
            "외화: 1\n",
            "쇼크: 1\n",
            "초비상: 1\n",
            "농업: 1\n",
            "고용: 1\n",
            "지표: 1\n",
            "대기: 1\n",
            "하락: 1\n",
            "위스키: 2\n",
            "발렌타인: 2\n",
            "상자: 2\n",
            "곰팡이: 2\n",
            "빗썸: 1\n",
            "수수료: 1\n",
            "무료: 1\n",
            "프로젝트: 1\n",
            "덕분: 1\n",
            "\n",
            "Total number of words: 145\n",
            "up : 58\n",
            "down : 87\n",
            "{'두산': -0.20000000000000007, '로보틱스': 0.19999999999999996, '대감': -0.4, '신고': -0.4, '경신': -1.2000000000000002, '집값': -0.4, '전세': -0.4, '욕망': -0.4, '거품': -0.4, '부동산': -0.4, '체인': 0.6, '데이터': 1.2, '코인': 0.6, '투자': 0.6, '토스': 1.2, '보기': 1.2, '개편': 1.2, '혜택': 1.2, '변화': 1.2, '누적': 0.6, '사용자': 0.6, '육박': 0.6, '교과서': 0.6, '플레이어': 0.6, '부상': 0.6, '삼성': 0.6, '전자': 0.6, '다시': 0.6, '서비스': -0.4, '이용자': -0.4, '화면': -0.4, '차지': -0.4, '슬슴슬금': -0.4, '비트코인': -0.4, '이유': 0.19999999999999996, '대륙': 0.6, '실수': 0.6, '반복': 0.6, '실력': 0.6, '방심': 0.6, '금물': 0.6, '부당': -0.4, '이득': -0.4, '최대': -0.4, '벌금': -0.4, '질서': -0.4, '중인': -0.4, '가상': -0.4, '자산': -0.4, '제네시스': -0.4, '럭셔리': -0.4, '쿠페': -0.4, '출시': -0.4, '리움': -0.4, '지갑': -0.4, '달러': -1.6, '환율': -0.6000000000000001, '연고': -0.8, '독주': -0.8, '인생': -0.8, '절반': -0.8, '나병환자': -0.8, '위해': -0.8, '명의': -0.8, '청년': -0.8, '의사': -0.8, '백양': -0.4, '라엘': -0.4, '대표': -0.4, '생리대': -0.4, '판매': -0.4, '글로벌': -0.4, '니스': -0.4, '케어': -0.4, '기업': 0.19999999999999996, '도약': -0.4, '스킨': 0.6, '부스터': 0.6, '발주': 0.6, '자신감': 0.6, '테크': 0.6, '전문가': 0.6, '육성': 0.6, '한경': 0.6, '부산': 0.6, '상장': 0.6, '첫날': 0.6, '따블': 0.6, '기업인': 1.2, '출신': 1.2, '연세': 1.2, '크림': 1.2, '외화': 0.6, '쇼크': 0.6, '초비상': 0.6, '농업': -0.4, '고용': -0.4, '지표': -0.4, '대기': -0.4, '하락': -0.4, '위스키': -0.8, '발렌타인': -0.8, '상자': -0.8, '곰팡이': -0.8, '빗썸': -0.4, '수수료': -0.4, '무료': -0.4, '프로젝트': -0.4, '덕분': -0.4}\n",
            "감성 사전 평균 점수 :  -1.7417260294578143e-16\n",
            "          date  Up/Down  sent_score  sent_label  \\\n",
            "0   2023-09-12        0     -0.3667           0   \n",
            "1   2023-09-12        0     -0.4000           0   \n",
            "2   2023-09-13        1      0.8400           1   \n",
            "3   2023-09-13        1      1.2000           1   \n",
            "4   2023-09-13        1      1.2000           1   \n",
            "5   2023-09-14        1      0.6000           1   \n",
            "6   2023-09-19        1      0.6000           1   \n",
            "7   2023-09-21        0     -0.4000           0   \n",
            "8   2023-09-22        0     -0.2000           0   \n",
            "9   2023-09-26        1      0.6000           1   \n",
            "10  2023-09-27        0     -0.4000           0   \n",
            "11  2023-09-27        0     -0.4000           0   \n",
            "12  2023-09-27        0     -0.4000           0   \n",
            "13  2023-09-27        0     -1.1000           0   \n",
            "14  2023-09-27        0     -0.8000           0   \n",
            "15  2023-09-27        0     -0.8000           0   \n",
            "16  2023-09-27        0     -1.1000           0   \n",
            "17  2023-10-04        0     -0.3400           0   \n",
            "18  2023-10-05        1      0.5200           1   \n",
            "19  2023-10-05        1      0.6000           1   \n",
            "20  2023-10-05        1      0.3600           1   \n",
            "21  2023-10-05        1      1.2000           1   \n",
            "22  2023-10-05        1      1.2000           1   \n",
            "23  2023-10-05        1      0.2800           1   \n",
            "24  2023-10-06        0     -0.4333           0   \n",
            "25  2023-10-06        0     -0.8000           0   \n",
            "26  2023-10-06        0     -0.4000           0   \n",
            "27  2023-10-06        0     -0.8000           0   \n",
            "\n",
            "                                       title  \\\n",
            "0                 두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1           집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2                 온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5            누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6                      삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7           \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8                     슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9                   '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12                        이더리움 NFT가 지갑이 된다고?   \n",
            "13         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델링\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 데이터 준비\n",
        "X_data = merged_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values\n",
        "Y_data = merged_df['sent_label'].values  # 기존의 'merged_df'를 사용\n",
        "\n",
        "# 2. 토큰화 및 패딩\n",
        "vocab_size = 2000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_data)\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=30)\n",
        "\n",
        "# 3. Bi-LSTM 모델 구축 및 훈련\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU0fANWF6cDB",
        "outputId": "089a7e74-6549-417e-b96e-21f24a16f8de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5000\n",
            "Epoch 1: val_acc improved from -inf to 0.66667, saving model to best_model.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6938 - acc: 0.5000 - val_loss: 0.6803 - val_acc: 0.6667\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6885 - acc: 0.5455\n",
            "Epoch 2: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6885 - acc: 0.5455 - val_loss: 0.6737 - val_acc: 0.6667\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6857 - acc: 0.5455"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6857 - acc: 0.5455 - val_loss: 0.6700 - val_acc: 0.6667\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.5455\n",
            "Epoch 4: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6833 - acc: 0.5455 - val_loss: 0.6679 - val_acc: 0.6667\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.5455\n",
            "Epoch 5: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6809 - acc: 0.5455 - val_loss: 0.6667 - val_acc: 0.6667\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.5455\n",
            "Epoch 6: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6782 - acc: 0.5455 - val_loss: 0.6658 - val_acc: 0.6667\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6750 - acc: 0.5455\n",
            "Epoch 7: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6750 - acc: 0.5455 - val_loss: 0.6652 - val_acc: 0.6667\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6712 - acc: 0.5455\n",
            "Epoch 8: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6712 - acc: 0.5455 - val_loss: 0.6646 - val_acc: 0.6667\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6668 - acc: 0.5455\n",
            "Epoch 9: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6668 - acc: 0.5455 - val_loss: 0.6640 - val_acc: 0.6667\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6614 - acc: 0.5455\n",
            "Epoch 10: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6614 - acc: 0.5455 - val_loss: 0.6634 - val_acc: 0.6667\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6549 - acc: 0.5455\n",
            "Epoch 11: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6549 - acc: 0.5455 - val_loss: 0.6627 - val_acc: 0.6667\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6472 - acc: 0.5455\n",
            "Epoch 12: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.6472 - acc: 0.5455 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6379 - acc: 0.5455\n",
            "Epoch 13: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6379 - acc: 0.5455 - val_loss: 0.6614 - val_acc: 0.6667\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6267 - acc: 0.6364\n",
            "Epoch 14: val_acc improved from 0.66667 to 0.83333, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6267 - acc: 0.6364 - val_loss: 0.6609 - val_acc: 0.8333\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6133 - acc: 0.8182\n",
            "Epoch 15: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6133 - acc: 0.8182 - val_loss: 0.6606 - val_acc: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 뉴스 수집 후 모델 통과시키기\n",
        "# 결과 확인\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from konlpy.tag import Okt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 4일전 ~ 1일전의 뉴스 제목 크롤링\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=4)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "current_date = start_date\n",
        "news_data = []\n",
        "\n",
        "# 크롤링 코드...\n",
        "\n",
        "# 데이터 정렬 및 저장\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 2. 명사 추출\n",
        "okt = Okt()\n",
        "news_df['nouns'] = news_df['title'].apply(lambda x: ', '.join(okt.nouns(x)))\n",
        "\n",
        "# 3. 감성 사전과 비교하여 감성 점수 계산\n",
        "news_df['filtered_nouns'] = news_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "news_df['sent_score'] = news_df['filtered_nouns'].apply(calculate_sentiment_score)  # 이전에 정의한 함수\n",
        "\n",
        "# 4. 예측을 위한 데이터 전처리\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(news_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values)\n",
        "X_test_padded = pad_sequences(X_test_tokenized, maxlen=30)\n",
        "\n",
        "# 5. 훈련된 Bi-LSTM 모델로 예측\n",
        "predicted = model.predict(X_test_padded)\n",
        "news_df['predicted_label'] = (predicted > 0.5).astype(int)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\n",
        "\n",
        "# 훈련 데이터와 검증 데이터에 대한 정확도 출력\n",
        "train_acc = history.history['acc'][-1]  # 훈련 데이터의 마지막 epoch의 정확도\n",
        "val_acc = history.history['val_acc'][-1]  # 검증 데이터의 마지막 epoch의 정확도\n",
        "\n",
        "print(f\"훈련 데이터 정확도: {train_acc:.4f}\")\n",
        "print(f\"검증 데이터 정확도: {val_acc:.4f}\")\n",
        "\n",
        "# 6. 미래 주식 시장 예측 결과 출력\n",
        "positive_news_ratio = news_df['predicted_label'].sum() / len(news_df)\n",
        "if positive_news_ratio > 0.5:\n",
        "    print(\"미래 주식 시장은 긍정적으로 움직일 것으로 예상됩니다.\")\n",
        "else:\n",
        "    print(\"미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\")\n",
        "\n",
        "pos_news = len(merged_df[merged_df['sent_score'] > sent_mean])\n",
        "neg_news = len(merged_df[merged_df['sent_score'] <= sent_mean])\n",
        "total_news = len(merged_df)\n",
        "\n",
        "print(f\"긍정적 뉴스 수: {pos_news} ({pos_news/total_news*100:.2f}%)\")\n",
        "print(f\"부정적 뉴스 수: {neg_news} ({neg_news/total_news*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n긍정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] > sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)\n",
        "\n",
        "print(\"\\n부정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] <= sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVhdxyFX83-V",
        "outputId": "6ad6212e-a991-44d1-f8eb-b33b89e63718"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 19ms/step\n",
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5971 - acc: 0.9091\n",
            "Epoch 1: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5971 - acc: 0.9091 - val_loss: 0.6603 - val_acc: 0.8333\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5778 - acc: 1.0000\n",
            "Epoch 2: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5778 - acc: 1.0000 - val_loss: 0.6599 - val_acc: 0.8333\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5549 - acc: 1.0000\n",
            "Epoch 3: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.5549 - acc: 1.0000 - val_loss: 0.6587 - val_acc: 0.8333\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5282 - acc: 1.0000\n",
            "Epoch 4: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.5282 - acc: 1.0000 - val_loss: 0.6584 - val_acc: 0.8333\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4975 - acc: 1.0000\n",
            "Epoch 5: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4975 - acc: 1.0000 - val_loss: 0.6386 - val_acc: 0.8333\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4652 - acc: 1.0000\n",
            "Epoch 6: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4652 - acc: 1.0000 - val_loss: 1.0218 - val_acc: 0.3333\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6120 - acc: 0.5000\n",
            "Epoch 7: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.6120 - acc: 0.5000 - val_loss: 0.6158 - val_acc: 0.8333\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.8636\n",
            "Epoch 8: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4534 - acc: 0.8636 - val_loss: 0.6321 - val_acc: 0.8333\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4208 - acc: 1.0000\n",
            "Epoch 9: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4208 - acc: 1.0000 - val_loss: 0.6398 - val_acc: 0.8333\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3981 - acc: 1.0000\n",
            "Epoch 10: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3981 - acc: 1.0000 - val_loss: 0.6440 - val_acc: 0.8333\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3763 - acc: 1.0000\n",
            "Epoch 11: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3763 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.8333\n",
            "Epoch 11: early stopping\n",
            "훈련 데이터 정확도: 1.0000\n",
            "검증 데이터 정확도: 0.8333\n",
            "미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\n",
            "긍정적 뉴스 수: 12 (42.86%)\n",
            "부정적 뉴스 수: 16 (57.14%)\n",
            "\n",
            "긍정적 뉴스 예시:\n",
            "- 온체인 데이터를 보면 코인 투자 데이터가 보인다\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 누적 사용자 100만명 육박…AI 교과서 키플레이어 부상\n",
            "- 삼성전자, 3주만에 다시 '6만전자'로\n",
            "\n",
            "부정적 뉴스 예시:\n",
            "- 두산, 두산로보틱스 IPO 기대감에 신고가 경신\n",
            "- 집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다\n",
            "- \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"\n",
            "- 슬슴슬금 오르는 비트코인, 이유가 뭘까?\n",
            "- “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산\n"
          ]
        }
      ]
    }
  ]
}