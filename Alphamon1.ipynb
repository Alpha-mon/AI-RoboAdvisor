{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPy9XlIh8DhY3ChUJbQrH4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alpha-mon/AI-RoboAdvisor/blob/main/Alphamon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 야후 파이낸스 주식 정보 가져오기\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'NVDA'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LYWreJgOIF-",
        "outputId": "aa782cc1-5d08-479d-ab85-28ceeb2df46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 알고리즘 전체 코드\n",
        "\n",
        "# Bollinger\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)"
      ],
      "metadata": {
        "id": "6xjMYRWXODZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O-IMmO7OZj3",
        "outputId": "72795fee-74b7-43a4-a6c8-5dbcba9624bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2014-02-13    43.8505 55.0000 40.3087 56.0362 50.1006 27.0795\n",
            "2014-02-14    28.7469 55.0000 40.3868 57.5436 51.5429 25.5513\n",
            "2014-02-18    42.4711 55.0000 40.3676 57.5078 53.3726 28.8534\n",
            "2014-02-19    51.9447 55.0000 40.4145 58.1338 54.9226 29.2618\n",
            "2014-02-20    47.6999 55.0000 40.4602 59.7165 56.5726 28.8429\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.6999 80.0000 32.6874 59.5827 25.8739 59.6536\n",
            "2023-10-05    47.6999 75.0000 43.8813 59.7339 26.7274 63.8320\n",
            "2023-10-06    47.6999 70.0000 48.7282 60.0288 28.9526 55.8796\n",
            "2023-10-09    47.6999 65.0000 48.5214 60.2380 31.8860 55.9784\n",
            "2023-10-10    47.6999 60.0000 57.2054 60.5174 35.5890 51.8735\n",
            "\n",
            "[2431 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "\n",
        "# 첫 번째 LSTM 레이어 (시퀀스 출력을 반환하여 다음 레이어로 전달)\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(T, 6)))\n",
        "\n",
        "# 두 번째 LSTM 레이어\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "\n",
        "# 세 번째 LSTM 레이어 (시퀀스 출력을 반환하지 않음)\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "\n",
        "# 출력 레이어\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx-EF5efOouf",
        "outputId": "9bc0159a-9b9e-4767-9b74-e0ed58bb5ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 6s 31ms/step - loss: 504.8314 - val_loss: 8547.4258\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 403.3673 - val_loss: 9506.1572\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 380.6676 - val_loss: 9572.8760\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 349.2497 - val_loss: 9695.2969\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 313.1483 - val_loss: 10939.1572\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 317.0918 - val_loss: 9302.9932\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 25ms/step - loss: 291.0522 - val_loss: 9176.7188\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 24ms/step - loss: 311.4862 - val_loss: 8884.0010\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 263.8724 - val_loss: 9107.6924\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 21ms/step - loss: 270.0659 - val_loss: 9011.0215\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 33092.5000\n",
            "Test Loss: 33092.5\n",
            "16/16 [==============================] - 1s 5ms/step\n",
            "[[ 45.181683 ]\n",
            " [ 47.567657 ]\n",
            " [ 47.595608 ]\n",
            " [ 49.325146 ]\n",
            " [ 51.853962 ]\n",
            " [ 52.998516 ]\n",
            " [ 61.934624 ]\n",
            " [ 63.608006 ]\n",
            " [ 59.74411  ]\n",
            " [ 40.13178  ]\n",
            " [ 47.317116 ]\n",
            " [ 45.021194 ]\n",
            " [ 46.727894 ]\n",
            " [ 50.101524 ]\n",
            " [ 45.749355 ]\n",
            " [ 46.98221  ]\n",
            " [ 55.58029  ]\n",
            " [ 58.631836 ]\n",
            " [ 62.85607  ]\n",
            " [ 68.70336  ]\n",
            " [ 70.48724  ]\n",
            " [ 64.88407  ]\n",
            " [ 53.76958  ]\n",
            " [ 43.96835  ]\n",
            " [ 35.421135 ]\n",
            " [ 34.029957 ]\n",
            " [ 31.0104   ]\n",
            " [ 31.72558  ]\n",
            " [ 33.666737 ]\n",
            " [ 31.050613 ]\n",
            " [ 32.331192 ]\n",
            " [ 30.042349 ]\n",
            " [ 29.459967 ]\n",
            " [ 32.486557 ]\n",
            " [ 36.919132 ]\n",
            " [ 39.372887 ]\n",
            " [ 33.493324 ]\n",
            " [ 24.94697  ]\n",
            " [ 25.795185 ]\n",
            " [ 25.947315 ]\n",
            " [ 23.899532 ]\n",
            " [ 19.778893 ]\n",
            " [ 19.305433 ]\n",
            " [ 17.58831  ]\n",
            " [ 17.751314 ]\n",
            " [ 18.9362   ]\n",
            " [ 17.771362 ]\n",
            " [ 15.275535 ]\n",
            " [ 14.505725 ]\n",
            " [ 16.068674 ]\n",
            " [ 18.985811 ]\n",
            " [ 22.262909 ]\n",
            " [ 23.504284 ]\n",
            " [ 27.006893 ]\n",
            " [ 32.094166 ]\n",
            " [ 36.221638 ]\n",
            " [ 40.80374  ]\n",
            " [ 49.03012  ]\n",
            " [ 58.655582 ]\n",
            " [ 61.938145 ]\n",
            " [ 64.6886   ]\n",
            " [ 67.214134 ]\n",
            " [ 52.141327 ]\n",
            " [ 53.019154 ]\n",
            " [ 49.246834 ]\n",
            " [ 46.806374 ]\n",
            " [ 38.66574  ]\n",
            " [ 26.15819  ]\n",
            " [ 20.727337 ]\n",
            " [ 16.864933 ]\n",
            " [ 14.596902 ]\n",
            " [ 14.209734 ]\n",
            " [ 17.175198 ]\n",
            " [ 24.684322 ]\n",
            " [ 24.6876   ]\n",
            " [ 27.13896  ]\n",
            " [ 30.00653  ]\n",
            " [ 31.642767 ]\n",
            " [ 38.89885  ]\n",
            " [ 50.854774 ]\n",
            " [ 12.643271 ]\n",
            " [ 46.3725   ]\n",
            " [ 57.149246 ]\n",
            " [ 62.48067  ]\n",
            " [ 78.74038  ]\n",
            " [ 68.66954  ]\n",
            " [ 61.292057 ]\n",
            " [ 73.018135 ]\n",
            " [ 92.64101  ]\n",
            " [113.00557  ]\n",
            " [111.65414  ]\n",
            " [ 58.33426  ]\n",
            " [ 20.67829  ]\n",
            " [ 19.109594 ]\n",
            " [ 23.777706 ]\n",
            " [ 29.91622  ]\n",
            " [ 32.240536 ]\n",
            " [ 35.593967 ]\n",
            " [ 41.653244 ]\n",
            " [ 43.254433 ]\n",
            " [ 44.944286 ]\n",
            " [ 49.58491  ]\n",
            " [ 52.53972  ]\n",
            " [ 56.82826  ]\n",
            " [ 72.37923  ]\n",
            " [ 78.20161  ]\n",
            " [ 72.958664 ]\n",
            " [ 69.88776  ]\n",
            " [ 59.77974  ]\n",
            " [ 56.490868 ]\n",
            " [ 52.395775 ]\n",
            " [ 46.870815 ]\n",
            " [ 31.316221 ]\n",
            " [ 38.00892  ]\n",
            " [ 68.80069  ]\n",
            " [ 95.1869   ]\n",
            " [117.49518  ]\n",
            " [127.22284  ]\n",
            " [134.47128  ]\n",
            " [127.645836 ]\n",
            " [116.92051  ]\n",
            " [115.381546 ]\n",
            " [124.92702  ]\n",
            " [109.28432  ]\n",
            " [ 96.77487  ]\n",
            " [ 81.312256 ]\n",
            " [ 77.15604  ]\n",
            " [ 55.362904 ]\n",
            " [ 51.95678  ]\n",
            " [ 45.547935 ]\n",
            " [ 41.72754  ]\n",
            " [ 34.181114 ]\n",
            " [ 34.527927 ]\n",
            " [ 36.753563 ]\n",
            " [ 41.18671  ]\n",
            " [ 43.011093 ]\n",
            " [ 42.037727 ]\n",
            " [ 39.454235 ]\n",
            " [ 37.695637 ]\n",
            " [ 33.551838 ]\n",
            " [ 24.79924  ]\n",
            " [ 19.12099  ]\n",
            " [ 21.022558 ]\n",
            " [ 22.581097 ]\n",
            " [ 27.210497 ]\n",
            " [ 34.552216 ]\n",
            " [ 45.639454 ]\n",
            " [ 55.748764 ]\n",
            " [ 58.680767 ]\n",
            " [ 63.440887 ]\n",
            " [ 66.93311  ]\n",
            " [ 68.36675  ]\n",
            " [ 71.28902  ]\n",
            " [ 73.16562  ]\n",
            " [ 72.85726  ]\n",
            " [ 72.40437  ]\n",
            " [ 68.75284  ]\n",
            " [ 65.85519  ]\n",
            " [ 62.099144 ]\n",
            " [ 49.927578 ]\n",
            " [ 35.859245 ]\n",
            " [ 34.225613 ]\n",
            " [ 44.123528 ]\n",
            " [ 30.594704 ]\n",
            " [ 30.644623 ]\n",
            " [ 31.671581 ]\n",
            " [ 35.470848 ]\n",
            " [ 37.646484 ]\n",
            " [ 39.076347 ]\n",
            " [ 38.775524 ]\n",
            " [ 43.720615 ]\n",
            " [ 59.21666  ]\n",
            " [ 53.267967 ]\n",
            " [ 30.73166  ]\n",
            " [ 15.506009 ]\n",
            " [ 11.899968 ]\n",
            " [ 10.676633 ]\n",
            " [ 10.923721 ]\n",
            " [ 18.273226 ]\n",
            " [ 23.769373 ]\n",
            " [ 26.536156 ]\n",
            " [ 30.43552  ]\n",
            " [ 33.364956 ]\n",
            " [ 40.757282 ]\n",
            " [ 56.86659  ]\n",
            " [ 64.39149  ]\n",
            " [ 66.237915 ]\n",
            " [ 60.94793  ]\n",
            " [ 58.524952 ]\n",
            " [ 66.57226  ]\n",
            " [ 73.340614 ]\n",
            " [ 72.741684 ]\n",
            " [ 71.12136  ]\n",
            " [ 63.439484 ]\n",
            " [ 59.385452 ]\n",
            " [ 49.656437 ]\n",
            " [ 44.134777 ]\n",
            " [ 43.09138  ]\n",
            " [ 40.907604 ]\n",
            " [ 40.90958  ]\n",
            " [ 43.595848 ]\n",
            " [ 46.660835 ]\n",
            " [ 48.62151  ]\n",
            " [ 47.11444  ]\n",
            " [ 47.302105 ]\n",
            " [ 46.147366 ]\n",
            " [ 39.069027 ]\n",
            " [ 35.527252 ]\n",
            " [ 36.17267  ]\n",
            " [ 37.79433  ]\n",
            " [ 39.729237 ]\n",
            " [ 42.769665 ]\n",
            " [ 48.18514  ]\n",
            " [ 55.562183 ]\n",
            " [ 58.813156 ]\n",
            " [ 55.026905 ]\n",
            " [ 68.917496 ]\n",
            " [ 61.15638  ]\n",
            " [ 51.765    ]\n",
            " [ 39.513557 ]\n",
            " [ 41.352398 ]\n",
            " [ 38.67898  ]\n",
            " [ 42.087048 ]\n",
            " [ 48.40051  ]\n",
            " [ 45.277363 ]\n",
            " [ 37.887737 ]\n",
            " [ 33.357216 ]\n",
            " [ 32.208477 ]\n",
            " [ 27.509554 ]\n",
            " [ 14.202583 ]\n",
            " [  7.9030857]\n",
            " [  9.030206 ]\n",
            " [ 12.845479 ]\n",
            " [ 12.329003 ]\n",
            " [ 11.25293  ]\n",
            " [ 12.300485 ]\n",
            " [ 13.837258 ]\n",
            " [ 14.102399 ]\n",
            " [ 16.305447 ]\n",
            " [ 24.422915 ]\n",
            " [ 39.003273 ]\n",
            " [ 53.16875  ]\n",
            " [ 64.49245  ]\n",
            " [ 67.57743  ]\n",
            " [ 66.84808  ]\n",
            " [ 64.89812  ]\n",
            " [ 64.77447  ]\n",
            " [ 59.438618 ]\n",
            " [ 59.65898  ]\n",
            " [ 60.892525 ]\n",
            " [ 61.09887  ]\n",
            " [ 62.132824 ]\n",
            " [ 62.340626 ]\n",
            " [ 63.854855 ]\n",
            " [ 63.579117 ]\n",
            " [ 67.732765 ]\n",
            " [ 69.328896 ]\n",
            " [ 72.48831  ]\n",
            " [ 69.73516  ]\n",
            " [ 55.154423 ]\n",
            " [ 55.70466  ]\n",
            " [ 54.442287 ]\n",
            " [ 59.993866 ]\n",
            " [ 66.815346 ]\n",
            " [ 66.088356 ]\n",
            " [ 63.598633 ]\n",
            " [ 57.94842  ]\n",
            " [ 55.346096 ]\n",
            " [ 50.667095 ]\n",
            " [ 45.796917 ]\n",
            " [ 44.349506 ]\n",
            " [ 45.5774   ]\n",
            " [ 46.877453 ]\n",
            " [ 47.674335 ]\n",
            " [ 47.182835 ]\n",
            " [ 46.799065 ]\n",
            " [ 46.626926 ]\n",
            " [ 46.10158  ]\n",
            " [ 45.326443 ]\n",
            " [ 42.91648  ]\n",
            " [ 43.059643 ]\n",
            " [ 43.78016  ]\n",
            " [ 44.968826 ]\n",
            " [ 46.59526  ]\n",
            " [ 47.632896 ]\n",
            " [ 47.26653  ]\n",
            " [ 46.166916 ]\n",
            " [ 41.41289  ]\n",
            " [ 35.07562  ]\n",
            " [ 28.670755 ]\n",
            " [ 20.899456 ]\n",
            " [ 16.509089 ]\n",
            " [ 18.283663 ]\n",
            " [ 22.440834 ]\n",
            " [ 23.550121 ]\n",
            " [ 21.984772 ]\n",
            " [ 21.647522 ]\n",
            " [ 21.50835  ]\n",
            " [ 20.5112   ]\n",
            " [ 20.569212 ]\n",
            " [ 22.027473 ]\n",
            " [ 19.49178  ]\n",
            " [ 18.579964 ]\n",
            " [ 17.58942  ]\n",
            " [ 16.752638 ]\n",
            " [ 17.361984 ]\n",
            " [ 23.63301  ]\n",
            " [ 29.928318 ]\n",
            " [ 35.249805 ]\n",
            " [ 41.796467 ]\n",
            " [ 47.971344 ]\n",
            " [ 54.07544  ]\n",
            " [ 54.72725  ]\n",
            " [ 53.47294  ]\n",
            " [ 56.626106 ]\n",
            " [ 60.773724 ]\n",
            " [ 64.94615  ]\n",
            " [ 64.357216 ]\n",
            " [ 59.44758  ]\n",
            " [ 55.06491  ]\n",
            " [ 56.24576  ]\n",
            " [ 55.79408  ]\n",
            " [ 55.5976   ]\n",
            " [ 56.71355  ]\n",
            " [ 56.271824 ]\n",
            " [ 51.650154 ]\n",
            " [ 46.002346 ]\n",
            " [ 43.0221   ]\n",
            " [ 43.897438 ]\n",
            " [ 44.809788 ]\n",
            " [ 45.43656  ]\n",
            " [ 46.899155 ]\n",
            " [ 48.189724 ]\n",
            " [ 44.558495 ]\n",
            " [ 44.304134 ]\n",
            " [ 46.369987 ]\n",
            " [ 45.409786 ]\n",
            " [ 44.876915 ]\n",
            " [ 46.32681  ]\n",
            " [ 45.536884 ]\n",
            " [ 44.65778  ]\n",
            " [ 42.39239  ]\n",
            " [ 35.94578  ]\n",
            " [ 35.95471  ]\n",
            " [ 33.057304 ]\n",
            " [ 30.34459  ]\n",
            " [ 19.16824  ]\n",
            " [ 23.064957 ]\n",
            " [ 24.378353 ]\n",
            " [ 34.41404  ]\n",
            " [ 40.197792 ]\n",
            " [ 44.830303 ]\n",
            " [ 53.647694 ]\n",
            " [ 58.5088   ]\n",
            " [ 60.660084 ]\n",
            " [ 59.25504  ]\n",
            " [ 55.226307 ]\n",
            " [ 53.73534  ]\n",
            " [ 53.505215 ]\n",
            " [ 53.001884 ]\n",
            " [ 54.02965  ]\n",
            " [ 54.813503 ]\n",
            " [ 53.59906  ]\n",
            " [ 47.677666 ]\n",
            " [ 43.003025 ]\n",
            " [ 39.962975 ]\n",
            " [ 35.462162 ]\n",
            " [ 33.31992  ]\n",
            " [ 35.78193  ]\n",
            " [ 37.887657 ]\n",
            " [ 37.304047 ]\n",
            " [ 27.895107 ]\n",
            " [ 16.881012 ]\n",
            " [ 15.529186 ]\n",
            " [ 14.739023 ]\n",
            " [ 14.370227 ]\n",
            " [ 14.203014 ]\n",
            " [ 14.094489 ]\n",
            " [ 13.862482 ]\n",
            " [ 12.996808 ]\n",
            " [ 12.561824 ]\n",
            " [ 12.500961 ]\n",
            " [ 14.409971 ]\n",
            " [ 16.26583  ]\n",
            " [ 17.811346 ]\n",
            " [ 20.816303 ]\n",
            " [ 32.733097 ]\n",
            " [ 41.329937 ]\n",
            " [ 47.77917  ]\n",
            " [ 52.13168  ]\n",
            " [ 53.0167   ]\n",
            " [ 49.13821  ]\n",
            " [ 47.523018 ]\n",
            " [ 49.322773 ]\n",
            " [ 47.80348  ]\n",
            " [ 49.154823 ]\n",
            " [ 49.890205 ]\n",
            " [ 50.571117 ]\n",
            " [ 63.700237 ]\n",
            " [ 52.525867 ]\n",
            " [ 55.719322 ]\n",
            " [ 59.155838 ]\n",
            " [ 65.043465 ]\n",
            " [ 73.11643  ]\n",
            " [ 38.54199  ]\n",
            " [ 27.696001 ]\n",
            " [ 25.402414 ]\n",
            " [ 40.213764 ]\n",
            " [ 54.33208  ]\n",
            " [ 60.60332  ]\n",
            " [ 61.493393 ]\n",
            " [ 61.06663  ]\n",
            " [ 78.98534  ]\n",
            " [ 60.53443  ]\n",
            " [ 49.277084 ]\n",
            " [ 43.6256   ]\n",
            " [ 44.878933 ]\n",
            " [ 46.560856 ]\n",
            " [ 49.955868 ]\n",
            " [ 52.85985  ]\n",
            " [ 55.4594   ]\n",
            " [ 53.484676 ]\n",
            " [ 48.264984 ]\n",
            " [ 37.139618 ]\n",
            " [ 21.581673 ]\n",
            " [ 20.585794 ]\n",
            " [ 29.772995 ]\n",
            " [ 35.05259  ]\n",
            " [ 38.457233 ]\n",
            " [ 42.13844  ]\n",
            " [ 44.059696 ]\n",
            " [ 41.126305 ]\n",
            " [ 41.82082  ]\n",
            " [ 38.890537 ]\n",
            " [ 36.308533 ]\n",
            " [ 34.756428 ]\n",
            " [ 34.653515 ]\n",
            " [ 35.612568 ]\n",
            " [ 36.358204 ]\n",
            " [ 34.927834 ]\n",
            " [ 30.33867  ]\n",
            " [ 17.242422 ]\n",
            " [ 15.732726 ]\n",
            " [ 17.797306 ]\n",
            " [ 19.126852 ]\n",
            " [ 21.750975 ]\n",
            " [ 22.714733 ]\n",
            " [ 24.20082  ]\n",
            " [ 25.279694 ]\n",
            " [ 25.910248 ]\n",
            " [ 26.41224  ]\n",
            " [ 26.282143 ]\n",
            " [ 22.788286 ]\n",
            " [ 18.475769 ]\n",
            " [ 17.29392  ]\n",
            " [ 16.496233 ]\n",
            " [ 16.374624 ]\n",
            " [ 16.85974  ]\n",
            " [ 16.790781 ]\n",
            " [ 20.213291 ]\n",
            " [ 34.232677 ]\n",
            " [ 45.95084  ]\n",
            " [ 54.846344 ]\n",
            " [ 59.13849  ]\n",
            " [ 61.33663  ]\n",
            " [ 62.204002 ]\n",
            " [ 59.024254 ]\n",
            " [ 53.26522  ]\n",
            " [ 50.371082 ]\n",
            " [ 44.478985 ]\n",
            " [ 31.819931 ]\n",
            " [ 21.168402 ]\n",
            " [ 23.900047 ]\n",
            " [ 29.896374 ]\n",
            " [ 37.45901  ]\n",
            " [ 51.44046  ]\n",
            " [105.67581  ]\n",
            " [141.1581   ]\n",
            " [129.21735  ]\n",
            " [ 71.46738  ]\n",
            " [ 43.815266 ]\n",
            " [ 43.477856 ]\n",
            " [ 39.750095 ]\n",
            " [ 27.003536 ]\n",
            " [ 22.152164 ]]\n",
            "     Predictions\n",
            "0        45.1817\n",
            "1        47.5677\n",
            "2        47.5956\n",
            "3        49.3251\n",
            "4        51.8540\n",
            "..           ...\n",
            "480      43.8153\n",
            "481      43.4779\n",
            "482      39.7501\n",
            "483      27.0035\n",
            "484      22.1522\n",
            "\n",
            "[485 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLWQhGpOO3Cu",
        "outputId": "bdf04155-ea5b-45ab-fc50-9d1833b4baaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7b743ab9e2f2>:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)"
      ],
      "metadata": {
        "id": "aRP4uYp6O4JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")"
      ],
      "metadata": {
        "id": "RaqE9b67O8Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "#학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "#학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juv4MC4gPDVw",
        "outputId": "485b886a-06dd-4a30-c51a-34e9858220d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 1s 848ms/step\n",
            "0/10 [D loss: 0.6527732610702515 | D accuracy: 51.5625] [G loss: 0.6945138573646545]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.6584954559803009 | D accuracy: 78.125] [G loss: 0.6950043439865112]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "2/10 [D loss: 0.6436459124088287 | D accuracy: 98.4375] [G loss: 0.6960358619689941]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "3/10 [D loss: 0.6232345700263977 | D accuracy: 100.0] [G loss: 0.6969425678253174]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "4/10 [D loss: 0.618502289056778 | D accuracy: 100.0] [G loss: 0.6974769830703735]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "5/10 [D loss: 0.6075031459331512 | D accuracy: 100.0] [G loss: 0.6976855993270874]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "6/10 [D loss: 0.5987242460250854 | D accuracy: 100.0] [G loss: 0.6983477473258972]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "7/10 [D loss: 0.5874119997024536 | D accuracy: 100.0] [G loss: 0.6980558633804321]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.5759556293487549 | D accuracy: 100.0] [G loss: 0.6985049247741699]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "9/10 [D loss: 0.5653042495250702 | D accuracy: 100.0] [G loss: 0.6986566781997681]\n",
            "16/16 [==============================] - 0s 6ms/step\n",
            "16/16 [==============================] - 1s 12ms/step\n",
            "[[22.591293 22.576921 22.565102 ... 22.646496 22.666279 22.669437]\n",
            " [23.78428  23.769909 23.75809  ... 23.839483 23.859266 23.862425]\n",
            " [23.798256 23.783884 23.772064 ... 23.853458 23.873241 23.8764  ]\n",
            " ...\n",
            " [19.8755   19.861128 19.849308 ... 19.930702 19.950485 19.953644]\n",
            " [13.50222  13.487847 13.476028 ... 13.557423 13.577206 13.580364]\n",
            " [11.076534 11.062161 11.050343 ... 11.131737 11.15152  11.154678]]\n",
            "22.245869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrG64NUKN5ro",
        "outputId": "d9e56986-7a31-4f39-da24-8b795ae1bc6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "            Bollinger    MACD     MOK     RSI    STCK      WR\n",
            "Date                                                         \n",
            "2013-12-06    47.4883 60.0000 54.5310 65.5699 70.6979 34.4688\n",
            "2013-12-09    47.4883 55.0000 55.4552 66.6116 72.1410 38.1999\n",
            "2013-12-10    47.4883 50.0000 55.3249 66.3360 73.8539 41.9822\n",
            "2013-12-11    47.4883 45.0000 55.3510 65.0548 75.1676 45.3457\n",
            "2013-12-12    47.4883 40.0000 54.8473 64.8099 76.5296 44.1786\n",
            "...               ...     ...     ...     ...     ...     ...\n",
            "2023-10-04    47.4883 80.0000 47.7812 52.8057 13.6627 60.1780\n",
            "2023-10-05    47.4883 80.0000 51.7933 52.8968 15.0374 59.0812\n",
            "2023-10-06    47.4883 80.0000 51.0233 53.2469 17.6592 58.7956\n",
            "2023-10-09    47.4883 75.0000 51.8338 53.9705 20.9058 58.6424\n",
            "2023-10-10    47.4883 70.0000 57.8723 53.8735 24.7798 59.4091\n",
            "\n",
            "[2477 rows x 6 columns]\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 2s 13ms/step - loss: 288.5013 - val_loss: 5907.2471\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 147.4686 - val_loss: 5986.4375\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 142.5626 - val_loss: 5876.0371\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 136.5634 - val_loss: 6070.0464\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 128.3015 - val_loss: 5954.5669\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.2404 - val_loss: 6035.2227\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 127.6632 - val_loss: 5800.6040\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 129.5344 - val_loss: 6073.8711\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 132.1304 - val_loss: 5958.4849\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 117.2527 - val_loss: 5754.4131\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 12979.8340\n",
            "Test Loss: 12979.833984375\n",
            "16/16 [==============================] - 0s 3ms/step\n",
            "     Predictions\n",
            "0        33.2402\n",
            "1        30.5310\n",
            "2        32.8486\n",
            "3        31.9444\n",
            "4        39.3545\n",
            "..           ...\n",
            "489      69.7483\n",
            "490      71.1996\n",
            "491      64.5924\n",
            "492      51.1719\n",
            "493      33.6937\n",
            "\n",
            "[494 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-74ded9a2326e>:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.array(final_df_sequences)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "0/10 [D loss: 0.749887079000473 | D accuracy: 45.3125] [G loss: 0.6953793168067932]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/10 [D loss: 0.7350181043148041 | D accuracy: 50.0] [G loss: 0.696094274520874]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "2/10 [D loss: 0.7098847925662994 | D accuracy: 50.0] [G loss: 0.6962449550628662]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "3/10 [D loss: 0.6899526715278625 | D accuracy: 82.8125] [G loss: 0.6970466375350952]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "4/10 [D loss: 0.674744725227356 | D accuracy: 98.4375] [G loss: 0.6967859268188477]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "5/10 [D loss: 0.6626344323158264 | D accuracy: 96.875] [G loss: 0.6973535418510437]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "6/10 [D loss: 0.6468399465084076 | D accuracy: 98.4375] [G loss: 0.6977488994598389]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "7/10 [D loss: 0.6356022953987122 | D accuracy: 98.4375] [G loss: 0.697795033454895]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "8/10 [D loss: 0.6208434998989105 | D accuracy: 96.875] [G loss: 0.6997222900390625]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "9/10 [D loss: 0.6132535338401794 | D accuracy: 96.875] [G loss: 0.6994189023971558]\n",
            "16/16 [==============================] - 0s 5ms/step\n",
            "16/16 [==============================] - 2s 26ms/step\n",
            "[[16.666477 16.668842 16.680546 ... 16.65741  16.657124 16.659262]\n",
            " [15.311872 15.314238 15.325941 ... 15.302805 15.302518 15.304657]\n",
            " [16.47065  16.473015 16.484718 ... 16.461582 16.461296 16.463434]\n",
            " ...\n",
            " [32.34256  32.344925 32.35663  ... 32.333492 32.333206 32.335346]\n",
            " [25.632305 25.63467  25.646374 ... 25.623238 25.622952 25.62509 ]\n",
            " [16.89319  16.895555 16.907259 ... 16.884123 16.883837 16.885975]]\n",
            "23.329542\n"
          ]
        }
      ],
      "source": [
        "# 주가 예측 전체 코드\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 주식 정보 가져오기\n",
        "ticker = 'AAPL'\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=3650)\n",
        "data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "closing_prices = data['Close']\n",
        "volume = data['Volume']\n",
        "moving_average = closing_prices.rolling(window=120).mean()\n",
        "\n",
        "\n",
        "# Bollinger Bands 계산 함수\n",
        "\n",
        "def calculate_bollinger_bands(closing_prices, window=20, num_std=2):\n",
        "    rolling_mean = closing_prices.rolling(window=window).mean()\n",
        "    rolling_std = closing_prices.rolling(window=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + num_std * rolling_std\n",
        "    lower_band = rolling_mean - num_std * rolling_std\n",
        "    return upper_band, lower_band\n",
        "\n",
        "upper_band, lower_band = calculate_bollinger_bands(closing_prices)\n",
        "bollinger_bands_data = pd.DataFrame({'Upper Band': upper_band, 'Lower Band': lower_band})\n",
        "\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "result = np.where(closing_prices > upper_band, (closing_prices - upper_band) / (upper_band - lower_band), np.where(closing_prices < lower_band, (closing_prices - lower_band) / (lower_band - upper_band), np.nan))\n",
        "bollinger_bands_data['Result'] = pd.Series(result, index=closing_prices.index)\n",
        "\n",
        "bollinger_bands_data['Result'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "bollinger_bands_data['Prediction'] = bollinger_bands_data['Result'].shift(-1)\n",
        "bollinger_bands_data.dropna(inplace=True)\n",
        "bollinger_bands_data['Trend'] = bollinger_bands_data['Prediction'].diff()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] + 1) / 2\n",
        "\n",
        "min_pred = bollinger_bands_data['Prediction'].min()\n",
        "max_pred = bollinger_bands_data['Prediction'].max()\n",
        "bollinger_bands_data['Prediction'] = (bollinger_bands_data['Prediction'] - min_pred) / (max_pred - min_pred)\n",
        "\n",
        "min_trend = bollinger_bands_data['Trend'].min()\n",
        "max_trend = bollinger_bands_data['Trend'].max()\n",
        "bollinger_bands_data['Trend'] = (bollinger_bands_data['Trend'] - min_trend) / (max_trend - min_trend)*100\n",
        "\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "\n",
        "\n",
        "# MACD 계산 함수\n",
        "\n",
        "def calculate_macd(closing_prices):\n",
        "\n",
        "    short_ema = closing_prices.ewm(span=26, adjust=False).mean()\n",
        "    long_ema = closing_prices.ewm(span=12, adjust=False).mean()\n",
        "\n",
        "    dif = short_ema - long_ema\n",
        "    signal_line = dif.ewm(span=9, adjust=False).mean()\n",
        "    histogram = dif - signal_line\n",
        "\n",
        "    return dif, signal_line, histogram\n",
        "\n",
        "dif, signal_line, histogram = calculate_macd(closing_prices)\n",
        "macd_data = pd.DataFrame({'DIF': dif, 'Signal Line': signal_line, 'Histogram': histogram})\n",
        "macd_data['Result'] = np.where(macd_data['DIF'] > macd_data['Signal Line'], 1, 0)\n",
        "\n",
        "macd_data['Result'] = (macd_data['Result'].rolling(window=20, min_periods=1).mean())*100\n",
        "macd_data['Prediction'] = macd_data['Result'].shift(-1)\n",
        "macd_data.dropna(inplace=True)\n",
        "\n",
        "# MOK\n",
        "\n",
        "def calculate_mok(closing_prices, period=14, ma_period=20):\n",
        "    returns = closing_prices.pct_change()\n",
        "\n",
        "    moving_average = closing_prices.rolling(window=ma_period).mean()\n",
        "    momentum = closing_prices.diff(period)\n",
        "    normalized_momentum = 100 * (momentum - np.min(momentum)) / (np.max(momentum) - np.min(momentum))\n",
        "\n",
        "    return normalized_momentum\n",
        "\n",
        "mok_values = calculate_mok(closing_prices)\n",
        "\n",
        "# RSI\n",
        "\n",
        "def calculate_rsi(closing_prices, window=20):\n",
        "    price_changes = closing_prices.diff()\n",
        "    up_changes = price_changes.where(price_changes > 0, 0)\n",
        "    down_changes = -price_changes.where(price_changes < 0, 0)\n",
        "\n",
        "    avg_up_changes = up_changes.rolling(window=window, min_periods=1).mean()\n",
        "    avg_down_changes = down_changes.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_up_changes / avg_down_changes\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "rsi_result = calculate_rsi(closing_prices, window=len(closing_prices)//10)  # 기간을 데이터의 1/10로 동적으로 설정\n",
        "rsi_result.dropna(inplace=True)\n",
        "\n",
        "# STCK\n",
        "\n",
        "def calculate_stck(closing_prices, window=20):\n",
        "    lowest_low = closing_prices.rolling(window=window).min()\n",
        "    highest_high = closing_prices.rolling(window=window).max()\n",
        "\n",
        "    stck = 100 * (closing_prices - lowest_low) / (highest_high - lowest_low)\n",
        "\n",
        "    stck_ma = stck.rolling(window=20).mean()\n",
        "\n",
        "    return stck_ma\n",
        "\n",
        "stck_result = calculate_stck(closing_prices)\n",
        "stck_result.dropna(inplace=True)\n",
        "\n",
        "# WR\n",
        "\n",
        "def calculate_williams_r(closing_prices, period=14):\n",
        "    high_prices = data['High']\n",
        "    low_prices = data['Low']\n",
        "    moving_average = closing_prices.rolling(window=20).mean()\n",
        "\n",
        "    highest_high = high_prices.rolling(window=period).max()\n",
        "    lowest_low = low_prices.rolling(window=period).min()\n",
        "\n",
        "    williams_r = (highest_high - moving_average) / (highest_high - lowest_low) * -100\n",
        "\n",
        "    normalized_williams_r = 100 * (williams_r + 100) / 100\n",
        "\n",
        "    return normalized_williams_r\n",
        "\n",
        "wr_result = calculate_williams_r(closing_prices)\n",
        "\n",
        "\n",
        "# 6개 지표들을 하나의 데이터 프레임에 합치기\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    'Bollinger': bollinger_bands_data['Trend'],\n",
        "    'MACD': macd_data['Prediction'],\n",
        "    'MOK': mok_values,\n",
        "    'RSI': rsi_result,\n",
        "    'STCK': stck_result,\n",
        "    'WR': wr_result\n",
        "})\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "# 최종 데이터 프레임 출력\n",
        "print(final_df)\n",
        "\n",
        "\n",
        "final_df_values = final_df.values\n",
        "data_values = data.values\n",
        "\n",
        "# 입력 시퀀스에 대한 타임 스텝(T)을 정의합니다.\n",
        "\n",
        "T = 10  # 원하는대로 조정할 수 있습니다.\n",
        "\n",
        "# 입력 및 타겟을 위한 데이터 시퀀스 생성\n",
        "final_df_sequences = []\n",
        "data_sequences = []\n",
        "\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "    data_sequences.append(closing_prices.iloc[i+T])\n",
        "\n",
        "filtered_final_df_sequences = []\n",
        "filtered_data_sequences = []\n",
        "\n",
        "# 시퀀스 길이 맞추기\n",
        "for i, seq in enumerate(final_df_sequences):\n",
        "    if len(seq) == 10:\n",
        "        filtered_final_df_sequences.append(seq)\n",
        "        filtered_data_sequences.append(data_sequences[i])\n",
        "\n",
        "final_df_sequences = filtered_final_df_sequences\n",
        "data_sequences = filtered_data_sequences\n",
        "\n",
        "# 시퀀스를 넘파이 배열로 변환\n",
        "X = np.array(final_df_sequences)\n",
        "y = np.array(data_sequences)\n",
        "\n",
        "\n",
        "# 데이터를 훈련 및 테스트 세트로 분할\n",
        "split_ratio = 0.8  # 분할 비율을 조정할 수 있습니다.\n",
        "split_index = int(split_ratio * len(X))\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# LSTM 모델 구축\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(T, 6)))  # num_features는 지표의 수입니다.\n",
        "model.add(Dense(1))  # 주식 가격 예측을 위한 뉴런 하나를 가진 출력 레이어\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 테스트 데이터에서 모델 평가\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# 예측 수행\n",
        "predictions = model.predict(X_test)\n",
        "#print(predictions)\n",
        "\n",
        "# 예측 결과를 데이터 프레임으로 변환\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n",
        "print(predictions_df)\n",
        "\n",
        "# LSTM 모델의 예측 값을 6개 지표 값과 함께 생성 모델 입력값으로 넣기 위해\n",
        "\n",
        "# T 타임 스텝 만큼의 길이를 고려하여 LSTM 예측값을 올바른 위치에 넣습니다.\n",
        "final_df[\"LSTM_Predictions\"] = np.nan\n",
        "final_df[\"LSTM_Predictions\"].iloc[-len(predictions_df):] = predictions_df[\"Predictions\"].values\n",
        "\n",
        "# NaN 값을 제거합니다.\n",
        "final_df.dropna(inplace=True)\n",
        "\n",
        "# 다시 시퀀스로 변환\n",
        "final_df_values = final_df.values\n",
        "\n",
        "final_df_sequences = []\n",
        "for i in range(len(data_values) - T):\n",
        "    final_df_sequences.append(final_df_values[i:i+T])\n",
        "\n",
        "X = np.array(final_df_sequences)\n",
        "\n",
        "# 생성 / 구분 모델\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Flatten, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 생성 모델 G (주식 가격의 시퀀스를 생성)\n",
        "def build_generator(input_shape=(T, 7)):  # 입력 값 : LSTM 모델 예측 + 6개 지표 값\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 BatchNormalization, LeakyReLU 활성화 함수를 이용하여 시퀀스를 학습\n",
        "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    # Flatten layer를 제거하고, T 길이의 시퀀스를 생성하도록 수정\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(T, activation='linear'))\n",
        "\n",
        "    noise = tf.keras.layers.Input(shape=input_shape)\n",
        "    generated_sequence = model(noise)\n",
        "\n",
        "    return Model(noise, generated_sequence)\n",
        "\n",
        "# 구분 모델 D (주가의 실제 시퀀스와 생성된 시퀀스를 구분)\n",
        "def build_discriminator(input_shape=(T, 1)):\n",
        "    model = Sequential()\n",
        "\n",
        "    # LSTM 레이어와 LeakyReLU 활성화 함수를 사용하여 시퀀스를 처리\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    sequence = tf.keras.layers.Input(shape=input_shape)\n",
        "    validity = model(sequence)\n",
        "\n",
        "    # 주어진 시퀀스가 실제인지 생성된 것인지에 대한 확률\n",
        "    return Model(sequence, validity)\n",
        "\n",
        "# GAN 학습 함수 (생성된 주식 가격 시퀀스와 실제 주식 가격 시퀀스를 사용하여 판별자를 학습)\n",
        "def train_gan(generator, discriminator, combined, epochs, batch_size=32):\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        sequences = X_train[idx]\n",
        "\n",
        "        # 주식 예측값 생성\n",
        "        predicted_stock = model.predict(sequences)\n",
        "\n",
        "        # LSTM 예측값을 sequences에 추가\n",
        "        predicted_stock_reshaped = predicted_stock.reshape(batch_size, 1, 1)  # (batch_size, 1, 1) 형태로 변환\n",
        "        predicted_stock_expanded = np.repeat(predicted_stock_reshaped, T, axis=1)  # predicted_stock_reshaped를 T 타임 스텝만큼 확장\n",
        "        X_train_combined = np.concatenate([sequences, predicted_stock_expanded], axis=2)\n",
        "\n",
        "        generated_stock = generator.predict(X_train_combined)\n",
        "        generated_stock_reshaped = generated_stock.reshape(batch_size, T, 1)\n",
        "\n",
        "        # 판별자 학습\n",
        "        d_loss_real = discriminator.train_on_batch(predicted_stock_expanded, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_stock_reshaped, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # 생성자 학습\n",
        "        g_loss = combined.train_on_batch(X_train_combined, valid)\n",
        "\n",
        "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "\n",
        "# 판별자 및 생성자 모델 초기화, 생성자와 판별자를 결합하여 GAN 모델 만들\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "z = tf.keras.layers.Input(shape=(T, 7))\n",
        "generated_sequence = generator(z)\n",
        "\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(generated_sequence)\n",
        "\n",
        "combined = Model(z, validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# GAN 학습 시작\n",
        "train_gan(generator, discriminator, combined, epochs=10, batch_size=32)\n",
        "\n",
        "# GAN 학습 후 최종 예측을 수행하는 함수\n",
        "def final_predictions(generator, test_data, batch_size=32):\n",
        "    predicted_stock_lstm = model.predict(test_data)\n",
        "\n",
        "    # LSTM 예측값을 test_data에 추가\n",
        "    predicted_stock_expanded = np.repeat(predicted_stock_lstm[:, np.newaxis], T, axis=1)  # (batch_size, T)로 형태 변경\n",
        "    test_data_combined = np.concatenate([test_data, predicted_stock_expanded], axis=2)\n",
        "\n",
        "    # G 모델을 사용하여 합성 주가를 생성\n",
        "    generated_stock = generator.predict(test_data_combined)\n",
        "\n",
        "    # LSTM 예측과 G 모델의 예측을 평균\n",
        "    final_predicted_stock = (predicted_stock_lstm + generated_stock.mean(axis=1)) / 2.0\n",
        "\n",
        "    return final_predicted_stock\n",
        "\n",
        "# GAN 학습 후 최종 예측\n",
        "after_gan_predictions = final_predictions(generator, X_test, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(after_gan_predictions)\n",
        "\n",
        "average_prediction = np.mean(after_gan_predictions)\n",
        "print(average_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인기 주식 검색 목록 -> 분마다 새로운 값으로 자동 업데이트\n",
        "# 주식 이름과 순위 변동을 출력\n",
        "\n",
        "# 1. 필요한 라이브러리 설치\n",
        "!pip install beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# 웹 크롤링을 위한 함수 정의\n",
        "def get_naver_stock_list():\n",
        "    URL = \"https://finance.naver.com/sise/lastsearch2.nhn\"  # 네이버 증권 인기검색주식 URL\n",
        "\n",
        "    response = requests.get(URL)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    stock_list = []\n",
        "\n",
        "    for item in soup.select(\"a.tltle\"):\n",
        "        stock_list.append(item.text)\n",
        "\n",
        "    return stock_list\n",
        "\n",
        "prev_stock_list = []  # 이전 주식 리스트 초기화\n",
        "\n",
        "# 주기적인 데이터 수집 및 처리\n",
        "while True:\n",
        "    stocks = get_naver_stock_list()\n",
        "    top_10_stocks = stocks[:10]  # 처음 10개의 주식만 선택\n",
        "\n",
        "    rank_changes = []\n",
        "    for stock in top_10_stocks:\n",
        "        if stock in prev_stock_list:\n",
        "            # 순위의 변화 계산\n",
        "            rank_changes.append(prev_stock_list.index(stock) - top_10_stocks.index(stock))\n",
        "        else:\n",
        "            # 새롭게 진입한 주식은 0으로 처리\n",
        "            rank_changes.append(0)\n",
        "\n",
        "    # 데이터 프레임으로 변환\n",
        "    df = pd.DataFrame({\n",
        "        'Stock Name': top_10_stocks,\n",
        "        'Rank Change': rank_changes\n",
        "    })\n",
        "    print(df)\n",
        "\n",
        "    prev_stock_list = top_10_stocks.copy()  # 현재 주식 리스트를 이전 주식 리스트로 저장\n",
        "\n",
        "    time.sleep(60)  # 매 분마다 데이터 수집"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "h1DU9CsIDOy9",
        "outputId": "fcb53e5b-2245-41f1-c08f-f1a97890ed99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Stock Name  Rank Change\n",
            "0       삼성전자            0\n",
            "1       에코프로            0\n",
            "2     에코프로비엠            0\n",
            "3   LG에너지솔루션            0\n",
            "4   POSCO홀딩스            0\n",
            "5     두산로보틱스            0\n",
            "6      동운아나텍            0\n",
            "7     포스코퓨처엠            0\n",
            "8     신성델타테크            0\n",
            "9       신신제약            0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9768915086bd>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprev_stock_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_10_stocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 현재 주식 리스트를 이전 주식 리스트로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 매 분마다 데이터 수집\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwEYuJCr7ot0",
        "outputId": "dd2e7df0-a450-40bd-a547-5bb470ae38d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 제목, 일자 크롤링 성공\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "\n",
        "# 1. User-Agent를 설정합니다.\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# 시작 날짜와 종료 날짜 설정\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=30)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=5)\n",
        "\n",
        "current_date = start_date\n",
        "\n",
        "news_data = []\n",
        "\n",
        "while current_date <= end_date:\n",
        "    # 2. 요청을 보낼 때 headers를 추가합니다.\n",
        "    response = requests.get(base_url + current_date.strftime('%Y%m%d'), headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    for item in soup.select(\".cluster_text a\"):\n",
        "        title = item.text.strip()\n",
        "        if item.attrs[\"href\"].startswith(\"http\"):\n",
        "            news_url = item.attrs[\"href\"]\n",
        "        else:\n",
        "            news_url = \"https:\" + item.attrs[\"href\"]\n",
        "\n",
        "        detail_response = requests.get(news_url, headers=headers)  # headers 추가\n",
        "        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
        "        date_element = detail_soup.select_one(\"span.media_end_head_info_datestamp_time\")\n",
        "        date = date_element.attrs[\"data-date-time\"].split()[0]\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'date': date\n",
        "        })\n",
        "\n",
        "        # 요청 간에 약간의 지연을 두어 IP 차단을 피합니다.\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    # 다음 날짜로 이동\n",
        "    current_date += datetime.timedelta(days=1)\n",
        "\n",
        "# 뉴스 데이터를 날짜 순으로 정렬\n",
        "news_data_sorted = sorted(news_data, key=lambda x: x['date'])\n",
        "\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 시작 날짜와 종료 날짜를 문자열로 변환\n",
        "start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "# 원하는 날짜 범위만 선택\n",
        "news_df = news_df[(news_df['date'] >= start_date_str) & (news_df['date'] <= end_date_str)]\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt 객체 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 제목에서 명사만 추출하는 함수\n",
        "def extract_nouns(title):\n",
        "    return ', '.join(okt.nouns(title))\n",
        "\n",
        "# 'title' 열의 각 제목에 대하여 명사만 추출\n",
        "news_df['nouns'] = news_df['title'].apply(extract_nouns)\n",
        "\n",
        "print(news_df)\n",
        "\n",
        "# 코스피 종가 가져오기\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_kospi_closing_prices():\n",
        "    url = \"https://finance.naver.com/sise/sise_index_day.nhn?code=KOSPI\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    kospi_closings = []\n",
        "\n",
        "    for i in range(1, 7):  # 네이버 금융은 한 페이지에 6일치 데이터를 보여줍니다. 6페이지(36일치)를 가져옵니다.\n",
        "        response = requests.get(url + f\"&page={i}\", headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        dates = soup.select(\".date\")\n",
        "        closings = soup.select(\".number_1\")\n",
        "\n",
        "        for d, c in zip(dates, closings[::4]):  # 종가만 가져오기 위해 slicing 사용\n",
        "            kospi_closings.append([d.text.strip(), float(c.text.replace(',', ''))])\n",
        "\n",
        "    return kospi_closings\n",
        "\n",
        "kospi_data = get_kospi_closing_prices()\n",
        "df = pd.DataFrame(kospi_data, columns=[\"Date\", \"Closing\"])\n",
        "\n",
        "# Shift를 사용해 다음 날짜의 종가를 가져와서 현재 날짜와 비교\n",
        "df[\"Up/Down\"] = (df[\"Closing\"].shift(1) > df[\"Closing\"]).astype(int)\n",
        "\n",
        "# 날짜 기준으로 최근 30일의 데이터를 가져온 후, 정렬\n",
        "df = df.sort_values(by=\"Date\").tail(30).reset_index(drop=True)\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "print(df)\n",
        "\n",
        "merged_df = pd.merge(news_df, df, left_on='date', right_on='Date', how='inner')\n",
        "merged_df = merged_df[['date', 'Up/Down', 'title', 'nouns']]\n",
        "print(merged_df)\n",
        "\n",
        "# 단어 점수 초기화\n",
        "\n",
        "# 'nouns' 칼럼의 값을 문자열로 변환\n",
        "df['nouns'] = merged_df['nouns'].astype(str)\n",
        "df = df.dropna(subset=['nouns'])\n",
        "\n",
        "# 'filtered_nouns' 컬럼 생성\n",
        "merged_df['filtered_nouns'] = merged_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word_list in merged_df['filtered_nouns'] for word in word_list}\n",
        "\n",
        "# 단어 빈도수 계산\n",
        "from collections import Counter\n",
        "word_counts = Counter(word for word_list in merged_df['filtered_nouns'] for word in word_list)\n",
        "\n",
        "# 결과 출력\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# 전체 단어 개수 출력\n",
        "total_words = sum(word_counts.values())\n",
        "print(f\"\\nTotal number of words: {total_words}\")\n",
        "\n",
        "# Up/Down 값이 1인 데이터에서 포함된 단어의 리스트\n",
        "up = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 1]['filtered_nouns']:\n",
        "    up.extend(nouns)\n",
        "\n",
        "# Up/Down 값이 0인 데이터에서 포함된 단어의 리스트\n",
        "down = []\n",
        "for nouns in merged_df[merged_df['Up/Down'] == 0]['filtered_nouns']:\n",
        "    down.extend(nouns)\n",
        "\n",
        "print(\"up :\", len(up))\n",
        "print(\"down :\", len(down))\n",
        "\n",
        "# 상승 비율과 하락 비율 계산\n",
        "total_words = len(up) + len(down)\n",
        "up_ratio = len(up) / total_words\n",
        "down_ratio = len(down) / total_words\n",
        "\n",
        "# 단어 점수 초기화\n",
        "word_scores = {word: 0 for word in word_scores.keys()}  # 기존의 word_scores 딕셔너리 사용\n",
        "\n",
        "# Up(1) 데이터의 단어들에 대해서 하락 비율을 더해주기\n",
        "for word in up:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] += down_ratio\n",
        "\n",
        "# Down(0) 데이터의 단어들에 대해서 상승 비율을 차감해주기\n",
        "for word in down:\n",
        "    if word in word_scores:\n",
        "        word_scores[word] -= up_ratio\n",
        "\n",
        "# 결과 확인\n",
        "print(word_scores)\n",
        "\n",
        "total = []\n",
        "for nouns in merged_df['filtered_nouns']:\n",
        "    sent_score = 0\n",
        "    for noun in nouns:\n",
        "        if noun in word_scores:\n",
        "            sent_score += word_scores[noun]\n",
        "\n",
        "    # 해당 뉴스 제목에 포함된 단어의 수로 나누어 평균 점수를 계산\n",
        "    avg_sent_score = sent_score / len(nouns) if nouns else 0  # 단어가 없는 경우 0으로 처리\n",
        "    total.append(avg_sent_score)\n",
        "\n",
        "merged_df['sent_score'] = total\n",
        "\n",
        "# 감성사전의 평균 점수 계산\n",
        "\n",
        "sent_mean = sum(word_scores.values()) / len(word_scores)\n",
        "print('감성 사전 평균 점수 : ',sent_mean)\n",
        "\n",
        "# 감성 점수 계산\n",
        "def calculate_sentiment_score(noun_list):\n",
        "    score = 0\n",
        "    for noun in noun_list:\n",
        "        if noun in word_scores:\n",
        "            score += word_scores[noun]\n",
        "    return score / (len(noun_list) if len(noun_list) != 0 else 1)\n",
        "\n",
        "merged_df['sent_score'] = merged_df['filtered_nouns'].apply(calculate_sentiment_score)\n",
        "\n",
        "# 평균 점수를 기준으로 라벨링\n",
        "merged_df['sent_label'] = merged_df['sent_score'].apply(lambda x: 1 if x > sent_mean else 0)\n",
        "\n",
        "\n",
        "\n",
        "result_df = merged_df[['date', 'Up/Down', 'sent_score', 'sent_label', 'title', 'nouns']]\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDDG0VxG6TjR",
        "outputId": "369904ca-56b5-4dcc-9ffc-ffc8c251480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date                                     title  \\\n",
            "7   2023-09-12                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "8   2023-09-12          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "9   2023-09-13                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "10  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "11  2023-09-13               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "12  2023-09-14           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "13  2023-09-19                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "14  2023-09-21          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "15  2023-09-22                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "16  2023-09-26                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "17  2023-09-27          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "18  2023-09-27                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "19  2023-09-27                        이더리움 NFT가 지갑이 된다고?   \n",
            "20  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "21  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "22  2023-09-27      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "23  2023-09-27         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "24  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "25  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "26  2023-10-03        온라인 패션시장 뒤흔든 '쩐의 전쟁'…2년 후 '1강2중2약'   \n",
            "27  2023-10-03                공부하던 고시생, 기타 메고 방방곡곡 무대 위로   \n",
            "28  2023-10-04  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "29  2023-10-05                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "30  2023-10-05               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "31  2023-10-05                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "32  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "33  2023-10-05          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "34  2023-10-05       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "35  2023-10-06            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "36  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "37  2023-10-06                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "38  2023-10-06        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "39  2023-10-07     'LG트윈스' 우승하면 준다 했는데…금고 속 '롤렉스·소주' 나오나   \n",
            "\n",
            "                                             nouns  \n",
            "7                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "8                              집값, 전세, 욕망, 거품, 부동산  \n",
            "9                             체인, 데이터, 코인, 투자, 데이터  \n",
            "10                              토스, 보기, 개편, 혜택, 변화  \n",
            "11                              토스, 보기, 개편, 혜택, 변화  \n",
            "12                   누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "13                                삼성, 전자, 주, 다시, 로  \n",
            "14                       개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "15                               슬슴슬금, 비트코인, 이유, 뭘  \n",
            "16                          대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "17               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "18                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "19                                       더, 리움, 지갑  \n",
            "20                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "21                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "22                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "23                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "24                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "25               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "26               온라인, 패션, 시장, 뒤, 쩐, 전쟁, 후, 강, 중, 약  \n",
            "27                       공부, 고시생, 기타, 방방곡곡, 무대, 위로  \n",
            "28  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "29                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "30                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "31                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "32                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "33                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "34                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "35                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "36                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "37                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "38                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "39                             트윈스, 금고, 속, 롤렉스, 소주  \n",
            "          Date   Closing  Up/Down\n",
            "0   2023-08-25 2519.1400        1\n",
            "1   2023-08-28 2543.4100        1\n",
            "2   2023-08-29 2552.1600        1\n",
            "3   2023-08-30 2561.2200        0\n",
            "4   2023-08-31 2556.2700        1\n",
            "5   2023-09-01 2563.7100        1\n",
            "6   2023-09-04 2584.5500        0\n",
            "7   2023-09-05 2582.1800        0\n",
            "8   2023-09-06 2563.3400        0\n",
            "9   2023-09-07 2548.2600        0\n",
            "10  2023-09-08 2547.6800        1\n",
            "11  2023-09-11 2556.8800        0\n",
            "12  2023-09-12 2536.5800        0\n",
            "13  2023-09-13 2534.7000        1\n",
            "14  2023-09-14 2572.8900        1\n",
            "15  2023-09-15 2601.2800        0\n",
            "16  2023-09-18 2574.7200        0\n",
            "17  2023-09-19 2559.2100        1\n",
            "18  2023-09-20 2559.7400        0\n",
            "19  2023-09-21 2514.9700        0\n",
            "20  2023-09-22 2508.1300        0\n",
            "21  2023-09-25 2495.7600        0\n",
            "22  2023-09-26 2462.9700        1\n",
            "23  2023-09-27 2465.0700        0\n",
            "24  2023-10-04 2405.6900        0\n",
            "25  2023-10-05 2403.6000        1\n",
            "26  2023-10-06 2408.7300        0\n",
            "27  2023-10-10 2402.5800        1\n",
            "28  2023-10-11 2450.0800        1\n",
            "29  2023-10-12 2479.8200        0\n",
            "          date  Up/Down                                     title  \\\n",
            "0   2023-09-12        0                두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1   2023-09-12        0          집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2   2023-09-13        1                온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4   2023-09-13        1               토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5   2023-09-14        1           누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6   2023-09-19        1                     삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7   2023-09-21        0          \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8   2023-09-22        0                    슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9   2023-09-26        1                  '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10  2023-09-27        0          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11  2023-09-27        0                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12  2023-09-27        0                        이더리움 NFT가 지갑이 된다고?   \n",
            "13  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15  2023-09-27        0      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16  2023-09-27        0         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  2023-10-04        0  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18  2023-10-05        1                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19  2023-10-05        1               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20  2023-10-05        1                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22  2023-10-05        1          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23  2023-10-05        1       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24  2023-10-06        0            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26  2023-10-06        0                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27  2023-10-06        0        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "두산: 3\n",
            "로보틱스: 2\n",
            "대감: 1\n",
            "신고: 1\n",
            "경신: 3\n",
            "집값: 1\n",
            "전세: 1\n",
            "욕망: 1\n",
            "거품: 1\n",
            "부동산: 1\n",
            "체인: 1\n",
            "데이터: 2\n",
            "코인: 1\n",
            "투자: 1\n",
            "토스: 2\n",
            "보기: 2\n",
            "개편: 2\n",
            "혜택: 2\n",
            "변화: 2\n",
            "누적: 1\n",
            "사용자: 1\n",
            "육박: 1\n",
            "교과서: 1\n",
            "플레이어: 1\n",
            "부상: 1\n",
            "삼성: 1\n",
            "전자: 1\n",
            "다시: 1\n",
            "서비스: 1\n",
            "이용자: 1\n",
            "화면: 1\n",
            "차지: 1\n",
            "슬슴슬금: 1\n",
            "비트코인: 1\n",
            "이유: 2\n",
            "대륙: 1\n",
            "실수: 1\n",
            "반복: 1\n",
            "실력: 1\n",
            "방심: 1\n",
            "금물: 1\n",
            "부당: 1\n",
            "이득: 1\n",
            "최대: 1\n",
            "벌금: 1\n",
            "질서: 1\n",
            "중인: 1\n",
            "가상: 1\n",
            "자산: 1\n",
            "제네시스: 1\n",
            "럭셔리: 1\n",
            "쿠페: 1\n",
            "출시: 1\n",
            "리움: 1\n",
            "지갑: 1\n",
            "달러: 4\n",
            "환율: 4\n",
            "연고: 2\n",
            "독주: 2\n",
            "인생: 2\n",
            "절반: 2\n",
            "나병환자: 2\n",
            "위해: 2\n",
            "명의: 2\n",
            "청년: 2\n",
            "의사: 2\n",
            "백양: 1\n",
            "라엘: 1\n",
            "대표: 1\n",
            "생리대: 1\n",
            "판매: 1\n",
            "글로벌: 1\n",
            "니스: 1\n",
            "케어: 1\n",
            "기업: 2\n",
            "도약: 1\n",
            "스킨: 1\n",
            "부스터: 1\n",
            "발주: 1\n",
            "자신감: 1\n",
            "테크: 1\n",
            "전문가: 1\n",
            "육성: 1\n",
            "한경: 1\n",
            "부산: 1\n",
            "상장: 1\n",
            "첫날: 1\n",
            "따블: 1\n",
            "기업인: 2\n",
            "출신: 2\n",
            "연세: 2\n",
            "크림: 2\n",
            "외화: 1\n",
            "쇼크: 1\n",
            "초비상: 1\n",
            "농업: 1\n",
            "고용: 1\n",
            "지표: 1\n",
            "대기: 1\n",
            "하락: 1\n",
            "위스키: 2\n",
            "발렌타인: 2\n",
            "상자: 2\n",
            "곰팡이: 2\n",
            "빗썸: 1\n",
            "수수료: 1\n",
            "무료: 1\n",
            "프로젝트: 1\n",
            "덕분: 1\n",
            "\n",
            "Total number of words: 145\n",
            "up : 58\n",
            "down : 87\n",
            "{'두산': -0.20000000000000007, '로보틱스': 0.19999999999999996, '대감': -0.4, '신고': -0.4, '경신': -1.2000000000000002, '집값': -0.4, '전세': -0.4, '욕망': -0.4, '거품': -0.4, '부동산': -0.4, '체인': 0.6, '데이터': 1.2, '코인': 0.6, '투자': 0.6, '토스': 1.2, '보기': 1.2, '개편': 1.2, '혜택': 1.2, '변화': 1.2, '누적': 0.6, '사용자': 0.6, '육박': 0.6, '교과서': 0.6, '플레이어': 0.6, '부상': 0.6, '삼성': 0.6, '전자': 0.6, '다시': 0.6, '서비스': -0.4, '이용자': -0.4, '화면': -0.4, '차지': -0.4, '슬슴슬금': -0.4, '비트코인': -0.4, '이유': 0.19999999999999996, '대륙': 0.6, '실수': 0.6, '반복': 0.6, '실력': 0.6, '방심': 0.6, '금물': 0.6, '부당': -0.4, '이득': -0.4, '최대': -0.4, '벌금': -0.4, '질서': -0.4, '중인': -0.4, '가상': -0.4, '자산': -0.4, '제네시스': -0.4, '럭셔리': -0.4, '쿠페': -0.4, '출시': -0.4, '리움': -0.4, '지갑': -0.4, '달러': -1.6, '환율': -0.6000000000000001, '연고': -0.8, '독주': -0.8, '인생': -0.8, '절반': -0.8, '나병환자': -0.8, '위해': -0.8, '명의': -0.8, '청년': -0.8, '의사': -0.8, '백양': -0.4, '라엘': -0.4, '대표': -0.4, '생리대': -0.4, '판매': -0.4, '글로벌': -0.4, '니스': -0.4, '케어': -0.4, '기업': 0.19999999999999996, '도약': -0.4, '스킨': 0.6, '부스터': 0.6, '발주': 0.6, '자신감': 0.6, '테크': 0.6, '전문가': 0.6, '육성': 0.6, '한경': 0.6, '부산': 0.6, '상장': 0.6, '첫날': 0.6, '따블': 0.6, '기업인': 1.2, '출신': 1.2, '연세': 1.2, '크림': 1.2, '외화': 0.6, '쇼크': 0.6, '초비상': 0.6, '농업': -0.4, '고용': -0.4, '지표': -0.4, '대기': -0.4, '하락': -0.4, '위스키': -0.8, '발렌타인': -0.8, '상자': -0.8, '곰팡이': -0.8, '빗썸': -0.4, '수수료': -0.4, '무료': -0.4, '프로젝트': -0.4, '덕분': -0.4}\n",
            "감성 사전 평균 점수 :  -1.7417260294578143e-16\n",
            "          date  Up/Down  sent_score  sent_label  \\\n",
            "0   2023-09-12        0     -0.3667           0   \n",
            "1   2023-09-12        0     -0.4000           0   \n",
            "2   2023-09-13        1      0.8400           1   \n",
            "3   2023-09-13        1      1.2000           1   \n",
            "4   2023-09-13        1      1.2000           1   \n",
            "5   2023-09-14        1      0.6000           1   \n",
            "6   2023-09-19        1      0.6000           1   \n",
            "7   2023-09-21        0     -0.4000           0   \n",
            "8   2023-09-22        0     -0.2000           0   \n",
            "9   2023-09-26        1      0.6000           1   \n",
            "10  2023-09-27        0     -0.4000           0   \n",
            "11  2023-09-27        0     -0.4000           0   \n",
            "12  2023-09-27        0     -0.4000           0   \n",
            "13  2023-09-27        0     -1.1000           0   \n",
            "14  2023-09-27        0     -0.8000           0   \n",
            "15  2023-09-27        0     -0.8000           0   \n",
            "16  2023-09-27        0     -1.1000           0   \n",
            "17  2023-10-04        0     -0.3400           0   \n",
            "18  2023-10-05        1      0.5200           1   \n",
            "19  2023-10-05        1      0.6000           1   \n",
            "20  2023-10-05        1      0.3600           1   \n",
            "21  2023-10-05        1      1.2000           1   \n",
            "22  2023-10-05        1      1.2000           1   \n",
            "23  2023-10-05        1      0.2800           1   \n",
            "24  2023-10-06        0     -0.4333           0   \n",
            "25  2023-10-06        0     -0.8000           0   \n",
            "26  2023-10-06        0     -0.4000           0   \n",
            "27  2023-10-06        0     -0.8000           0   \n",
            "\n",
            "                                       title  \\\n",
            "0                 두산, 두산로보틱스 IPO 기대감에 신고가 경신   \n",
            "1           집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다   \n",
            "2                 온체인 데이터를 보면 코인 투자 데이터가 보인다   \n",
            "3                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "4                토스 '만보기' 개편된다고?… 혜택 변화 따져보니   \n",
            "5            누적 사용자 100만명 육박…AI 교과서 키플레이어 부상   \n",
            "6                      삼성전자, 3주만에 다시 '6만전자'로   \n",
            "7           \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"   \n",
            "8                     슬슴슬금 오르는 비트코인, 이유가 뭘까?   \n",
            "9                   '대륙의 실수' 반복되면 실력… 방심은 금물   \n",
            "10          “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산   \n",
            "11                  100만대 팔린 제네시스…럭셔리 쿠페도 출시   \n",
            "12                        이더리움 NFT가 지갑이 된다고?   \n",
            "13         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "14      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "15      인생의 절반을 나병환자 위해… 명의로 불리는 72세 '청년 의사'   \n",
            "16         원/달러 환율 1350원 연고점 경신… 강달러 독주 이어지나   \n",
            "17  백양희 라엘 대표 \"생리대 판매 1위 넘어…글로벌 웰니스케어 기업 도약\"   \n",
            "18                   '스킨부스터' 후발주자의 이유 있는 자신감   \n",
            "19               “핀테크 전문가 육성”…한경TV-부산대 손 잡았다   \n",
            "20                 IPO 대어 두산로보틱스, 상장 첫날 ‘따블’   \n",
            "21          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "22          '기업인 출신'이 일냈다…'연세크림빵' 대박나자 벌어진 일   \n",
            "23       \"환율 왜 이래?\"…'210조' 외화빚 쇼크에 기업들 '초비상'   \n",
            "24            美 9월 비농업고용 지표 대기…환율 1340원대로 하락   \n",
            "25        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "26                빗썸 수수료 무료, 830 프로젝트 덕분이라고?   \n",
            "27        20만원짜리 위스키 ‘발렌타인 23년’ 상자에 곰팡이가 웬말?   \n",
            "\n",
            "                                             nouns  \n",
            "0                         두산, 두산, 로보틱스, 대감, 신고, 경신  \n",
            "1                              집값, 전세, 욕망, 거품, 부동산  \n",
            "2                             체인, 데이터, 코인, 투자, 데이터  \n",
            "3                               토스, 보기, 개편, 혜택, 변화  \n",
            "4                               토스, 보기, 개편, 혜택, 변화  \n",
            "5                    누적, 사용자, 육박, 교과서, 키, 플레이어, 부상  \n",
            "6                                 삼성, 전자, 주, 다시, 로  \n",
            "7                        개, 서비스, 이용자, 첫, 화면, 차지, 것  \n",
            "8                                슬슴슬금, 비트코인, 이유, 뭘  \n",
            "9                           대륙, 실수, 반복, 실력, 방심, 금물  \n",
            "10               부당, 이득, 최대, 배, 벌금, 질서, 중인, 가상, 자산  \n",
            "11                               제네시스, 럭셔리, 쿠페, 출시  \n",
            "12                                       더, 리움, 지갑  \n",
            "13                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "14                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "15                 인생, 절반, 나병환자, 위해, 명의, 세, 청년, 의사  \n",
            "16                 원, 달러, 환율, 연고, 점, 경신, 강, 달러, 독주  \n",
            "17  백양, 라엘, 대표, 생리대, 판매, 위, 글로벌, 웰, 니스, 케어, 기업, 도약  \n",
            "18                         스킨, 부스터, 후, 발주, 이유, 자신감  \n",
            "19                       핀, 테크, 전문가, 육성, 한경, 부산, 손  \n",
            "20                            두산, 로보틱스, 상장, 첫날, 따블  \n",
            "21                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "22                     기업인, 출신, 이, 일, 연세, 크림, 빵, 일  \n",
            "23                       환율, 왜, 외화, 빚, 쇼크, 기업, 초비상  \n",
            "24                       비, 농업, 고용, 지표, 대기, 환율, 하락  \n",
            "25                           위스키, 발렌타인, 상자, 곰팡이, 말  \n",
            "26                           빗썸, 수수료, 무료, 프로젝트, 덕분  \n",
            "27                           위스키, 발렌타인, 상자, 곰팡이, 말  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델링\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 데이터 준비\n",
        "X_data = merged_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values\n",
        "Y_data = merged_df['sent_label'].values  # 기존의 'merged_df'를 사용\n",
        "\n",
        "# 2. 토큰화 및 패딩\n",
        "vocab_size = 2000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_data)\n",
        "X_padded = pad_sequences(X_tokenized, maxlen=30)\n",
        "\n",
        "# 3. Bi-LSTM 모델 구축 및 훈련\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU0fANWF6cDB",
        "outputId": "089a7e74-6549-417e-b96e-21f24a16f8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5000\n",
            "Epoch 1: val_acc improved from -inf to 0.66667, saving model to best_model.h5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.6938 - acc: 0.5000 - val_loss: 0.6803 - val_acc: 0.6667\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6885 - acc: 0.5455\n",
            "Epoch 2: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6885 - acc: 0.5455 - val_loss: 0.6737 - val_acc: 0.6667\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6857 - acc: 0.5455"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6857 - acc: 0.5455 - val_loss: 0.6700 - val_acc: 0.6667\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.5455\n",
            "Epoch 4: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6833 - acc: 0.5455 - val_loss: 0.6679 - val_acc: 0.6667\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.5455\n",
            "Epoch 5: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6809 - acc: 0.5455 - val_loss: 0.6667 - val_acc: 0.6667\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.5455\n",
            "Epoch 6: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6782 - acc: 0.5455 - val_loss: 0.6658 - val_acc: 0.6667\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6750 - acc: 0.5455\n",
            "Epoch 7: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6750 - acc: 0.5455 - val_loss: 0.6652 - val_acc: 0.6667\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6712 - acc: 0.5455\n",
            "Epoch 8: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6712 - acc: 0.5455 - val_loss: 0.6646 - val_acc: 0.6667\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6668 - acc: 0.5455\n",
            "Epoch 9: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6668 - acc: 0.5455 - val_loss: 0.6640 - val_acc: 0.6667\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6614 - acc: 0.5455\n",
            "Epoch 10: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6614 - acc: 0.5455 - val_loss: 0.6634 - val_acc: 0.6667\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6549 - acc: 0.5455\n",
            "Epoch 11: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6549 - acc: 0.5455 - val_loss: 0.6627 - val_acc: 0.6667\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6472 - acc: 0.5455\n",
            "Epoch 12: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.6472 - acc: 0.5455 - val_loss: 0.6620 - val_acc: 0.6667\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6379 - acc: 0.5455\n",
            "Epoch 13: val_acc did not improve from 0.66667\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.6379 - acc: 0.5455 - val_loss: 0.6614 - val_acc: 0.6667\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6267 - acc: 0.6364\n",
            "Epoch 14: val_acc improved from 0.66667 to 0.83333, saving model to best_model.h5\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6267 - acc: 0.6364 - val_loss: 0.6609 - val_acc: 0.8333\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6133 - acc: 0.8182\n",
            "Epoch 15: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6133 - acc: 0.8182 - val_loss: 0.6606 - val_acc: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 뉴스 수집 후 모델 통과시키기\n",
        "# 결과 확인\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from konlpy.tag import Okt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 4일전 ~ 1일전의 뉴스 제목 크롤링\n",
        "base_url = \"https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101&date=\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "start_date = datetime.datetime.now() - datetime.timedelta(days=4)\n",
        "end_date = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "current_date = start_date\n",
        "news_data = []\n",
        "\n",
        "# 크롤링 코드...\n",
        "\n",
        "# 데이터 정렬 및 저장\n",
        "news_df = pd.DataFrame(news_data_sorted, columns=['date', 'title'])\n",
        "\n",
        "# 2. 명사 추출\n",
        "okt = Okt()\n",
        "news_df['nouns'] = news_df['title'].apply(lambda x: ', '.join(okt.nouns(x)))\n",
        "\n",
        "# 3. 감성 사전과 비교하여 감성 점수 계산\n",
        "news_df['filtered_nouns'] = news_df['nouns'].apply(lambda x: [word for word in x.split(', ') if len(word) > 1])\n",
        "news_df['sent_score'] = news_df['filtered_nouns'].apply(calculate_sentiment_score)  # 이전에 정의한 함수\n",
        "\n",
        "# 4. 예측을 위한 데이터 전처리\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(news_df['nouns'].apply(lambda x: ' '.join([word for word in x.split(', ') if len(word) > 1])).values)\n",
        "X_test_padded = pad_sequences(X_test_tokenized, maxlen=30)\n",
        "\n",
        "# 5. 훈련된 Bi-LSTM 모델로 예측\n",
        "predicted = model.predict(X_test_padded)\n",
        "news_df['predicted_label'] = (predicted > 0.5).astype(int)\n",
        "\n",
        "history = model.fit(X_padded, Y_data, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)\n",
        "\n",
        "# 훈련 데이터와 검증 데이터에 대한 정확도 출력\n",
        "train_acc = history.history['acc'][-1]  # 훈련 데이터의 마지막 epoch의 정확도\n",
        "val_acc = history.history['val_acc'][-1]  # 검증 데이터의 마지막 epoch의 정확도\n",
        "\n",
        "print(f\"훈련 데이터 정확도: {train_acc:.4f}\")\n",
        "print(f\"검증 데이터 정확도: {val_acc:.4f}\")\n",
        "\n",
        "# 6. 미래 주식 시장 예측 결과 출력\n",
        "positive_news_ratio = news_df['predicted_label'].sum() / len(news_df)\n",
        "if positive_news_ratio > 0.5:\n",
        "    print(\"미래 주식 시장은 긍정적으로 움직일 것으로 예상됩니다.\")\n",
        "else:\n",
        "    print(\"미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\")\n",
        "\n",
        "pos_news = len(merged_df[merged_df['sent_score'] > sent_mean])\n",
        "neg_news = len(merged_df[merged_df['sent_score'] <= sent_mean])\n",
        "total_news = len(merged_df)\n",
        "\n",
        "print(f\"긍정적 뉴스 수: {pos_news} ({pos_news/total_news*100:.2f}%)\")\n",
        "print(f\"부정적 뉴스 수: {neg_news} ({neg_news/total_news*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n긍정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] > sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)\n",
        "\n",
        "print(\"\\n부정적 뉴스 예시:\")\n",
        "for title in merged_df[merged_df['sent_score'] <= sent_mean]['title'].head(5):\n",
        "    print(\"-\", title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVhdxyFX83-V",
        "outputId": "6ad6212e-a991-44d1-f8eb-b33b89e63718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 19ms/step\n",
            "Epoch 1/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5971 - acc: 0.9091\n",
            "Epoch 1: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.5971 - acc: 0.9091 - val_loss: 0.6603 - val_acc: 0.8333\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5778 - acc: 1.0000\n",
            "Epoch 2: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5778 - acc: 1.0000 - val_loss: 0.6599 - val_acc: 0.8333\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5549 - acc: 1.0000\n",
            "Epoch 3: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.5549 - acc: 1.0000 - val_loss: 0.6587 - val_acc: 0.8333\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5282 - acc: 1.0000\n",
            "Epoch 4: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.5282 - acc: 1.0000 - val_loss: 0.6584 - val_acc: 0.8333\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4975 - acc: 1.0000\n",
            "Epoch 5: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4975 - acc: 1.0000 - val_loss: 0.6386 - val_acc: 0.8333\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4652 - acc: 1.0000\n",
            "Epoch 6: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4652 - acc: 1.0000 - val_loss: 1.0218 - val_acc: 0.3333\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6120 - acc: 0.5000\n",
            "Epoch 7: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.6120 - acc: 0.5000 - val_loss: 0.6158 - val_acc: 0.8333\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.8636\n",
            "Epoch 8: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4534 - acc: 0.8636 - val_loss: 0.6321 - val_acc: 0.8333\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4208 - acc: 1.0000\n",
            "Epoch 9: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4208 - acc: 1.0000 - val_loss: 0.6398 - val_acc: 0.8333\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3981 - acc: 1.0000\n",
            "Epoch 10: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.3981 - acc: 1.0000 - val_loss: 0.6440 - val_acc: 0.8333\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3763 - acc: 1.0000\n",
            "Epoch 11: val_acc did not improve from 0.83333\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.3763 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.8333\n",
            "Epoch 11: early stopping\n",
            "훈련 데이터 정확도: 1.0000\n",
            "검증 데이터 정확도: 0.8333\n",
            "미래 주식 시장은 부정적으로 움직일 것으로 예상됩니다.\n",
            "긍정적 뉴스 수: 12 (42.86%)\n",
            "부정적 뉴스 수: 16 (57.14%)\n",
            "\n",
            "긍정적 뉴스 예시:\n",
            "- 온체인 데이터를 보면 코인 투자 데이터가 보인다\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 토스 '만보기' 개편된다고?… 혜택 변화 따져보니\n",
            "- 누적 사용자 100만명 육박…AI 교과서 키플레이어 부상\n",
            "- 삼성전자, 3주만에 다시 '6만전자'로\n",
            "\n",
            "부정적 뉴스 예시:\n",
            "- 두산, 두산로보틱스 IPO 기대감에 신고가 경신\n",
            "- 집값 올리는 전세, 욕망의 '거품' 부동산 멈추면 사라진다\n",
            "- \"6000개 AI 서비스로 이용자 '첫 화면' 차지할 것\"\n",
            "- 슬슴슬금 오르는 비트코인, 이유가 뭘까?\n",
            "- “부당 이득에 최대 5배 벌금”…질서 만드는 중인 가상자산\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자로부터 보유 주식 입력 받기\n",
        "stock_list = input(\"보유하고 있는 주식의 티커 목록을 쉼표로 구분하여 입력하세요 (예: AAPL, GOOGL, MSFT): \").split(\",\")\n",
        "stock_list = [stock.strip() for stock in stock_list]\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "# 주식 데이터 가져오기\n",
        "def fetch_data(tickers):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.Ticker(ticker)\n",
        "        data[ticker] = stock_data.history(period=\"1y\")  # 최근 1년간의 데이터 가져오기\n",
        "    return data\n",
        "\n",
        "stock_data = fetch_data(stock_list)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 수익률 및 변동성 계산\n",
        "returns = {}\n",
        "for stock, data in stock_data.items():\n",
        "    prices = data['Close'].values.astype(float)\n",
        "    daily_returns = prices[1:] / prices[:-1] - 1\n",
        "    returns[stock] = daily_returns\n",
        "\n",
        "# LSTM 모델 학습을 위한 데이터 준비\n",
        "look_back = 5\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset) - look_back - 1):\n",
        "        a = dataset[i:(i + look_back)]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# LSTM 모델 학습\n",
        "models = {}\n",
        "for stock, daily_returns in returns.items():\n",
        "    # 데이터 정규화\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    daily_returns = scaler.fit_transform(daily_returns.reshape(-1, 1))\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_size = int(len(daily_returns) * 0.67)\n",
        "    test_size = len(daily_returns) - train_size\n",
        "    train, test = daily_returns[0:train_size, :], daily_returns[train_size:len(daily_returns), :]\n",
        "\n",
        "    # LSTM에 필요한 데이터 형식으로 변환\n",
        "    X_train, Y_train = create_dataset(train, look_back)\n",
        "    X_test, Y_test = create_dataset(test, look_back)\n",
        "\n",
        "    # LSTM 모델 구성\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "    # 모델 저장\n",
        "    models[stock] = model\n",
        "\n",
        "print(\"모든 주식에 대한 LSTM 모델 학습이 완료되었습니다.\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# 각 주식의 예측된 변동성 계산\n",
        "predicted_volatilities = {}\n",
        "for stock, model in models.items():\n",
        "    last_sequence = returns[stock][-look_back:]\n",
        "    scaled_sequence = scaler.transform(last_sequence.reshape(-1, 1))\n",
        "    predicted_return = model.predict(scaled_sequence.reshape(1, look_back, 1))\n",
        "    predicted_volatility = scaler.inverse_transform(predicted_return)[0][0]\n",
        "    predicted_volatilities[stock] = predicted_volatility\n",
        "\n",
        "# 리스크 기여도를 동일하게 하는 최적의 주식 비중 계산 함수\n",
        "def risk_parity_objective(weights, volatilities):\n",
        "    # 각 주식의 리스크 기여도 계산\n",
        "    risk_contributions = [vol * weight for vol, weight in zip(volatilities, weights)]\n",
        "    total_portfolio_volatility = np.sum(risk_contributions)\n",
        "    risk_contributions = [rc / total_portfolio_volatility for rc in risk_contributions]\n",
        "\n",
        "    # 리스크 기여도 간의 편차를 최소화하는 것이 목표\n",
        "    target_risk_contribution = 1 / len(volatilities)\n",
        "    return sum([(rc - target_risk_contribution)**2 for rc in risk_contributions])\n",
        "\n",
        "# 최적화 시작\n",
        "initial_weights = [1/len(stock_list) for _ in stock_list]\n",
        "bounds = [(0, 1) for _ in stock_list]\n",
        "constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}\n",
        "optimized = minimize(risk_parity_objective, initial_weights, args=(list(predicted_volatilities.values()),),\n",
        "                     method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "\n",
        "# 최적의 주식 비중 출력\n",
        "optimal_weights = optimized.x\n",
        "print(\"최적의 주식 비중:\")\n",
        "for stock, weight in zip(stock_list, optimal_weights):\n",
        "    print(f\"{stock}: {weight:.2f}\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# 실제 변동성 계산\n",
        "actual_volatilities = {stock: data['Close'].pct_change().std() for stock, data in stock_data.items()}\n",
        "\n",
        "# 그래프 그리기\n",
        "stocks = list(stock_data.keys())\n",
        "predicted_vols = [predicted_volatilities[stock] for stock in stocks]\n",
        "actual_vols = [actual_volatilities[stock] for stock in stocks]\n",
        "weights = [weight for weight in optimal_weights]\n",
        "\n",
        "barWidth = 0.25\n",
        "r1 = np.arange(len(stocks))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "# 바 차트 생성\n",
        "plt.bar(r1, predicted_vols, width=barWidth, color='blue', edgecolor='grey', label='예측된 변동성')\n",
        "plt.bar(r2, actual_vols, width=barWidth, color='red', edgecolor='grey', label='실제 변동성')\n",
        "plt.bar(r3, weights, width=barWidth, color='green', edgecolor='grey', label='최적의 주식 비중')\n",
        "\n",
        "# 그래프 제목 및 축 이름 설정\n",
        "plt.title('주식별 예측된 변동성, 실제 변동성 및 최적의 주식 비중', fontweight='bold')\n",
        "plt.xlabel('주식', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(stocks))], stocks)\n",
        "plt.ylabel('값', fontweight='bold')\n",
        "\n",
        "# 범례 표시\n",
        "plt.legend()\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()\n",
        "\n",
        "def recommend_weights(optimal_weights, stocks):\n",
        "    print(\"\\n추천 주식 비중:\")\n",
        "    for stock, weight in zip(stocks, optimal_weights):\n",
        "        print(f\"{stock}: {weight:.2%}\")\n",
        "\n",
        "recommend_weights(weights, stocks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXNq6cbi3VYu",
        "outputId": "f7b572e8-b6ca-4a6e-9b8f-3372f9af2ab7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "보유하고 있는 주식의 티커 목록을 쉼표로 구분하여 입력하세요 (예: AAPL, GOOGL, MSFT): AAPL,GOOGL,MSFT\n",
            "Epoch 1/100\n",
            "161/161 - 1s - loss: 0.0266 - 1s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "161/161 - 0s - loss: 0.0209 - 237ms/epoch - 1ms/step\n",
            "Epoch 3/100\n",
            "161/161 - 0s - loss: 0.0208 - 241ms/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "161/161 - 0s - loss: 0.0200 - 259ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "161/161 - 0s - loss: 0.0199 - 258ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "161/161 - 0s - loss: 0.0190 - 255ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "161/161 - 0s - loss: 0.0198 - 254ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "161/161 - 0s - loss: 0.0200 - 262ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "161/161 - 0s - loss: 0.0193 - 248ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "161/161 - 0s - loss: 0.0194 - 250ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "161/161 - 0s - loss: 0.0195 - 258ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "161/161 - 0s - loss: 0.0190 - 242ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "161/161 - 0s - loss: 0.0198 - 259ms/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "161/161 - 0s - loss: 0.0190 - 248ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "161/161 - 0s - loss: 0.0193 - 362ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "161/161 - 0s - loss: 0.0195 - 325ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "161/161 - 0s - loss: 0.0193 - 324ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "161/161 - 0s - loss: 0.0191 - 323ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "161/161 - 0s - loss: 0.0200 - 333ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "161/161 - 0s - loss: 0.0187 - 309ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "161/161 - 0s - loss: 0.0202 - 250ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "161/161 - 0s - loss: 0.0186 - 245ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "161/161 - 0s - loss: 0.0195 - 272ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "161/161 - 0s - loss: 0.0185 - 250ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "161/161 - 0s - loss: 0.0187 - 249ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "161/161 - 0s - loss: 0.0198 - 258ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "161/161 - 0s - loss: 0.0191 - 247ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "161/161 - 0s - loss: 0.0193 - 244ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "161/161 - 0s - loss: 0.0193 - 258ms/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "161/161 - 0s - loss: 0.0197 - 248ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "161/161 - 0s - loss: 0.0186 - 291ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "161/161 - 0s - loss: 0.0188 - 244ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "161/161 - 0s - loss: 0.0185 - 255ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "161/161 - 0s - loss: 0.0188 - 257ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "161/161 - 0s - loss: 0.0190 - 270ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "161/161 - 0s - loss: 0.0186 - 266ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "161/161 - 0s - loss: 0.0186 - 247ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "161/161 - 0s - loss: 0.0191 - 252ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "161/161 - 0s - loss: 0.0187 - 255ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "161/161 - 0s - loss: 0.0187 - 251ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "161/161 - 0s - loss: 0.0188 - 266ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "161/161 - 0s - loss: 0.0187 - 285ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "161/161 - 0s - loss: 0.0183 - 268ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "161/161 - 0s - loss: 0.0188 - 256ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "161/161 - 0s - loss: 0.0191 - 256ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "161/161 - 0s - loss: 0.0189 - 245ms/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "161/161 - 0s - loss: 0.0188 - 252ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "161/161 - 0s - loss: 0.0187 - 251ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "161/161 - 0s - loss: 0.0185 - 250ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "161/161 - 0s - loss: 0.0187 - 254ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "161/161 - 0s - loss: 0.0184 - 288ms/epoch - 2ms/step\n",
            "Epoch 52/100\n",
            "161/161 - 0s - loss: 0.0189 - 280ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "161/161 - 0s - loss: 0.0184 - 265ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "161/161 - 0s - loss: 0.0193 - 263ms/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "161/161 - 0s - loss: 0.0185 - 240ms/epoch - 1ms/step\n",
            "Epoch 56/100\n",
            "161/161 - 0s - loss: 0.0187 - 256ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "161/161 - 0s - loss: 0.0182 - 255ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "161/161 - 0s - loss: 0.0186 - 316ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "161/161 - 0s - loss: 0.0187 - 304ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "161/161 - 0s - loss: 0.0183 - 303ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "161/161 - 0s - loss: 0.0187 - 360ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "161/161 - 0s - loss: 0.0184 - 364ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "161/161 - 0s - loss: 0.0187 - 319ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "161/161 - 0s - loss: 0.0184 - 259ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "161/161 - 0s - loss: 0.0184 - 249ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "161/161 - 0s - loss: 0.0181 - 252ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "161/161 - 0s - loss: 0.0184 - 249ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "161/161 - 0s - loss: 0.0183 - 239ms/epoch - 1ms/step\n",
            "Epoch 69/100\n",
            "161/161 - 0s - loss: 0.0182 - 279ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "161/161 - 0s - loss: 0.0182 - 247ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "161/161 - 0s - loss: 0.0184 - 237ms/epoch - 1ms/step\n",
            "Epoch 72/100\n",
            "161/161 - 0s - loss: 0.0184 - 255ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "161/161 - 0s - loss: 0.0185 - 266ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "161/161 - 0s - loss: 0.0182 - 250ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "161/161 - 0s - loss: 0.0182 - 246ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "161/161 - 0s - loss: 0.0181 - 249ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "161/161 - 0s - loss: 0.0178 - 266ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "161/161 - 0s - loss: 0.0184 - 255ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "161/161 - 0s - loss: 0.0179 - 239ms/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "161/161 - 0s - loss: 0.0188 - 269ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "161/161 - 0s - loss: 0.0182 - 257ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "161/161 - 0s - loss: 0.0183 - 265ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "161/161 - 0s - loss: 0.0183 - 270ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "161/161 - 0s - loss: 0.0180 - 254ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "161/161 - 0s - loss: 0.0182 - 257ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "161/161 - 0s - loss: 0.0183 - 262ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "161/161 - 0s - loss: 0.0180 - 266ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "161/161 - 0s - loss: 0.0184 - 271ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "161/161 - 0s - loss: 0.0173 - 251ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "161/161 - 0s - loss: 0.0182 - 246ms/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "161/161 - 0s - loss: 0.0179 - 264ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "161/161 - 0s - loss: 0.0180 - 267ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "161/161 - 0s - loss: 0.0179 - 253ms/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "161/161 - 0s - loss: 0.0179 - 256ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "161/161 - 0s - loss: 0.0183 - 268ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "161/161 - 0s - loss: 0.0180 - 282ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "161/161 - 0s - loss: 0.0182 - 250ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "161/161 - 0s - loss: 0.0176 - 306ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "161/161 - 0s - loss: 0.0179 - 244ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "161/161 - 0s - loss: 0.0176 - 258ms/epoch - 2ms/step\n",
            "Epoch 1/100\n",
            "161/161 - 2s - loss: 0.0396 - 2s/epoch - 11ms/step\n",
            "Epoch 2/100\n",
            "161/161 - 0s - loss: 0.0245 - 274ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "161/161 - 0s - loss: 0.0224 - 238ms/epoch - 1ms/step\n",
            "Epoch 4/100\n",
            "161/161 - 0s - loss: 0.0217 - 245ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "161/161 - 0s - loss: 0.0222 - 246ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "161/161 - 0s - loss: 0.0217 - 265ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "161/161 - 0s - loss: 0.0214 - 243ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "161/161 - 0s - loss: 0.0226 - 246ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "161/161 - 0s - loss: 0.0217 - 237ms/epoch - 1ms/step\n",
            "Epoch 10/100\n",
            "161/161 - 0s - loss: 0.0236 - 245ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "161/161 - 0s - loss: 0.0213 - 266ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "161/161 - 0s - loss: 0.0219 - 289ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "161/161 - 0s - loss: 0.0208 - 241ms/epoch - 1ms/step\n",
            "Epoch 14/100\n",
            "161/161 - 0s - loss: 0.0216 - 254ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "161/161 - 0s - loss: 0.0213 - 248ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "161/161 - 0s - loss: 0.0220 - 266ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "161/161 - 0s - loss: 0.0214 - 298ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "161/161 - 0s - loss: 0.0213 - 272ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "161/161 - 0s - loss: 0.0207 - 267ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "161/161 - 0s - loss: 0.0209 - 245ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "161/161 - 0s - loss: 0.0210 - 245ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "161/161 - 0s - loss: 0.0217 - 266ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "161/161 - 0s - loss: 0.0210 - 256ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "161/161 - 0s - loss: 0.0210 - 356ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "161/161 - 0s - loss: 0.0209 - 367ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "161/161 - 0s - loss: 0.0209 - 367ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "161/161 - 0s - loss: 0.0208 - 264ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "161/161 - 0s - loss: 0.0209 - 248ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "161/161 - 0s - loss: 0.0214 - 257ms/epoch - 2ms/step\n",
            "Epoch 30/100\n",
            "161/161 - 0s - loss: 0.0206 - 247ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "161/161 - 0s - loss: 0.0208 - 244ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "161/161 - 0s - loss: 0.0215 - 234ms/epoch - 1ms/step\n",
            "Epoch 33/100\n",
            "161/161 - 0s - loss: 0.0207 - 336ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "161/161 - 0s - loss: 0.0202 - 251ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "161/161 - 0s - loss: 0.0206 - 238ms/epoch - 1ms/step\n",
            "Epoch 36/100\n",
            "161/161 - 0s - loss: 0.0205 - 252ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "161/161 - 0s - loss: 0.0201 - 253ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "161/161 - 0s - loss: 0.0208 - 275ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "161/161 - 0s - loss: 0.0197 - 296ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "161/161 - 0s - loss: 0.0213 - 321ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "161/161 - 0s - loss: 0.0199 - 342ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "161/161 - 0s - loss: 0.0202 - 316ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "161/161 - 0s - loss: 0.0198 - 315ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "161/161 - 0s - loss: 0.0204 - 320ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "161/161 - 0s - loss: 0.0200 - 263ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "161/161 - 0s - loss: 0.0201 - 293ms/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "161/161 - 0s - loss: 0.0200 - 237ms/epoch - 1ms/step\n",
            "Epoch 48/100\n",
            "161/161 - 0s - loss: 0.0205 - 258ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "161/161 - 0s - loss: 0.0194 - 265ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "161/161 - 0s - loss: 0.0204 - 237ms/epoch - 1ms/step\n",
            "Epoch 51/100\n",
            "161/161 - 0s - loss: 0.0203 - 238ms/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "161/161 - 0s - loss: 0.0196 - 261ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "161/161 - 0s - loss: 0.0206 - 262ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "161/161 - 0s - loss: 0.0200 - 237ms/epoch - 1ms/step\n",
            "Epoch 55/100\n",
            "161/161 - 0s - loss: 0.0201 - 242ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "161/161 - 0s - loss: 0.0202 - 253ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "161/161 - 0s - loss: 0.0199 - 254ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "161/161 - 0s - loss: 0.0212 - 243ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "161/161 - 0s - loss: 0.0207 - 243ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "161/161 - 0s - loss: 0.0199 - 251ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "161/161 - 0s - loss: 0.0196 - 256ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "161/161 - 0s - loss: 0.0201 - 259ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "161/161 - 0s - loss: 0.0199 - 253ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "161/161 - 0s - loss: 0.0195 - 294ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "161/161 - 0s - loss: 0.0203 - 273ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "161/161 - 0s - loss: 0.0196 - 259ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "161/161 - 0s - loss: 0.0200 - 260ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "161/161 - 0s - loss: 0.0199 - 255ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "161/161 - 0s - loss: 0.0200 - 261ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "161/161 - 0s - loss: 0.0210 - 251ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "161/161 - 0s - loss: 0.0196 - 249ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "161/161 - 0s - loss: 0.0197 - 262ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "161/161 - 0s - loss: 0.0199 - 257ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "161/161 - 0s - loss: 0.0196 - 251ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "161/161 - 0s - loss: 0.0200 - 338ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "161/161 - 0s - loss: 0.0195 - 349ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "161/161 - 0s - loss: 0.0195 - 422ms/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "161/161 - 0s - loss: 0.0195 - 299ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "161/161 - 0s - loss: 0.0200 - 267ms/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "161/161 - 0s - loss: 0.0200 - 249ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "161/161 - 0s - loss: 0.0200 - 262ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "161/161 - 0s - loss: 0.0200 - 313ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "161/161 - 0s - loss: 0.0198 - 303ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "161/161 - 0s - loss: 0.0196 - 315ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "161/161 - 0s - loss: 0.0198 - 366ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "161/161 - 0s - loss: 0.0201 - 357ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "161/161 - 0s - loss: 0.0193 - 252ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "161/161 - 0s - loss: 0.0196 - 237ms/epoch - 1ms/step\n",
            "Epoch 89/100\n",
            "161/161 - 0s - loss: 0.0196 - 274ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "161/161 - 0s - loss: 0.0195 - 236ms/epoch - 1ms/step\n",
            "Epoch 91/100\n",
            "161/161 - 0s - loss: 0.0200 - 263ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "161/161 - 0s - loss: 0.0197 - 264ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "161/161 - 0s - loss: 0.0197 - 241ms/epoch - 1ms/step\n",
            "Epoch 94/100\n",
            "161/161 - 0s - loss: 0.0199 - 254ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "161/161 - 0s - loss: 0.0196 - 243ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "161/161 - 0s - loss: 0.0194 - 246ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "161/161 - 0s - loss: 0.0195 - 249ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "161/161 - 0s - loss: 0.0193 - 245ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "161/161 - 0s - loss: 0.0196 - 266ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "161/161 - 0s - loss: 0.0193 - 243ms/epoch - 2ms/step\n",
            "Epoch 1/100\n",
            "161/161 - 1s - loss: 0.0313 - 1s/epoch - 9ms/step\n",
            "Epoch 2/100\n",
            "161/161 - 0s - loss: 0.0193 - 242ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "161/161 - 0s - loss: 0.0196 - 264ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "161/161 - 0s - loss: 0.0190 - 257ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "161/161 - 0s - loss: 0.0197 - 238ms/epoch - 1ms/step\n",
            "Epoch 6/100\n",
            "161/161 - 0s - loss: 0.0200 - 252ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "161/161 - 0s - loss: 0.0190 - 260ms/epoch - 2ms/step\n",
            "Epoch 8/100\n",
            "161/161 - 0s - loss: 0.0181 - 259ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "161/161 - 0s - loss: 0.0183 - 291ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "161/161 - 0s - loss: 0.0182 - 274ms/epoch - 2ms/step\n",
            "Epoch 11/100\n",
            "161/161 - 0s - loss: 0.0180 - 271ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "161/161 - 0s - loss: 0.0177 - 332ms/epoch - 2ms/step\n",
            "Epoch 13/100\n",
            "161/161 - 0s - loss: 0.0177 - 323ms/epoch - 2ms/step\n",
            "Epoch 14/100\n",
            "161/161 - 0s - loss: 0.0178 - 353ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "161/161 - 0s - loss: 0.0178 - 357ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "161/161 - 0s - loss: 0.0171 - 416ms/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "161/161 - 0s - loss: 0.0184 - 236ms/epoch - 1ms/step\n",
            "Epoch 18/100\n",
            "161/161 - 0s - loss: 0.0181 - 240ms/epoch - 1ms/step\n",
            "Epoch 19/100\n",
            "161/161 - 0s - loss: 0.0175 - 253ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "161/161 - 0s - loss: 0.0171 - 250ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "161/161 - 0s - loss: 0.0179 - 259ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "161/161 - 0s - loss: 0.0177 - 245ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "161/161 - 0s - loss: 0.0175 - 242ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "161/161 - 0s - loss: 0.0168 - 244ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "161/161 - 0s - loss: 0.0170 - 297ms/epoch - 2ms/step\n",
            "Epoch 26/100\n",
            "161/161 - 0s - loss: 0.0173 - 246ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "161/161 - 0s - loss: 0.0181 - 253ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "161/161 - 0s - loss: 0.0172 - 246ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "161/161 - 0s - loss: 0.0171 - 239ms/epoch - 1ms/step\n",
            "Epoch 30/100\n",
            "161/161 - 0s - loss: 0.0168 - 243ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "161/161 - 0s - loss: 0.0161 - 248ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "161/161 - 0s - loss: 0.0161 - 258ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "161/161 - 0s - loss: 0.0164 - 285ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "161/161 - 0s - loss: 0.0156 - 266ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "161/161 - 0s - loss: 0.0163 - 283ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "161/161 - 0s - loss: 0.0165 - 260ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "161/161 - 0s - loss: 0.0160 - 253ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "161/161 - 0s - loss: 0.0167 - 241ms/epoch - 1ms/step\n",
            "Epoch 39/100\n",
            "161/161 - 0s - loss: 0.0161 - 250ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "161/161 - 0s - loss: 0.0169 - 264ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "161/161 - 0s - loss: 0.0158 - 234ms/epoch - 1ms/step\n",
            "Epoch 42/100\n",
            "161/161 - 0s - loss: 0.0160 - 248ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "161/161 - 0s - loss: 0.0163 - 279ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "161/161 - 0s - loss: 0.0158 - 247ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "161/161 - 0s - loss: 0.0164 - 261ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "161/161 - 0s - loss: 0.0160 - 238ms/epoch - 1ms/step\n",
            "Epoch 47/100\n",
            "161/161 - 0s - loss: 0.0157 - 250ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "161/161 - 0s - loss: 0.0156 - 275ms/epoch - 2ms/step\n",
            "Epoch 49/100\n",
            "161/161 - 0s - loss: 0.0160 - 264ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "161/161 - 0s - loss: 0.0159 - 246ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "161/161 - 0s - loss: 0.0158 - 237ms/epoch - 1ms/step\n",
            "Epoch 52/100\n",
            "161/161 - 0s - loss: 0.0155 - 248ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "161/161 - 0s - loss: 0.0166 - 292ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "161/161 - 0s - loss: 0.0162 - 288ms/epoch - 2ms/step\n",
            "Epoch 55/100\n",
            "161/161 - 0s - loss: 0.0162 - 366ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "161/161 - 0s - loss: 0.0157 - 336ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "161/161 - 0s - loss: 0.0156 - 324ms/epoch - 2ms/step\n",
            "Epoch 58/100\n",
            "161/161 - 0s - loss: 0.0156 - 316ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "161/161 - 0s - loss: 0.0155 - 348ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "161/161 - 0s - loss: 0.0156 - 301ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "161/161 - 0s - loss: 0.0157 - 249ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "161/161 - 0s - loss: 0.0157 - 244ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "161/161 - 0s - loss: 0.0158 - 234ms/epoch - 1ms/step\n",
            "Epoch 64/100\n",
            "161/161 - 0s - loss: 0.0155 - 287ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "161/161 - 0s - loss: 0.0159 - 256ms/epoch - 2ms/step\n",
            "Epoch 66/100\n",
            "161/161 - 0s - loss: 0.0159 - 283ms/epoch - 2ms/step\n",
            "Epoch 67/100\n",
            "161/161 - 0s - loss: 0.0155 - 266ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "161/161 - 0s - loss: 0.0158 - 279ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "161/161 - 0s - loss: 0.0154 - 261ms/epoch - 2ms/step\n",
            "Epoch 70/100\n",
            "161/161 - 0s - loss: 0.0159 - 259ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "161/161 - 0s - loss: 0.0151 - 257ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "161/161 - 0s - loss: 0.0155 - 243ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "161/161 - 0s - loss: 0.0153 - 243ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "161/161 - 0s - loss: 0.0156 - 245ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "161/161 - 0s - loss: 0.0153 - 259ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "161/161 - 0s - loss: 0.0151 - 250ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "161/161 - 0s - loss: 0.0153 - 248ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "161/161 - 0s - loss: 0.0154 - 253ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "161/161 - 0s - loss: 0.0152 - 238ms/epoch - 1ms/step\n",
            "Epoch 80/100\n",
            "161/161 - 0s - loss: 0.0150 - 289ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "161/161 - 0s - loss: 0.0152 - 302ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "161/161 - 0s - loss: 0.0153 - 310ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "161/161 - 0s - loss: 0.0152 - 259ms/epoch - 2ms/step\n",
            "Epoch 84/100\n",
            "161/161 - 0s - loss: 0.0152 - 240ms/epoch - 1ms/step\n",
            "Epoch 85/100\n",
            "161/161 - 0s - loss: 0.0153 - 250ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "161/161 - 0s - loss: 0.0152 - 260ms/epoch - 2ms/step\n",
            "Epoch 87/100\n",
            "161/161 - 0s - loss: 0.0151 - 288ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "161/161 - 0s - loss: 0.0152 - 252ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "161/161 - 0s - loss: 0.0151 - 268ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "161/161 - 0s - loss: 0.0148 - 246ms/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "161/161 - 0s - loss: 0.0147 - 261ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "161/161 - 0s - loss: 0.0152 - 234ms/epoch - 1ms/step\n",
            "Epoch 93/100\n",
            "161/161 - 0s - loss: 0.0150 - 271ms/epoch - 2ms/step\n",
            "Epoch 94/100\n",
            "161/161 - 0s - loss: 0.0153 - 246ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "161/161 - 0s - loss: 0.0150 - 249ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "161/161 - 0s - loss: 0.0152 - 287ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "161/161 - 0s - loss: 0.0149 - 276ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "161/161 - 0s - loss: 0.0148 - 316ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "161/161 - 0s - loss: 0.0146 - 357ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "161/161 - 0s - loss: 0.0147 - 369ms/epoch - 2ms/step\n",
            "모든 주식에 대한 LSTM 모델 학습이 완료되었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x783678a08ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 292ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78367e6c5bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 276ms/step\n",
            "1/1 [==============================] - 1s 857ms/step\n",
            "최적의 주식 비중:\n",
            "AAPL: 1.00\n",
            "GOOGL: 0.00\n",
            "MSFT: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44050 (\\N{HANGUL SYLLABLE GABS}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50696 (\\N{HANGUL SYLLABLE YE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52769 (\\N{HANGUL SYLLABLE CEUG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46108 (\\N{HANGUL SYLLABLE DOEN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49457 (\\N{HANGUL SYLLABLE SEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51228 (\\N{HANGUL SYLLABLE JE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48143 (\\N{HANGUL SYLLABLE MIC}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52572 (\\N{HANGUL SYLLABLE COE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51201 (\\N{HANGUL SYLLABLE JEOG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "추천 주식 비중:\n",
            "AAPL: 100.00%\n",
            "GOOGL: 0.00%\n",
            "MSFT: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_volatility, actual_volatilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZOLr5RcMjRv",
        "outputId": "d2bfd627-b0e4-4012-a574-9f7ad3badcb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0045356615 {'AAPL': 0.016843363696269154, 'GOOGL': 0.021245942107535838, 'MSFT': 0.018728909728422814}\n"
          ]
        }
      ]
    }
  ]
}